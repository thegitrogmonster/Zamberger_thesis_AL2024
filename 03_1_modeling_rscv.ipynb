{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the Initial Model \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Notebook is develped to identify and specify the models, which will be used to apply the Active Learning strategies on. At least two models will be created, as described in the initial Research Proposal: \n",
    "1. PLS-Regression-Model \n",
    "2. Random-Forest-Regression-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation\n",
    "\n",
    "To work in python, various libraries are needed. So the neccessary libraries are imported in the next cell. \n",
    "\n",
    "The code is developed inspired by the machine learining course by [Peter Sykacek](peter.sykacek[at]boku.ac.at) in the winter of 2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python312.zip',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/lib-dynload',\n",
       " '',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages',\n",
       " './',\n",
       " './server_files/ml_group/course.lib',\n",
       " './models',\n",
       " './data',\n",
       " './figures/03_modeling_figures',\n",
       " './results/03_modeling_results',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis',\n",
       " './',\n",
       " './server_files/ml_group/course.lib',\n",
       " './models',\n",
       " './data',\n",
       " './figures/03_modeling_figures',\n",
       " './results/03_modeling_results',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis',\n",
       " './models',\n",
       " './',\n",
       " './server_files/ml_group/course.lib',\n",
       " './models',\n",
       " './data',\n",
       " './figures/03_modeling_figures',\n",
       " './results/03_modeling_results',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.clear()\n",
    "\n",
    "# Basepath\n",
    "basepath=\"./\" # Project directory\n",
    "sys.path.append(basepath)\n",
    "sys.path.append(basepath+\"server_files/ml_group/course.lib\")\n",
    "\n",
    "# Data\n",
    "DATA_PATH = basepath + \"data\"\n",
    "\n",
    "#Figure\n",
    "FIGURE_PATH = basepath + \"figures/03_modeling_figures\"\n",
    "\n",
    "# Modelpath\n",
    "MODEL_PATH = basepath + \"models\"\n",
    "\n",
    "# Path to environment\n",
    "\n",
    "ENV_PATH = \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib\"\n",
    "\n",
    "# Resultspath\n",
    "RESULTS_PATH = basepath + \"results/03_modeling_results\"\n",
    "\n",
    "# Add the paths\n",
    "sys.path.extend({DATA_PATH, FIGURE_PATH, MODEL_PATH, ENV_PATH, RESULTS_PATH})\n",
    "sys.path # Check if the path is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## timing the full notebook\n",
    "import time\n",
    "nb_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Path to store the ML Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the path to save the ml models\n",
    "\n",
    "MODEL_PATH = basepath + \"models\"\n",
    "sys.path.append(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ml_lib as mlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### turn off convergence warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate various models an import of the respective functions from preexisting packages is neccessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch Crossvalidation\n",
    "\n",
    "[sklearn GSCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "Exhaustive search over specified parameter values for an estimator.\n",
    "Important members are fit, predict.\n",
    "\n",
    "* GridSearchCV implements a \"fit\" and a \"score\" method.\n",
    "* It also implements \"score_samples\", \"predict\", \"predict_proba\", \"decision_function\", \"transform\" and \"inverse_transform\" if they are implemented in the estimator used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV as GSCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Parameter Optimization\n",
    "\n",
    "[sklearn RandomizedSearchCV](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)  \n",
    "\n",
    " RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "A budget can be chosen independent of the number of parameters and possible values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold cross-validator.\n",
    "[sklearn KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
    "\n",
    "Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).\n",
    "\n",
    "Each fold is then used once as a validation while the k - 1 remaining folds form the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge\n",
    "\n",
    "[sklearn KRR](https://scikit-learn.org/stable/modules/kernel_ridge.html#kernel-ridge-regression)\n",
    "\n",
    "Kernel ridge regression (KRR) [M2012] combines Ridge regression and classification (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge as KRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inspection\n",
    "\n",
    "[sklearn cv_results_](https://scikit-learn.org/stable/modules/grid_search.html#analyzing-results-with-the-cv-results-attribute)\n",
    "\n",
    "\"The cv_results_ attribute contains useful information for analyzing the results of a search. It can be converted to a pandas dataframe with df = pd.DataFrame(est.cv_results_).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the sample data will be imported. \n",
    "\n",
    "Currently 2 Datasets are of interest for us: \n",
    "1. PS20191107_gegl.csv\n",
    "2. dps1200.csv\n",
    "\n",
    "The differences are that the first is a dataframe containing the data unmodified and full. It was used to generate the later, which contains only selected sections of the spectra. The Wavelengths of this dataset were selected by discarding Wavelengths, based on critieria ???\n",
    "\n",
    "**TODO**: Research the criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS20191107 (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>Origin</th>\n",
       "      <th>type</th>\n",
       "      <th>3996</th>\n",
       "      <th>3994</th>\n",
       "      <th>3992</th>\n",
       "      <th>3990</th>\n",
       "      <th>3988</th>\n",
       "      <th>3987</th>\n",
       "      <th>...</th>\n",
       "      <th>417</th>\n",
       "      <th>415</th>\n",
       "      <th>413</th>\n",
       "      <th>411</th>\n",
       "      <th>409</th>\n",
       "      <th>407</th>\n",
       "      <th>405</th>\n",
       "      <th>403</th>\n",
       "      <th>401</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2GOS-18_1955</td>\n",
       "      <td>1955</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>0.015830</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.015787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027973</td>\n",
       "      <td>-0.028180</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>-0.028595</td>\n",
       "      <td>-0.029011</td>\n",
       "      <td>-0.029123</td>\n",
       "      <td>-0.029323</td>\n",
       "      <td>-0.029610</td>\n",
       "      <td>-0.029759</td>\n",
       "      <td>-0.029746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2GOS-18_1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>0.016663</td>\n",
       "      <td>0.016569</td>\n",
       "      <td>0.016333</td>\n",
       "      <td>0.016217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029520</td>\n",
       "      <td>-0.029747</td>\n",
       "      <td>-0.029978</td>\n",
       "      <td>-0.030204</td>\n",
       "      <td>-0.030087</td>\n",
       "      <td>-0.030284</td>\n",
       "      <td>-0.030746</td>\n",
       "      <td>-0.031163</td>\n",
       "      <td>-0.031519</td>\n",
       "      <td>-0.031815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2GOS-18_1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.021364</td>\n",
       "      <td>0.021662</td>\n",
       "      <td>0.021862</td>\n",
       "      <td>0.021573</td>\n",
       "      <td>0.020925</td>\n",
       "      <td>0.020585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031046</td>\n",
       "      <td>-0.031270</td>\n",
       "      <td>-0.031483</td>\n",
       "      <td>-0.031701</td>\n",
       "      <td>-0.032089</td>\n",
       "      <td>-0.032390</td>\n",
       "      <td>-0.032609</td>\n",
       "      <td>-0.032653</td>\n",
       "      <td>-0.032627</td>\n",
       "      <td>-0.032784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2GOS-18_1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.019351</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.019181</td>\n",
       "      <td>0.018998</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.019205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029852</td>\n",
       "      <td>-0.030092</td>\n",
       "      <td>-0.030361</td>\n",
       "      <td>-0.030647</td>\n",
       "      <td>-0.031115</td>\n",
       "      <td>-0.031281</td>\n",
       "      <td>-0.031376</td>\n",
       "      <td>-0.031721</td>\n",
       "      <td>-0.032172</td>\n",
       "      <td>-0.032433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2GOS-18_1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.018548</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>0.018616</td>\n",
       "      <td>0.018375</td>\n",
       "      <td>0.018266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029963</td>\n",
       "      <td>-0.030206</td>\n",
       "      <td>-0.030436</td>\n",
       "      <td>-0.030643</td>\n",
       "      <td>-0.030917</td>\n",
       "      <td>-0.031127</td>\n",
       "      <td>-0.031338</td>\n",
       "      <td>-0.031409</td>\n",
       "      <td>-0.031364</td>\n",
       "      <td>-0.031465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1870 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  year Origin    type      3996      3994      3992      3990  \\\n",
       "0  2GOS-18_1955  1955    POL  living  0.016119  0.015972  0.015830  0.015728   \n",
       "1  2GOS-18_1969  1969    POL  living  0.016368  0.016543  0.016663  0.016569   \n",
       "2  2GOS-18_1974  1974    POL  living  0.021364  0.021662  0.021862  0.021573   \n",
       "3  2GOS-18_1976  1976    POL  living  0.019351  0.019246  0.019181  0.018998   \n",
       "4  2GOS-18_1996  1996    POL  living  0.018548  0.018604  0.018670  0.018616   \n",
       "\n",
       "       3988      3987  ...       417       415       413       411       409  \\\n",
       "0  0.015734  0.015787  ... -0.027973 -0.028180 -0.028389 -0.028595 -0.029011   \n",
       "1  0.016333  0.016217  ... -0.029520 -0.029747 -0.029978 -0.030204 -0.030087   \n",
       "2  0.020925  0.020585  ... -0.031046 -0.031270 -0.031483 -0.031701 -0.032089   \n",
       "3  0.018926  0.019205  ... -0.029852 -0.030092 -0.030361 -0.030647 -0.031115   \n",
       "4  0.018375  0.018266  ... -0.029963 -0.030206 -0.030436 -0.030643 -0.030917   \n",
       "\n",
       "        407       405       403       401       399  \n",
       "0 -0.029123 -0.029323 -0.029610 -0.029759 -0.029746  \n",
       "1 -0.030284 -0.030746 -0.031163 -0.031519 -0.031815  \n",
       "2 -0.032390 -0.032609 -0.032653 -0.032627 -0.032784  \n",
       "3 -0.031281 -0.031376 -0.031721 -0.032172 -0.032433  \n",
       "4 -0.031127 -0.031338 -0.031409 -0.031364 -0.031465  \n",
       "\n",
       "[5 rows x 1870 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_full = pd.read_csv(basepath+\"data/PS20191107_gegl.csv\", \n",
    "                            sep=\";\", decimal=\",\", encoding=\"utf-8\")\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>3996</th>\n",
       "      <th>3994</th>\n",
       "      <th>3992</th>\n",
       "      <th>3990</th>\n",
       "      <th>3988</th>\n",
       "      <th>3987</th>\n",
       "      <th>3985</th>\n",
       "      <th>3983</th>\n",
       "      <th>3981</th>\n",
       "      <th>...</th>\n",
       "      <th>417</th>\n",
       "      <th>415</th>\n",
       "      <th>413</th>\n",
       "      <th>411</th>\n",
       "      <th>409</th>\n",
       "      <th>407</th>\n",
       "      <th>405</th>\n",
       "      <th>403</th>\n",
       "      <th>401</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "      <td>2244.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-152.291889</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>0.011238</td>\n",
       "      <td>0.011167</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.010838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024586</td>\n",
       "      <td>-0.024782</td>\n",
       "      <td>-0.024978</td>\n",
       "      <td>-0.025175</td>\n",
       "      <td>-0.025414</td>\n",
       "      <td>-0.025638</td>\n",
       "      <td>-0.025847</td>\n",
       "      <td>-0.026018</td>\n",
       "      <td>-0.026165</td>\n",
       "      <td>-0.026328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3659.189806</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.005231</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.005212</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>0.005203</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.003334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-13555.000000</td>\n",
       "      <td>-0.002773</td>\n",
       "      <td>-0.002953</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>-0.002312</td>\n",
       "      <td>-0.002147</td>\n",
       "      <td>-0.002444</td>\n",
       "      <td>-0.003096</td>\n",
       "      <td>-0.003154</td>\n",
       "      <td>-0.003191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035057</td>\n",
       "      <td>-0.035212</td>\n",
       "      <td>-0.035350</td>\n",
       "      <td>-0.035504</td>\n",
       "      <td>-0.036023</td>\n",
       "      <td>-0.036370</td>\n",
       "      <td>-0.036337</td>\n",
       "      <td>-0.036325</td>\n",
       "      <td>-0.036375</td>\n",
       "      <td>-0.036506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-370.250000</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>0.007587</td>\n",
       "      <td>0.007504</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>0.007297</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027123</td>\n",
       "      <td>-0.027286</td>\n",
       "      <td>-0.027461</td>\n",
       "      <td>-0.027654</td>\n",
       "      <td>-0.027906</td>\n",
       "      <td>-0.028134</td>\n",
       "      <td>-0.028300</td>\n",
       "      <td>-0.028394</td>\n",
       "      <td>-0.028544</td>\n",
       "      <td>-0.028711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1472.500000</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.012160</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.011959</td>\n",
       "      <td>0.011875</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>0.011888</td>\n",
       "      <td>0.011831</td>\n",
       "      <td>0.011757</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024376</td>\n",
       "      <td>-0.024564</td>\n",
       "      <td>-0.024764</td>\n",
       "      <td>-0.024939</td>\n",
       "      <td>-0.025142</td>\n",
       "      <td>-0.025372</td>\n",
       "      <td>-0.025599</td>\n",
       "      <td>-0.025777</td>\n",
       "      <td>-0.025945</td>\n",
       "      <td>-0.026034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1806.000000</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.014983</td>\n",
       "      <td>0.014875</td>\n",
       "      <td>0.014752</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.014619</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021932</td>\n",
       "      <td>-0.022128</td>\n",
       "      <td>-0.022342</td>\n",
       "      <td>-0.022558</td>\n",
       "      <td>-0.022820</td>\n",
       "      <td>-0.023040</td>\n",
       "      <td>-0.023253</td>\n",
       "      <td>-0.023459</td>\n",
       "      <td>-0.023626</td>\n",
       "      <td>-0.023843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>0.028401</td>\n",
       "      <td>0.027898</td>\n",
       "      <td>0.027302</td>\n",
       "      <td>0.027014</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.026733</td>\n",
       "      <td>0.027129</td>\n",
       "      <td>0.026971</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013279</td>\n",
       "      <td>-0.013542</td>\n",
       "      <td>-0.013811</td>\n",
       "      <td>-0.014076</td>\n",
       "      <td>-0.014361</td>\n",
       "      <td>-0.014733</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>-0.015328</td>\n",
       "      <td>-0.015409</td>\n",
       "      <td>-0.015598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1867 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               year         3996         3994         3992         3990  \\\n",
       "count   2244.000000  2244.000000  2244.000000  2244.000000  2244.000000   \n",
       "mean    -152.291889     0.011321     0.011238     0.011167     0.011087   \n",
       "std     3659.189806     0.005232     0.005231     0.005229     0.005212   \n",
       "min   -13555.000000    -0.002773    -0.002953    -0.002774    -0.002312   \n",
       "25%     -370.250000     0.007695     0.007627     0.007587     0.007504   \n",
       "50%     1472.500000     0.012248     0.012160     0.012073     0.011959   \n",
       "75%     1806.000000     0.015064     0.014983     0.014875     0.014752   \n",
       "max     2009.000000     0.028401     0.027898     0.027302     0.027014   \n",
       "\n",
       "              3988         3987         3985         3983         3981  ...  \\\n",
       "count  2244.000000  2244.000000  2244.000000  2244.000000  2244.000000  ...   \n",
       "mean      0.011004     0.010989     0.010963     0.010900     0.010838  ...   \n",
       "std       0.005180     0.005176     0.005207     0.005203     0.005198  ...   \n",
       "min      -0.002147    -0.002444    -0.003096    -0.003154    -0.003191  ...   \n",
       "25%       0.007385     0.007355     0.007297     0.007233     0.007174  ...   \n",
       "50%       0.011875     0.011864     0.011888     0.011831     0.011757  ...   \n",
       "75%       0.014673     0.014688     0.014675     0.014619     0.014553  ...   \n",
       "max       0.026885     0.026733     0.027129     0.026971     0.026841  ...   \n",
       "\n",
       "               417          415          413          411          409  \\\n",
       "count  2244.000000  2244.000000  2244.000000  2244.000000  2244.000000   \n",
       "mean     -0.024586    -0.024782    -0.024978    -0.025175    -0.025414   \n",
       "std       0.003439     0.003428     0.003417     0.003405     0.003410   \n",
       "min      -0.035057    -0.035212    -0.035350    -0.035504    -0.036023   \n",
       "25%      -0.027123    -0.027286    -0.027461    -0.027654    -0.027906   \n",
       "50%      -0.024376    -0.024564    -0.024764    -0.024939    -0.025142   \n",
       "75%      -0.021932    -0.022128    -0.022342    -0.022558    -0.022820   \n",
       "max      -0.013279    -0.013542    -0.013811    -0.014076    -0.014361   \n",
       "\n",
       "               407          405          403          401          399  \n",
       "count  2244.000000  2244.000000  2244.000000  2244.000000  2244.000000  \n",
       "mean     -0.025638    -0.025847    -0.026018    -0.026165    -0.026328  \n",
       "std       0.003386     0.003367     0.003356     0.003346     0.003334  \n",
       "min      -0.036370    -0.036337    -0.036325    -0.036375    -0.036506  \n",
       "25%      -0.028134    -0.028300    -0.028394    -0.028544    -0.028711  \n",
       "50%      -0.025372    -0.025599    -0.025777    -0.025945    -0.026034  \n",
       "75%      -0.023040    -0.023253    -0.023459    -0.023626    -0.023843  \n",
       "max      -0.014733    -0.015133    -0.015328    -0.015409    -0.015598  \n",
       "\n",
       "[8 rows x 1867 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrive basic characteristics for each variable\n",
    "data_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>constr</th>\n",
       "      <td>1936</td>\n",
       "      <td>1628.966667</td>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dry</th>\n",
       "      <td>1765</td>\n",
       "      <td>948.591716</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>living</th>\n",
       "      <td>2009</td>\n",
       "      <td>1886.843700</td>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>1912</td>\n",
       "      <td>-2682.173774</td>\n",
       "      <td>-13555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year                    \n",
       "         max         mean    min\n",
       "type                            \n",
       "constr  1936  1628.966667   1239\n",
       "dry     1765   948.591716    327\n",
       "living  2009  1886.843700   1524\n",
       "water   1912 -2682.173774 -13555"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full.groupby('type')[['year']].agg(['max', 'mean', 'min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset dps1200.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>tree</th>\n",
       "      <th>Origin</th>\n",
       "      <th>type</th>\n",
       "      <th>X2970</th>\n",
       "      <th>X2968</th>\n",
       "      <th>X2966</th>\n",
       "      <th>X2964</th>\n",
       "      <th>X2962</th>\n",
       "      <th>...</th>\n",
       "      <th>X818</th>\n",
       "      <th>X816</th>\n",
       "      <th>X814</th>\n",
       "      <th>X812</th>\n",
       "      <th>X810</th>\n",
       "      <th>X808</th>\n",
       "      <th>X806</th>\n",
       "      <th>X804</th>\n",
       "      <th>X802</th>\n",
       "      <th>X800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2GOS-18_1955</td>\n",
       "      <td>1955</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.019849</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>0.020414</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023469</td>\n",
       "      <td>-0.023367</td>\n",
       "      <td>-0.023283</td>\n",
       "      <td>-0.023220</td>\n",
       "      <td>-0.023183</td>\n",
       "      <td>-0.023174</td>\n",
       "      <td>-0.023190</td>\n",
       "      <td>-0.023228</td>\n",
       "      <td>-0.023293</td>\n",
       "      <td>-0.023388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2GOS-18_1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.023933</td>\n",
       "      <td>0.024378</td>\n",
       "      <td>0.024827</td>\n",
       "      <td>0.025273</td>\n",
       "      <td>0.025712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024117</td>\n",
       "      <td>-0.024076</td>\n",
       "      <td>-0.024043</td>\n",
       "      <td>-0.024021</td>\n",
       "      <td>-0.024015</td>\n",
       "      <td>-0.024033</td>\n",
       "      <td>-0.024077</td>\n",
       "      <td>-0.024147</td>\n",
       "      <td>-0.024238</td>\n",
       "      <td>-0.024346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2GOS-18_1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.022342</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>0.023099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026266</td>\n",
       "      <td>-0.026214</td>\n",
       "      <td>-0.026172</td>\n",
       "      <td>-0.026149</td>\n",
       "      <td>-0.026146</td>\n",
       "      <td>-0.026165</td>\n",
       "      <td>-0.026208</td>\n",
       "      <td>-0.026273</td>\n",
       "      <td>-0.026363</td>\n",
       "      <td>-0.026479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2GOS-18_1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.021999</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.022651</td>\n",
       "      <td>0.022999</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025113</td>\n",
       "      <td>-0.025030</td>\n",
       "      <td>-0.024959</td>\n",
       "      <td>-0.024909</td>\n",
       "      <td>-0.024885</td>\n",
       "      <td>-0.024888</td>\n",
       "      <td>-0.024918</td>\n",
       "      <td>-0.024971</td>\n",
       "      <td>-0.025049</td>\n",
       "      <td>-0.025153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2GOS-18_1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.021031</td>\n",
       "      <td>0.021338</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>0.021923</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025256</td>\n",
       "      <td>-0.025158</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>-0.025035</td>\n",
       "      <td>-0.025013</td>\n",
       "      <td>-0.025015</td>\n",
       "      <td>-0.025040</td>\n",
       "      <td>-0.025094</td>\n",
       "      <td>-0.025177</td>\n",
       "      <td>-0.025282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  year     tree Origin    type     X2970     X2968     X2966  \\\n",
       "0  2GOS-18_1955  1955  2GOS-18    POL  living  0.019849  0.020121  0.020414   \n",
       "1  2GOS-18_1969  1969  2GOS-18    POL  living  0.023933  0.024378  0.024827   \n",
       "2  2GOS-18_1974  1974  2GOS-18    POL  living  0.021605  0.021971  0.022342   \n",
       "3  2GOS-18_1976  1976  2GOS-18    POL  living  0.021999  0.022315  0.022651   \n",
       "4  2GOS-18_1996  1996  2GOS-18    POL  living  0.021031  0.021338  0.021626   \n",
       "\n",
       "      X2964     X2962  ...      X818      X816      X814      X812      X810  \\\n",
       "0  0.020724  0.021030  ... -0.023469 -0.023367 -0.023283 -0.023220 -0.023183   \n",
       "1  0.025273  0.025712  ... -0.024117 -0.024076 -0.024043 -0.024021 -0.024015   \n",
       "2  0.022719  0.023099  ... -0.026266 -0.026214 -0.026172 -0.026149 -0.026146   \n",
       "3  0.022999  0.023345  ... -0.025113 -0.025030 -0.024959 -0.024909 -0.024885   \n",
       "4  0.021923  0.022248  ... -0.025256 -0.025158 -0.025083 -0.025035 -0.025013   \n",
       "\n",
       "       X808      X806      X804      X802      X800  \n",
       "0 -0.023174 -0.023190 -0.023228 -0.023293 -0.023388  \n",
       "1 -0.024033 -0.024077 -0.024147 -0.024238 -0.024346  \n",
       "2 -0.026165 -0.026208 -0.026273 -0.026363 -0.026479  \n",
       "3 -0.024888 -0.024918 -0.024971 -0.025049 -0.025153  \n",
       "4 -0.025015 -0.025040 -0.025094 -0.025177 -0.025282  \n",
       "\n",
       "[5 rows x 415 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small = pd.read_csv(basepath+\"data/dps1200.csv\", \n",
    "                            sep=\",\", decimal=\".\", encoding=\"utf-8\")\n",
    "data_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>year</th>\n",
       "      <th>tree</th>\n",
       "      <th>Origin</th>\n",
       "      <th>type</th>\n",
       "      <th>2970</th>\n",
       "      <th>2968</th>\n",
       "      <th>2966</th>\n",
       "      <th>2964</th>\n",
       "      <th>2962</th>\n",
       "      <th>...</th>\n",
       "      <th>818</th>\n",
       "      <th>816</th>\n",
       "      <th>814</th>\n",
       "      <th>812</th>\n",
       "      <th>810</th>\n",
       "      <th>808</th>\n",
       "      <th>806</th>\n",
       "      <th>804</th>\n",
       "      <th>802</th>\n",
       "      <th>800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2GOS-18_1955</td>\n",
       "      <td>1955</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.019849</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>0.020414</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023469</td>\n",
       "      <td>-0.023367</td>\n",
       "      <td>-0.023283</td>\n",
       "      <td>-0.023220</td>\n",
       "      <td>-0.023183</td>\n",
       "      <td>-0.023174</td>\n",
       "      <td>-0.023190</td>\n",
       "      <td>-0.023228</td>\n",
       "      <td>-0.023293</td>\n",
       "      <td>-0.023388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2GOS-18_1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.023933</td>\n",
       "      <td>0.024378</td>\n",
       "      <td>0.024827</td>\n",
       "      <td>0.025273</td>\n",
       "      <td>0.025712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024117</td>\n",
       "      <td>-0.024076</td>\n",
       "      <td>-0.024043</td>\n",
       "      <td>-0.024021</td>\n",
       "      <td>-0.024015</td>\n",
       "      <td>-0.024033</td>\n",
       "      <td>-0.024077</td>\n",
       "      <td>-0.024147</td>\n",
       "      <td>-0.024238</td>\n",
       "      <td>-0.024346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2GOS-18_1974</td>\n",
       "      <td>1974</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.022342</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>0.023099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026266</td>\n",
       "      <td>-0.026214</td>\n",
       "      <td>-0.026172</td>\n",
       "      <td>-0.026149</td>\n",
       "      <td>-0.026146</td>\n",
       "      <td>-0.026165</td>\n",
       "      <td>-0.026208</td>\n",
       "      <td>-0.026273</td>\n",
       "      <td>-0.026363</td>\n",
       "      <td>-0.026479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2GOS-18_1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.021999</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.022651</td>\n",
       "      <td>0.022999</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025113</td>\n",
       "      <td>-0.025030</td>\n",
       "      <td>-0.024959</td>\n",
       "      <td>-0.024909</td>\n",
       "      <td>-0.024885</td>\n",
       "      <td>-0.024888</td>\n",
       "      <td>-0.024918</td>\n",
       "      <td>-0.024971</td>\n",
       "      <td>-0.025049</td>\n",
       "      <td>-0.025153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2GOS-18_1996</td>\n",
       "      <td>1996</td>\n",
       "      <td>2GOS-18</td>\n",
       "      <td>POL</td>\n",
       "      <td>living</td>\n",
       "      <td>0.021031</td>\n",
       "      <td>0.021338</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>0.021923</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025256</td>\n",
       "      <td>-0.025158</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>-0.025035</td>\n",
       "      <td>-0.025013</td>\n",
       "      <td>-0.025015</td>\n",
       "      <td>-0.025040</td>\n",
       "      <td>-0.025094</td>\n",
       "      <td>-0.025177</td>\n",
       "      <td>-0.025282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  year     tree Origin    type      2970      2968      2966  \\\n",
       "0  2GOS-18_1955  1955  2GOS-18    POL  living  0.019849  0.020121  0.020414   \n",
       "1  2GOS-18_1969  1969  2GOS-18    POL  living  0.023933  0.024378  0.024827   \n",
       "2  2GOS-18_1974  1974  2GOS-18    POL  living  0.021605  0.021971  0.022342   \n",
       "3  2GOS-18_1976  1976  2GOS-18    POL  living  0.021999  0.022315  0.022651   \n",
       "4  2GOS-18_1996  1996  2GOS-18    POL  living  0.021031  0.021338  0.021626   \n",
       "\n",
       "       2964      2962  ...       818       816       814       812       810  \\\n",
       "0  0.020724  0.021030  ... -0.023469 -0.023367 -0.023283 -0.023220 -0.023183   \n",
       "1  0.025273  0.025712  ... -0.024117 -0.024076 -0.024043 -0.024021 -0.024015   \n",
       "2  0.022719  0.023099  ... -0.026266 -0.026214 -0.026172 -0.026149 -0.026146   \n",
       "3  0.022999  0.023345  ... -0.025113 -0.025030 -0.024959 -0.024909 -0.024885   \n",
       "4  0.021923  0.022248  ... -0.025256 -0.025158 -0.025083 -0.025035 -0.025013   \n",
       "\n",
       "        808       806       804       802       800  \n",
       "0 -0.023174 -0.023190 -0.023228 -0.023293 -0.023388  \n",
       "1 -0.024033 -0.024077 -0.024147 -0.024238 -0.024346  \n",
       "2 -0.026165 -0.026208 -0.026273 -0.026363 -0.026479  \n",
       "3 -0.024888 -0.024918 -0.024971 -0.025049 -0.025153  \n",
       "4 -0.025015 -0.025040 -0.025094 -0.025177 -0.025282  \n",
       "\n",
       "[5 rows x 415 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the column headers\n",
    "\n",
    "# data_1200.rename(lambda x: x[1:], axis='columns')\n",
    "data_small = data_small.rename(columns=lambda x: x.replace('X', ''))\n",
    "data_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>2970</th>\n",
       "      <th>2968</th>\n",
       "      <th>2966</th>\n",
       "      <th>2964</th>\n",
       "      <th>2962</th>\n",
       "      <th>2960</th>\n",
       "      <th>2959</th>\n",
       "      <th>2957</th>\n",
       "      <th>2955</th>\n",
       "      <th>...</th>\n",
       "      <th>818</th>\n",
       "      <th>816</th>\n",
       "      <th>814</th>\n",
       "      <th>812</th>\n",
       "      <th>810</th>\n",
       "      <th>808</th>\n",
       "      <th>806</th>\n",
       "      <th>804</th>\n",
       "      <th>802</th>\n",
       "      <th>800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1740.420930</td>\n",
       "      <td>0.018827</td>\n",
       "      <td>0.019122</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>0.019740</td>\n",
       "      <td>0.020061</td>\n",
       "      <td>0.020389</td>\n",
       "      <td>0.020728</td>\n",
       "      <td>0.021078</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020705</td>\n",
       "      <td>-0.020572</td>\n",
       "      <td>-0.020456</td>\n",
       "      <td>-0.020361</td>\n",
       "      <td>-0.020292</td>\n",
       "      <td>-0.020254</td>\n",
       "      <td>-0.020248</td>\n",
       "      <td>-0.020276</td>\n",
       "      <td>-0.020335</td>\n",
       "      <td>-0.020420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>196.420289</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.002560</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>0.002735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1194.000000</td>\n",
       "      <td>0.011339</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.012466</td>\n",
       "      <td>0.012791</td>\n",
       "      <td>0.013134</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>0.013861</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026419</td>\n",
       "      <td>-0.026362</td>\n",
       "      <td>-0.026319</td>\n",
       "      <td>-0.026296</td>\n",
       "      <td>-0.026293</td>\n",
       "      <td>-0.026308</td>\n",
       "      <td>-0.026335</td>\n",
       "      <td>-0.026373</td>\n",
       "      <td>-0.026451</td>\n",
       "      <td>-0.026601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1616.000000</td>\n",
       "      <td>0.017552</td>\n",
       "      <td>0.017789</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>0.018325</td>\n",
       "      <td>0.018587</td>\n",
       "      <td>0.018869</td>\n",
       "      <td>0.019175</td>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022815</td>\n",
       "      <td>-0.022723</td>\n",
       "      <td>-0.022603</td>\n",
       "      <td>-0.022533</td>\n",
       "      <td>-0.022505</td>\n",
       "      <td>-0.022469</td>\n",
       "      <td>-0.022500</td>\n",
       "      <td>-0.022531</td>\n",
       "      <td>-0.022616</td>\n",
       "      <td>-0.022704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1769.000000</td>\n",
       "      <td>0.018673</td>\n",
       "      <td>0.018942</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.019521</td>\n",
       "      <td>0.019805</td>\n",
       "      <td>0.020112</td>\n",
       "      <td>0.020430</td>\n",
       "      <td>0.020781</td>\n",
       "      <td>0.021118</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020678</td>\n",
       "      <td>-0.020497</td>\n",
       "      <td>-0.020383</td>\n",
       "      <td>-0.020287</td>\n",
       "      <td>-0.020214</td>\n",
       "      <td>-0.020141</td>\n",
       "      <td>-0.020116</td>\n",
       "      <td>-0.020118</td>\n",
       "      <td>-0.020152</td>\n",
       "      <td>-0.020231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1913.000000</td>\n",
       "      <td>0.019991</td>\n",
       "      <td>0.020269</td>\n",
       "      <td>0.020567</td>\n",
       "      <td>0.020911</td>\n",
       "      <td>0.021258</td>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.021966</td>\n",
       "      <td>0.022307</td>\n",
       "      <td>0.022655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018458</td>\n",
       "      <td>-0.018294</td>\n",
       "      <td>-0.018154</td>\n",
       "      <td>-0.018033</td>\n",
       "      <td>-0.017950</td>\n",
       "      <td>-0.017888</td>\n",
       "      <td>-0.017871</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.017919</td>\n",
       "      <td>-0.017988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>0.028247</td>\n",
       "      <td>0.029124</td>\n",
       "      <td>0.029990</td>\n",
       "      <td>0.030832</td>\n",
       "      <td>0.031645</td>\n",
       "      <td>0.032436</td>\n",
       "      <td>0.033220</td>\n",
       "      <td>0.034017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013677</td>\n",
       "      <td>-0.013408</td>\n",
       "      <td>-0.013178</td>\n",
       "      <td>-0.012991</td>\n",
       "      <td>-0.012843</td>\n",
       "      <td>-0.012738</td>\n",
       "      <td>-0.012686</td>\n",
       "      <td>-0.012691</td>\n",
       "      <td>-0.012745</td>\n",
       "      <td>-0.012844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              year         2970         2968         2966         2964  \\\n",
       "count  1290.000000  1290.000000  1290.000000  1290.000000  1290.000000   \n",
       "mean   1740.420930     0.018827     0.019122     0.019427     0.019740   \n",
       "std     196.420289     0.001922     0.001978     0.002038     0.002100   \n",
       "min    1194.000000     0.011339     0.011597     0.011871     0.012159   \n",
       "25%    1616.000000     0.017552     0.017789     0.018044     0.018325   \n",
       "50%    1769.000000     0.018673     0.018942     0.019230     0.019521   \n",
       "75%    1913.000000     0.019991     0.020269     0.020567     0.020911   \n",
       "max    2009.000000     0.027378     0.028247     0.029124     0.029990   \n",
       "\n",
       "              2962         2960         2959         2957         2955  ...  \\\n",
       "count  1290.000000  1290.000000  1290.000000  1290.000000  1290.000000  ...   \n",
       "mean      0.020061     0.020389     0.020728     0.021078     0.021439  ...   \n",
       "std       0.002163     0.002223     0.002279     0.002333     0.002385  ...   \n",
       "min       0.012466     0.012791     0.013134     0.013493     0.013861  ...   \n",
       "25%       0.018587     0.018869     0.019175     0.019468     0.019784  ...   \n",
       "50%       0.019805     0.020112     0.020430     0.020781     0.021118  ...   \n",
       "75%       0.021258     0.021605     0.021966     0.022307     0.022655  ...   \n",
       "max       0.030832     0.031645     0.032436     0.033220     0.034017  ...   \n",
       "\n",
       "               818          816          814          812          810  \\\n",
       "count  1290.000000  1290.000000  1290.000000  1290.000000  1290.000000   \n",
       "mean     -0.020705    -0.020572    -0.020456    -0.020361    -0.020292   \n",
       "std       0.002526     0.002560     0.002593     0.002622     0.002649   \n",
       "min      -0.026419    -0.026362    -0.026319    -0.026296    -0.026293   \n",
       "25%      -0.022815    -0.022723    -0.022603    -0.022533    -0.022505   \n",
       "50%      -0.020678    -0.020497    -0.020383    -0.020287    -0.020214   \n",
       "75%      -0.018458    -0.018294    -0.018154    -0.018033    -0.017950   \n",
       "max      -0.013677    -0.013408    -0.013178    -0.012991    -0.012843   \n",
       "\n",
       "               808          806          804          802          800  \n",
       "count  1290.000000  1290.000000  1290.000000  1290.000000  1290.000000  \n",
       "mean     -0.020254    -0.020248    -0.020276    -0.020335    -0.020420  \n",
       "std       0.002673     0.002693     0.002710     0.002723     0.002735  \n",
       "min      -0.026308    -0.026335    -0.026373    -0.026451    -0.026601  \n",
       "25%      -0.022469    -0.022500    -0.022531    -0.022616    -0.022704  \n",
       "50%      -0.020141    -0.020116    -0.020118    -0.020152    -0.020231  \n",
       "75%      -0.017888    -0.017871    -0.017885    -0.017919    -0.017988  \n",
       "max      -0.012738    -0.012686    -0.012691    -0.012745    -0.012844  \n",
       "\n",
       "[8 rows x 411 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small.describe()\n",
    "# describe() gives some basic statistics for numeric columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tree</th>\n",
       "      <th>Origin</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1290</td>\n",
       "      <td>1290</td>\n",
       "      <td>1290</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1290</td>\n",
       "      <td>139</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>SZLPS15a_1982</td>\n",
       "      <td>Dev2b</td>\n",
       "      <td>AUT</td>\n",
       "      <td>living</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>631</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0   tree Origin    type\n",
       "count            1290   1290   1290    1290\n",
       "unique           1290    139      4       4\n",
       "top     SZLPS15a_1982  Dev2b    AUT  living\n",
       "freq                1     29    631     627"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small.describe(include=\"object\")\n",
    "# describe() gives some basic statistics for numeric columns, \n",
    "# categorial columns are included with the option include=\"object\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mode for Cross Validation\n",
      "Splitting the data for faster modelling\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters for the CV\n",
    "\n",
    "# Switch for the dataset\n",
    "    # Select from (data_1200, data_full) or other if implemented\n",
    "data = data_small\n",
    "\n",
    "# Switch for testing mode (use only 10% of the data, among others)\n",
    "testing = True\n",
    "\n",
    "# Define a random state for randomized processes\n",
    "random_state = np.random.RandomState(202375)\n",
    "\n",
    "# Define a metrics for model evaluation\n",
    "cv_scorer = 'neg_mean_squared_error'\n",
    "\n",
    "######################################################\n",
    "if testing == True:\n",
    "    nfolds = 2\n",
    "    NoTrials = 2\n",
    "    n_jobs = 20\n",
    "    save_model = False\n",
    "    print(\"Testing mode for Cross Validation\")\n",
    "    print(\"Splitting the data for faster modelling\")\n",
    "    data = data.sample(frac=0.1)\n",
    "else:\n",
    "    nfolds = 10\n",
    "    NoTrials = 15\n",
    "    n_jobs = -1\n",
    "    save_model = True\n",
    "    print(\"Extensive mode for Cross Validation\")\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2970</th>\n",
       "      <th>2968</th>\n",
       "      <th>2966</th>\n",
       "      <th>2964</th>\n",
       "      <th>2962</th>\n",
       "      <th>2960</th>\n",
       "      <th>2959</th>\n",
       "      <th>2957</th>\n",
       "      <th>2955</th>\n",
       "      <th>2953</th>\n",
       "      <th>...</th>\n",
       "      <th>818</th>\n",
       "      <th>816</th>\n",
       "      <th>814</th>\n",
       "      <th>812</th>\n",
       "      <th>810</th>\n",
       "      <th>808</th>\n",
       "      <th>806</th>\n",
       "      <th>804</th>\n",
       "      <th>802</th>\n",
       "      <th>800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019849</td>\n",
       "      <td>0.020121</td>\n",
       "      <td>0.020414</td>\n",
       "      <td>0.020724</td>\n",
       "      <td>0.021030</td>\n",
       "      <td>0.021321</td>\n",
       "      <td>0.021615</td>\n",
       "      <td>0.021931</td>\n",
       "      <td>0.022270</td>\n",
       "      <td>0.022634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023469</td>\n",
       "      <td>-0.023367</td>\n",
       "      <td>-0.023283</td>\n",
       "      <td>-0.023220</td>\n",
       "      <td>-0.023183</td>\n",
       "      <td>-0.023174</td>\n",
       "      <td>-0.023190</td>\n",
       "      <td>-0.023228</td>\n",
       "      <td>-0.023293</td>\n",
       "      <td>-0.023388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023933</td>\n",
       "      <td>0.024378</td>\n",
       "      <td>0.024827</td>\n",
       "      <td>0.025273</td>\n",
       "      <td>0.025712</td>\n",
       "      <td>0.026149</td>\n",
       "      <td>0.026586</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.027490</td>\n",
       "      <td>0.027963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024117</td>\n",
       "      <td>-0.024076</td>\n",
       "      <td>-0.024043</td>\n",
       "      <td>-0.024021</td>\n",
       "      <td>-0.024015</td>\n",
       "      <td>-0.024033</td>\n",
       "      <td>-0.024077</td>\n",
       "      <td>-0.024147</td>\n",
       "      <td>-0.024238</td>\n",
       "      <td>-0.024346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021605</td>\n",
       "      <td>0.021971</td>\n",
       "      <td>0.022342</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>0.023099</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>0.024207</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>0.025038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026266</td>\n",
       "      <td>-0.026214</td>\n",
       "      <td>-0.026172</td>\n",
       "      <td>-0.026149</td>\n",
       "      <td>-0.026146</td>\n",
       "      <td>-0.026165</td>\n",
       "      <td>-0.026208</td>\n",
       "      <td>-0.026273</td>\n",
       "      <td>-0.026363</td>\n",
       "      <td>-0.026479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021999</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.022651</td>\n",
       "      <td>0.022999</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>0.024386</td>\n",
       "      <td>0.024768</td>\n",
       "      <td>0.025150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025113</td>\n",
       "      <td>-0.025030</td>\n",
       "      <td>-0.024959</td>\n",
       "      <td>-0.024909</td>\n",
       "      <td>-0.024885</td>\n",
       "      <td>-0.024888</td>\n",
       "      <td>-0.024918</td>\n",
       "      <td>-0.024971</td>\n",
       "      <td>-0.025049</td>\n",
       "      <td>-0.025153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021031</td>\n",
       "      <td>0.021338</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>0.021923</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.022589</td>\n",
       "      <td>0.022925</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.023624</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025256</td>\n",
       "      <td>-0.025158</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>-0.025035</td>\n",
       "      <td>-0.025013</td>\n",
       "      <td>-0.025015</td>\n",
       "      <td>-0.025040</td>\n",
       "      <td>-0.025094</td>\n",
       "      <td>-0.025177</td>\n",
       "      <td>-0.025282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.018577</td>\n",
       "      <td>0.018906</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>0.019604</td>\n",
       "      <td>0.019962</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.020670</td>\n",
       "      <td>0.021035</td>\n",
       "      <td>0.021428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018632</td>\n",
       "      <td>-0.018450</td>\n",
       "      <td>-0.018285</td>\n",
       "      <td>-0.018149</td>\n",
       "      <td>-0.018048</td>\n",
       "      <td>-0.017984</td>\n",
       "      <td>-0.017955</td>\n",
       "      <td>-0.017961</td>\n",
       "      <td>-0.017997</td>\n",
       "      <td>-0.018058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>0.018508</td>\n",
       "      <td>0.018778</td>\n",
       "      <td>0.019051</td>\n",
       "      <td>0.019341</td>\n",
       "      <td>0.019650</td>\n",
       "      <td>0.019962</td>\n",
       "      <td>0.020274</td>\n",
       "      <td>0.020597</td>\n",
       "      <td>0.020944</td>\n",
       "      <td>0.021318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019053</td>\n",
       "      <td>-0.018876</td>\n",
       "      <td>-0.018717</td>\n",
       "      <td>-0.018583</td>\n",
       "      <td>-0.018481</td>\n",
       "      <td>-0.018413</td>\n",
       "      <td>-0.018380</td>\n",
       "      <td>-0.018379</td>\n",
       "      <td>-0.018410</td>\n",
       "      <td>-0.018469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.017486</td>\n",
       "      <td>0.017786</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.018423</td>\n",
       "      <td>0.018749</td>\n",
       "      <td>0.019080</td>\n",
       "      <td>0.019421</td>\n",
       "      <td>0.019777</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018587</td>\n",
       "      <td>-0.018406</td>\n",
       "      <td>-0.018242</td>\n",
       "      <td>-0.018103</td>\n",
       "      <td>-0.017993</td>\n",
       "      <td>-0.017912</td>\n",
       "      <td>-0.017865</td>\n",
       "      <td>-0.017854</td>\n",
       "      <td>-0.017877</td>\n",
       "      <td>-0.017928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>0.017298</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.017791</td>\n",
       "      <td>0.018060</td>\n",
       "      <td>0.018352</td>\n",
       "      <td>0.018656</td>\n",
       "      <td>0.018964</td>\n",
       "      <td>0.019273</td>\n",
       "      <td>0.019592</td>\n",
       "      <td>0.019929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018916</td>\n",
       "      <td>-0.018720</td>\n",
       "      <td>-0.018541</td>\n",
       "      <td>-0.018390</td>\n",
       "      <td>-0.018273</td>\n",
       "      <td>-0.018189</td>\n",
       "      <td>-0.018138</td>\n",
       "      <td>-0.018122</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.018194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>0.017682</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.018186</td>\n",
       "      <td>0.018456</td>\n",
       "      <td>0.018740</td>\n",
       "      <td>0.019026</td>\n",
       "      <td>0.019317</td>\n",
       "      <td>0.019621</td>\n",
       "      <td>0.019942</td>\n",
       "      <td>0.020279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018848</td>\n",
       "      <td>-0.018661</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>-0.018340</td>\n",
       "      <td>-0.018227</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>-0.018104</td>\n",
       "      <td>-0.018094</td>\n",
       "      <td>-0.018115</td>\n",
       "      <td>-0.018165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1290 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          2970      2968      2966      2964      2962      2960      2959  \\\n",
       "0     0.019849  0.020121  0.020414  0.020724  0.021030  0.021321  0.021615   \n",
       "1     0.023933  0.024378  0.024827  0.025273  0.025712  0.026149  0.026586   \n",
       "2     0.021605  0.021971  0.022342  0.022719  0.023099  0.023470  0.023832   \n",
       "3     0.021999  0.022315  0.022651  0.022999  0.023345  0.023682  0.024023   \n",
       "4     0.021031  0.021338  0.021626  0.021923  0.022248  0.022589  0.022925   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1285  0.018254  0.018577  0.018906  0.019248  0.019604  0.019962  0.020317   \n",
       "1286  0.018508  0.018778  0.019051  0.019341  0.019650  0.019962  0.020274   \n",
       "1287  0.017196  0.017486  0.017786  0.018100  0.018423  0.018749  0.019080   \n",
       "1288  0.017298  0.017541  0.017791  0.018060  0.018352  0.018656  0.018964   \n",
       "1289  0.017682  0.017931  0.018186  0.018456  0.018740  0.019026  0.019317   \n",
       "\n",
       "          2957      2955      2953  ...       818       816       814  \\\n",
       "0     0.021931  0.022270  0.022634  ... -0.023469 -0.023367 -0.023283   \n",
       "1     0.027030  0.027490  0.027963  ... -0.024117 -0.024076 -0.024043   \n",
       "2     0.024207  0.024610  0.025038  ... -0.026266 -0.026214 -0.026172   \n",
       "3     0.024386  0.024768  0.025150  ... -0.025113 -0.025030 -0.024959   \n",
       "4     0.023264  0.023624  0.024004  ... -0.025256 -0.025158 -0.025083   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1285  0.020670  0.021035  0.021428  ... -0.018632 -0.018450 -0.018285   \n",
       "1286  0.020597  0.020944  0.021318  ... -0.019053 -0.018876 -0.018717   \n",
       "1287  0.019421  0.019777  0.020150  ... -0.018587 -0.018406 -0.018242   \n",
       "1288  0.019273  0.019592  0.019929  ... -0.018916 -0.018720 -0.018541   \n",
       "1289  0.019621  0.019942  0.020279  ... -0.018848 -0.018661 -0.018488   \n",
       "\n",
       "           812       810       808       806       804       802       800  \n",
       "0    -0.023220 -0.023183 -0.023174 -0.023190 -0.023228 -0.023293 -0.023388  \n",
       "1    -0.024021 -0.024015 -0.024033 -0.024077 -0.024147 -0.024238 -0.024346  \n",
       "2    -0.026149 -0.026146 -0.026165 -0.026208 -0.026273 -0.026363 -0.026479  \n",
       "3    -0.024909 -0.024885 -0.024888 -0.024918 -0.024971 -0.025049 -0.025153  \n",
       "4    -0.025035 -0.025013 -0.025015 -0.025040 -0.025094 -0.025177 -0.025282  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1285 -0.018149 -0.018048 -0.017984 -0.017955 -0.017961 -0.017997 -0.018058  \n",
       "1286 -0.018583 -0.018481 -0.018413 -0.018380 -0.018379 -0.018410 -0.018469  \n",
       "1287 -0.018103 -0.017993 -0.017912 -0.017865 -0.017854 -0.017877 -0.017928  \n",
       "1288 -0.018390 -0.018273 -0.018189 -0.018138 -0.018122 -0.018142 -0.018194  \n",
       "1289 -0.018340 -0.018227 -0.018148 -0.018104 -0.018094 -0.018115 -0.018165  \n",
       "\n",
       "[1290 rows x 410 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.select_dtypes('float')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1955\n",
       "1       1969\n",
       "2       1974\n",
       "3       1976\n",
       "4       1996\n",
       "        ... \n",
       "1285    1942\n",
       "1286    1952\n",
       "1287    1962\n",
       "1288    1972\n",
       "1289    1982\n",
       "Name: year, Length: 1290, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['year']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7FC1AD8A8C40"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "During this Project, we will generate statistical model with a random fraction of the dataset. The remainder will be retained to be used as test values to estimate the accuracy of the model and potentially detect overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (RSCV)\n",
    "Implemented: \n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 of 15\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 40\u001b[0m\n\u001b[1;32m     29\u001b[0m rf_rscv \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     30\u001b[0m     rf, \u001b[38;5;66;03m# regressor\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_distribs, \u001b[38;5;66;03m# hyperparameter space\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     37\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# fit the model on the Trainig Data\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mrf_rscv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# calculate the CV scores\u001b[39;00m\n\u001b[1;32m     43\u001b[0m rf_rscv_rmse1[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m-\u001b[39mrf_rscv\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RF Define the parameters for the CV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rf = RandomForestRegressor() # default criterion to evaluate the quality of the split is the ”squared_error”\n",
    "\n",
    "param_distribs = {'n_estimators': randint(low=3, high=150), # for hyperparameter with discrete values \n",
    "                  'min_samples_split': randint(low=2, high=20), \n",
    "                  'max_depth': randint(low=1, high=20), \n",
    "                  'min_samples_leaf': randint(low=1, high=10),\n",
    "                  }\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "rf_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "rf_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    rf_rscv = RandomizedSearchCV(\n",
    "        rf, # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=1, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    rf_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    rf_rscv_rmse1[i] = np.sqrt(-rf_rscv.best_score_)\n",
    "    y_pred_rf = cvp(rf_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    rf_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_rf))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"Execution time: {execution_time} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal Parameters according to RSCV:', rf_rscv.best_params_)\n",
    "print('best score', rf_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF with optimal parameters\n",
    "\n",
    "extract the best parameters and run the RF Regression with the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "rf_opt = RandomForestRegressor(**rf_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "rf_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "y_pred_rf = rf_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS (RSCV)\n",
    "Implemented:\n",
    "\n",
    "\n",
    "*TODO*\n",
    "\n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter search should be limited in regards to the numebr of components to keep:\n",
    "# \"Should be in [1, min(n_samples, n_features, n_targets)]\" sklearn\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the CV\n",
    "from sklearn import cross_decomposition\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pls = cross_decomposition.PLSRegression()\n",
    "\n",
    "param_distribs = {'n_components': randint(low=1, high=90), #  should be in [1, min(n_samples, n_features, n_targets = 90)].\n",
    "                  'max_iter': randint(low=2, high=700), \n",
    "                  }\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "pls_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "pls_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    pls_rscv = RandomizedSearchCV(\n",
    "        pls, # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=1, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    pls_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    pls_rscv_rmse1[i] = np.sqrt(-pls_rscv.best_score_)\n",
    "    y_pred_pls = cvp(pls_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    pls_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_pls))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"Execution time: {execution_time} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal Parameters according to RSCV:', pls_rscv.best_params_)\n",
    "print('best score' ,pls_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLS with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "pls_opt = cross_decomposition.PLSRegression(**pls_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "pls_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "\n",
    "y_pred_pls = pls_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_pls = np.sqrt(mean_squared_error(y_test, y_pred_pls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_pls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KRR with RBF (RSCV)\n",
    "Implemented:\n",
    "\n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters  \n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRR with RBF \n",
    "\n",
    "# alpha: Regularization strength\n",
    "\n",
    "param_distribs = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3], \n",
    "                  \"gamma\": np.logspace(-2, 2, 7)}\n",
    "\n",
    "# param_distribs = {\"alpha\": np.logspace(0.0001, 0.1), \n",
    "#                  \"gamma\": np.logspace(0.0001, 0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-02, 4.64158883e-02, 2.15443469e-01, 1.00000000e+00,\n",
       "       4.64158883e+00, 2.15443469e+01, 1.00000000e+02])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-2, 2, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 of 2\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'activation' for estimator KernelRidge(kernel='rbf'). Valid parameters are: ['alpha', 'coef0', 'degree', 'gamma', 'kernel', 'kernel_params'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 883, in _fit_and_score\n    estimator = estimator.set_params(**clone(parameters, safe=False))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/base.py\", line 279, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'activation' for estimator KernelRidge(kernel='rbf'). Valid parameters are: ['alpha', 'coef0', 'degree', 'gamma', 'kernel', 'kernel_params'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 28\u001b[0m\n\u001b[1;32m     17\u001b[0m krr_rscv \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     18\u001b[0m     KRR(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;66;03m# regressor\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_distribs, \u001b[38;5;66;03m# hyperparameter space\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     25\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# fit the model on the Trainig Data\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mkrr_rscv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# calculate the CV scores\u001b[39;00m\n\u001b[1;32m     31\u001b[0m krr_rscv_rmse1[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m-\u001b[39mkrr_rscv\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1914\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter 'activation' for estimator KernelRidge(kernel='rbf'). Valid parameters are: ['alpha', 'coef0', 'degree', 'gamma', 'kernel', 'kernel_params']."
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "krr_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "krr_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    krr_rscv = RandomizedSearchCV(\n",
    "        KRR(kernel='rbf'), # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=1, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    krr_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    krr_rscv_rmse1[i] = np.sqrt(-krr_rscv.best_score_)\n",
    "    y_pred_krr = cvp(krr_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    krr_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_krr))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/ 60\n",
    "print(f\"Execution time: {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Parameters according to RSCV: {'gamma': 4.641588833612777, 'alpha': 0.001}\n",
      "best score -11991.66413517979\n"
     ]
    }
   ],
   "source": [
    "print('optimal Parameters according to RSCV:', krr_rscv.best_params_)\n",
    "print('best score' ,krr_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR (rbf) with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "krr_opt = KRR(**krr_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "krr_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "\n",
    "y_pred_krr = krr_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_krr = np.sqrt(mean_squared_error(y_test, y_pred_krr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.7497505300415"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_krr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (RSCV)\n",
    "\n",
    "This Method, the multi-layer perceptron creates a neural network, where neurons are organized in three or more layers (1 input-, n hidden-, and 1 output-layer). The MLP is based on a threshold logic unit (TLU, sometimes linear threshold unit LTU). A TLU recieves input from its connections and calculates 'weights' from the sum of all inputs and calculates a step function. Common step functions are the *Heaviside step function* or *sign function*. \n",
    "\n",
    "To compute the outputs of a single fully connnected layer the following eq. can be used \n",
    "\n",
    "(citation: A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems, 2nd ed. O’Reilly Media, Inc., 2019. p.283)\n",
    "‌\n",
    "\n",
    "$$h_{W,b}(X) = \\phi(WX + b)$$\n",
    "\n",
    "Implemented: \n",
    "\n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters \n",
    "\n",
    "*TODO* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter Distribution for mlp\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_distribs = {\"hidden_layer_sizes\": randint(low=50, high=200), # number of neurons in each layer\n",
    "                  \"activation\": ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                  \"solver\": ['lbfgs','sgd', 'adam'],\n",
    "                  'alpha': uniform(loc=0.0001, scale=0.1),\n",
    "                  'early_stopping': [True, False],  \n",
    "                  'validation_fraction': uniform(loc=0.1, scale=0.1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 of 2\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "2 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 476, in _fit\n",
      "    self._fit_stochastic(\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 660, in _fit_stochastic\n",
      "    self._update_no_improvement_count(early_stopping, X_val, y_val)\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 708, in _update_no_improvement_count\n",
      "    self.validation_scores_.append(self._score(X_val, y_val))\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1611, in _score\n",
      "    return r2_score(y, y_pred)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 186, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 1180, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 104, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1049, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 126, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 175, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [  -16903.5416852  -2823158.1724955    -38640.75167932 -1917961.0164821\n",
      "   -38687.27410528               nan   -16408.71708034 -2900822.87164633\n",
      " -1449532.96966981   -38671.76809257]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Trial 1 of 2\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "4 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 496, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 476, in _fit\n",
      "    self._fit_stochastic(\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 660, in _fit_stochastic\n",
      "    self._update_no_improvement_count(early_stopping, X_val, y_val)\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 708, in _update_no_improvement_count\n",
      "    self.validation_scores_.append(self._score(X_val, y_val))\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1611, in _score\n",
      "    return r2_score(y, y_pred)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 186, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 1180, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/metrics/_regression.py\", line 104, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1049, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 126, in _assert_all_finite\n",
      "    _assert_all_finite_element_wise(\n",
      "  File \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 175, in _assert_all_finite_element_wise\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [  -38599.7284109    -15533.1425444                nan -2756269.24240205\n",
      " -2758141.962074     -16966.63029102   -16292.5154304  -2809442.54589209\n",
      "   -16175.06150485               nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Execution time: 2.6294793248176576 minutes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "mlp_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "mlp_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    mlp_rscv = RandomizedSearchCV(\n",
    "        MLP(), # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=1, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    mlp_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    mlp_rscv_rmse1[i] = np.sqrt(-mlp_rscv.best_score_)\n",
    "    y_pred_mlp = cvp(mlp_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    mlp_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_mlp))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/ 60\n",
    "print(f\"Execution time: {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Parameters according to RSCV: {'activation': 'identity', 'alpha': 0.06501356369183195, 'early_stopping': True, 'hidden_layer_sizes': 151, 'solver': 'lbfgs', 'validation_fraction': 0.125833371501127}\n",
      "best score -15533.142544400609\n"
     ]
    }
   ],
   "source": [
    "print('optimal Parameters according to RSCV:', mlp_rscv.best_params_)\n",
    "print('best score' ,mlp_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "mlp_opt = MLP(**mlp_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "mlp_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "y_pred_mlp = mlp_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.9663381316122"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (RSCV)\n",
    "\n",
    "Implemented:\n",
    "\n",
    "- import\n",
    "\n",
    "*TODO*\n",
    "\n",
    "\n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.3'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "xgboost.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform the data into the XGBoost data class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details see [datacamp](https://www.datacamp.com/tutorial/xgboost-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "dtrain_reg = xgboost.DMatrix(X_train, y_train)\n",
    "dtest_reg = xgboost.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the objective\n",
    "\n",
    "XGBoost will be used here for a regression problem, with the objective to minimize the squared error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"reg:squarederror\", \n",
    "          \"tree_method\": \"hist\"} # \"gpu_hist\" for gpu only, set to 'hist' if on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:158.91235\tvalidation-rmse:170.96301\n",
      "[20]\ttrain-rmse:23.83772\tvalidation-rmse:124.03000\n",
      "[40]\ttrain-rmse:8.38947\tvalidation-rmse:124.03466\n",
      "[60]\ttrain-rmse:3.18431\tvalidation-rmse:123.78980\n",
      "[70]\ttrain-rmse:2.03336\tvalidation-rmse:123.78806\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "n = 500 # number of rounds\n",
    "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")] # specify the data for evaluation\n",
    "\n",
    "model = xgboost.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval = 20, \n",
    "   early_stopping_rounds=20,\n",
    ")\n",
    "\n",
    "# [60]\ttrain-rmse:3.18431\tvalidation-rmse:123.78980\n",
    "# [71]\ttrain-rmse:1.87073\tvalidation-rmse:123.80743"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157.790979</td>\n",
       "      <td>1.473083</td>\n",
       "      <td>170.071095</td>\n",
       "      <td>9.402711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129.446537</td>\n",
       "      <td>2.077011</td>\n",
       "      <td>154.529123</td>\n",
       "      <td>11.302401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109.085485</td>\n",
       "      <td>2.194860</td>\n",
       "      <td>143.455057</td>\n",
       "      <td>10.752623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93.441676</td>\n",
       "      <td>2.404388</td>\n",
       "      <td>136.020008</td>\n",
       "      <td>9.874314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81.071116</td>\n",
       "      <td>2.945638</td>\n",
       "      <td>133.587989</td>\n",
       "      <td>9.806164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0       157.790979        1.473083      170.071095       9.402711\n",
       "1       129.446537        2.077011      154.529123      11.302401\n",
       "2       109.085485        2.194860      143.455057      10.752623\n",
       "3        93.441676        2.404388      136.020008       9.874314\n",
       "4        81.071116        2.945638      133.587989       9.806164"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1000\n",
    "\n",
    "results = xgboost.cv(\n",
    "   params,\n",
    "   dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   early_stopping_rounds=20\n",
    ")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.57590195197679"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse = results['test-rmse-mean'].min()\n",
    "\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acessing the xgboost eval metrics via sklearn\n",
    "\n",
    "from tutorial of [xgboost](https://xgboost.readthedocs.io/en/latest/python/examples/sklearn_evals_result.html#demo-for-accessing-the-xgboost-eval-metrics-by-using-sklearn-interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "dtrain_reg = xgboost.DMatrix(X_train, y_train)\n",
    "dtest_reg = xgboost.DMatrix(X_test, y_test)\n",
    "\n",
    "params = {\"objective\": [\"reg:squarederror\"], \n",
    "          \"tree_method\": [\"hist\"]}\n",
    "\n",
    "XGB = xgboost.XGBModel(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 of 2\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Trial 1 of 2\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Execution time: 10.283693468570709 minutes\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "start_time = time.time()\n",
    "# Instantiate the regressor\n",
    "XGB = XGBRegressor()\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_estimators\": randint(100,500),\n",
    "    \"max_depth\": randint(3,100)\n",
    "}\n",
    "\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "xgb_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "xgb_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    xgb_rscv = RandomizedSearchCV(\n",
    "        XGB, # regressor\n",
    "        param_distributions = param_distribs, # hyperparameter space\n",
    "        n_iter = 10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv = inner_cv, # \"Determines the cross-validation splitting strategy\" [sklearn]\n",
    "        scoring = cv_scorer, \n",
    "        random_state = random_state, \n",
    "        verbose = 1, \n",
    "        n_jobs = n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    xgb_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    xgb_rscv_rmse1[i] = np.sqrt(-xgb_rscv.best_score_)\n",
    "    y_pred_xgb = cvp(xgb_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    xgb_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_xgb))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/ 60\n",
    "print(f\"Execution time: {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Parameters according to RSCV: {'max_depth': 82, 'n_estimators': 339}\n",
      "best score -18686.15042634845\n"
     ]
    }
   ],
   "source": [
    "print('optimal Parameters according to RSCV:', xgb_rscv.best_params_)\n",
    "print('best score' ,xgb_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "best_params = xgb_rscv.best_params_\n",
    "\n",
    "xgb_opt = XGBRegressor(**xgb_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "xgb_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "y_pred_xgb = xgb_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.86445125089867"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram-based Gradient Boosting Regression Tree\n",
    "\n",
    "*TODO*\n",
    "- parameter distribution\n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 of 2\n",
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:318: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Trial 1 of 2\n",
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages/sklearn/model_selection/_search.py:318: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Execution time: 71.69811614751816 minutes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "\n",
    "# Define parameters for HGB\n",
    "#param_distribs = {\n",
    "#    'learning_rate':randint(low=0.001,high=1),\n",
    "#    'max_iter':randint(low=5,high=250), \n",
    "#     'max_leaf_nodes': randint(low=2,high=50, scale = 1)\n",
    "#                  }\n",
    "param_distribs = {'max_iter': [5,10], \n",
    "                  'max_leaf_nodes': [15,31,40],\n",
    "                  }\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "hgb_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "hgb_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    hgb_rscv =RandomizedSearchCV(\n",
    "        HGB(), # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=1, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    hgb_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    hgb_rscv_rmse1[i] = np.sqrt(-hgb_rscv.best_score_)\n",
    "    y_pred_hgb = cvp(hgb_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    hgb_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_hgb))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"Execution time: {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Parameters according to RSCV: {'max_leaf_nodes': 31, 'max_iter': 10}\n",
      "best score -21658.686948967945\n"
     ]
    }
   ],
   "source": [
    "print('optimal Parameters according to RSCV:', hgb_rscv.best_params_)\n",
    "print('best score' ,hgb_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HGB with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141.15222233482842"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimal parameters\n",
    "hgb_opt = HGB(**hgb_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "hgb_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "\n",
    "y_pred_hgb = hgb_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_hgb = np.sqrt(mean_squared_error(y_test, y_pred_hgb))\n",
    "rmse_hgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testrun, no model is written\n"
     ]
    }
   ],
   "source": [
    "### Computational Considerations \n",
    "\n",
    "# Define the current models: \n",
    "\n",
    "model_list = [\"rf_opt\", \"pls_opt\", \"krr_opt\", \"xgb_opt\", \"hgb_opt\"]\n",
    "\n",
    "# write the models to memory: \n",
    "if save_model == True:\n",
    "    for i in model_list: \n",
    "        # Extract model name\n",
    "        model_name = i.__class__.__name__\n",
    "        # Construct a filepath\n",
    "        model_filepath = MODEL_PATH + f\"/{model_name}.pkl\"\n",
    "        # Save the model\n",
    "        joblib.dump(i, model_filepath)\n",
    "else:\n",
    "    print(\"Testrun, no model is written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models from memory\n",
    "\n",
    "for model in model_list:  \n",
    "    model_name = model.__class__.__name__  \n",
    "    model_filepath = MODEL_PATH + f\"/{model_name}.pkl\"  \n",
    "    model = joblib.load(model_filepath)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "In this section the goal is to document the packages which where used during the execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package informations\n",
    "from sklearn import show_versions\n",
    "show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_end_time = time.time()\n",
    "nb_execution_time = (nb_end_time - nb_start_time) / 60\n",
    "print(f\"Execution time: {nb_execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Get the current date\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# define the outputname\n",
    "notebook_name = '03_1_modeling_rscv.ipynb'\n",
    "output_name = f\"{notebook_name.split('.')[0]}_{date}.html\"\n",
    "\n",
    "# Function to convert the notebook to HTML\n",
    "def convert_notebook_to_html(notebook_name, output_name, RESULTS_PATH=RESULTS_PATH):\n",
    "    full_output_path = os.path.join(RESULTS_PATH, output_name)\n",
    "        # Use subprocess to call the jupyter nbconvert command\n",
    "    subprocess.call(['jupyter', 'nbconvert', '--to', 'html', output_name, '--output-dir', RESULTS_PATH])\n",
    "    \n",
    "    # Optionally, rename the output file if needed\n",
    "    # os.rename(notebook_name.split('.')[0] + '.html', full_output_path)\n",
    "\n",
    "# Wait for a short period to ensure all cells have finished executing\n",
    "time.sleep(3) # Adjust the sleep duration as needed\n",
    "\n",
    "# Convert the notebook to HTML\n",
    "convert_notebook_to_html(notebook_name, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
