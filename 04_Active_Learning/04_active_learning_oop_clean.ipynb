{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the active Learning strategies in the basic setting, while using an oop approach. The goal of this notebook is to\n",
    "* clearify the process\n",
    "* use algorithm settings accross multiple runs, without redefining them\n",
    "* create results for the main expriment for all selection criteria and model classes\n",
    "    * This goal needs to be revised, due to the computational resources\n",
    "    * Therefore the selected model classes are\n",
    "        - Random Forest, because it was used in previous study\n",
    "        - Kernel Ridge Regression, due to its fast computational time and good results in the GridSearchCrossValidation Hyperparameter Search\n",
    "        - Partial Least Squares Regression\n",
    "    * If another Model Class should be tested, XGB could be integrated, because of its provinience as a non-sklearn native regression method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Define the PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "basepath = \"../\"  # Project directory\n",
    "sys.path.append(basepath)\n",
    "# AL Scripts\n",
    "AL_SCRIPTS_PATH = basepath + \"al_lib/\"\n",
    "\n",
    "sys.path.append({AL_SCRIPTS_PATH})\n",
    "\n",
    "from al_lib.active_learning_setting import ActiveLearningPaths\n",
    "\n",
    "PATHS = ActiveLearningPaths()\n",
    "(DATA_PATH, FIGURE_PATH, ENV_PATH, RESULTS_PATH, LOG_DIR) = PATHS\n",
    "sys.path.extend(PATHS)\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the logging specifications from file 'logging_config.py'\n",
    "from al_lib.logging_config import create_logger\n",
    "import datetime\n",
    "\n",
    "# Add data/time information\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# date = now.strftime(\"%Y-%m-%d\")\n",
    "log_file_name = f\"{date}_active_learning.log\"\n",
    "log_file_path = f\"{LOG_DIR}{log_file_name}\"\n",
    "\n",
    "# Create logger\n",
    "logging = create_logger(__name__, log_file_path=log_file_path)\n",
    "# Usage of the logger as follows:\n",
    "logging.info(\"Logging started\")\n",
    "logging.info(f\"log stored at: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn of sklearn warnings\n",
    "from warnings import simplefilter\n",
    "import warnings\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*y residual is constant*.\", category=UserWarning, append=False\n",
    ")\n",
    "logging.info('Warning \"y residual is constant\" turned off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datafile\n",
    "\n",
    "data_name = \"dpsDeriv1200.csv\"\n",
    "\n",
    "datafile = DATA_PATH + data_name\n",
    "\n",
    "from al_lib.helper_functions import import_dpsDeriv1200\n",
    "\n",
    "data = import_dpsDeriv1200(datafile)\n",
    "logging.info(f\"Data loaded and preprocessed from {datafile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(100):\n",
    "    pass\n",
    "end = time.time()\n",
    "print(f\"Time taken for 100 passes: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into feature and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.select_dtypes(\"float\")\n",
    "y = data[\"year\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "since not every regression method is able to estimate its prediction accuracy, a split of the data is retained as validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of columns with std = 0.0 in X\n",
    "logging.info(f\"{(X.std() == 0.0).sum()} Columns dropped, where std = 0.0 in X\")\n",
    "\n",
    "# drop the columns with std = 0.0\n",
    "X = X.loc[:, X.std() != 0.0]\n",
    "logging.info(\n",
    "    f\"X: {X.shape},y: {y.shape} Dimensions after dropping columns with std = 0.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from al_lib.helper_functions import calc_set_sizes\n",
    "\n",
    "\n",
    "def _split_data(\n",
    "    X, y, random_state=None, test_size=None, validation_size=None, logging=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Split the data into training, test and validation set\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        assert random_state is not None, \"Random state not provided\"\n",
    "    if test_size is None:\n",
    "        test_size = 0.3\n",
    "    if validation_size is None:\n",
    "        validation_size = 0.1\n",
    "\n",
    "    # retain 10% of the data for validation\n",
    "    (\n",
    "        X_remainder,\n",
    "        X_val,\n",
    "        y_remainder,\n",
    "        y_val,\n",
    "    ) = train_test_split(X, y, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "    # split the remainder into training and test (30%) set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_remainder, y_remainder, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    calc_set_sizes(X_train, X_test, X_val, logging)\n",
    "    # assert the shapes for the sets and raise an error if they are not equal\n",
    "    assert (\n",
    "        X_train.shape[0] + X_test.shape[0] + X_val.shape[0] == X.shape[0]\n",
    "    ), \"Sum of samples in Train/Test/Validation set not equal to total samples\"\n",
    "    assert (\n",
    "        X_train.shape[1] == X_test.shape[1] == X_val.shape[1] == X.shape[1]\n",
    "    ), \"Number of features in Train/Test/Validation set not equal to total features\"\n",
    "    assert (\n",
    "        y_train.shape[0] + y_test.shape[0] + y_val.shape[0] == y.shape[0]\n",
    "    ), \"Sum of Sample-targets in Train/Test/Validation set not equal to total samples\"\n",
    "    assert (\n",
    "        X_train.shape[0] == y_train.shape[0]\n",
    "    ), \"Number of samples not equal to number of targets in Train set\"\n",
    "    assert (\n",
    "        X_test.shape[0] == y_test.shape[0]\n",
    "    ), \"Number of samples not equal to number of targets in Test set\"\n",
    "    assert (\n",
    "        X_val.shape[0] == y_val.shape[0]\n",
    "    ), \"Number of samples not equal to number of targets in Validation set\"\n",
    "\n",
    "    logging.info(f\"Shapes of Train/Test/Validation set verified\")\n",
    "    logging.info(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    logging.info(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    logging.info(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 12345\n",
    "\n",
    "validation_size = 0.1\n",
    "test_size = 0.3\n",
    "\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = _split_data(\n",
    "    X, y, random_state=random_state, logging=logging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters and Model Methods\n",
    "\n",
    "The optimal model parameters according to the CV Results are used to to fit the individual models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Regressors\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "from sklearn.cross_decomposition import PLSRegression as PLS\n",
    "from xgboost import XGBRegressor as XGB\n",
    "\n",
    "# Define the regressors\n",
    "Regressors = [RF, KRR, HGB, MLP, PLS, XGB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the rscv model parameters from 03_Modelling/03_1_rscv\n",
    "\n",
    "rscv_results_dir = basepath + \"03_Modelling/03_1_rscv/rscv_results/\"\n",
    "gscv_results_dir = basepath + \"03_Modelling/03_2_gscv/gscv_results/\"\n",
    "\n",
    "# models tested\n",
    "models = [\"hgb\", \"krr\", \"mlp\", \"pls\", \"rf\", \"xgb\"]\n",
    "models_available = {}\n",
    "optimal_params = {}\n",
    "rmse_from_cv = {}\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    try:\n",
    "        gscv_results = pd.read_csv(gscv_results_dir + f\"{model}_gscv_results.csv\")\n",
    "        # id the best parameters via min of RMSE\n",
    "        opt_run = gscv_results.loc[gscv_results[\"RMSE\"].idxmin()]\n",
    "        # extract the optimal parameters\n",
    "        optimal_params[model] = opt_run[\"params\"]\n",
    "        # convert the parameters to a dictionary\n",
    "        optimal_params[model] = eval(optimal_params[model])\n",
    "        # retrieve the optimal RMSE\n",
    "        rmse_from_cv[model] = opt_run[\"RMSE\"]\n",
    "        # convert the RMSE to a float, round, and assign integer value\n",
    "        rmse_from_cv[model] = round(float(rmse_from_cv[model]), 0)\n",
    "        models_available[model] = True\n",
    "        logging.info(\n",
    "            f\"Loaded the gscv results: {model} from {gscv_results_dir + f'{model}_gscv_results.csv'}\"\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        try:  # try to load the results of the rscv as a dataframe\n",
    "            rscv_results = pd.read_csv(rscv_results_dir + f\"{model}_rscv_results.csv\")\n",
    "            # id the best parameters via min of RMSE\n",
    "            opt_run = rscv_results.loc[rscv_results[\"RMSE\"].idxmin()]\n",
    "            # extract the optimal parameters\n",
    "            optimal_params[model] = opt_run[\"params\"]\n",
    "            # convert the parameters to a dictionary\n",
    "            optimal_params[model] = eval(optimal_params[model])\n",
    "            # retrieve the optimal RMSE\n",
    "            rmse_from_cv[model] = opt_run[\"RMSE\"]\n",
    "            # convert the RMSE to a float, round, and assign integer value\n",
    "            rmse_from_cv[model] = round(float(rmse_from_cv[model]), 0)\n",
    "            models_available[model] = True\n",
    "            logging.info(\n",
    "                f\"Loaded the rscv results: {model} from {rscv_results_dir + f'{model}_rscv_results.csv'}\"\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            models_available[model] = False\n",
    "            logging.error(\n",
    "                f\"Error loading the rscv results: {model} from {rscv_results_dir + f'{model}_rscv_results.csv'}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the model parameters which are used to perform active learning. These are important, since they potentially influence the performance of the AL-processes in an influental manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_available:\n",
    "    if models_available[model]:\n",
    "        logging.info(\n",
    "            f\"Optimal parameters (GSCV) for {model} with RMSE {rmse_from_cv[model]}: {optimal_params[model]}\"\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"Optimal parameters (RSCV) for {model} with RMSE {rmse_from_cv[model]}: {optimal_params[model]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate object with the optimized parameters to hand over to the Regressors\n",
    "for key in optimal_params.keys():\n",
    "    # generate a global variable with the optimal parameters\n",
    "    globals()[f\"params_{key}\"] = optimal_params[key]\n",
    "\n",
    "\n",
    "for key in optimal_params.keys():\n",
    "    logging.info(f\"Optimal parameters for {key}: {optimal_params[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Setup\n",
    "\n",
    "The Basic Active Learning Experiment follows the specifications:\n",
    "\n",
    "* Implementation of each Sampling Strategy in a Modular fashion\n",
    "* Selecting the inital samples randomly\n",
    "* Refitting the model after each selected sample\n",
    "* Runing the experiment n-fold with differing random states\n",
    "* Calculation of mean performance with confidence intervalls\n",
    "* Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.selection_criteria import (\n",
    "    _random_selection,\n",
    "    _gsx_selection,\n",
    "    _gsy_selection,\n",
    "    _uncertainty_selection,\n",
    "    _distance_weighing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the active Learning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.helper_functions import _validate_parameters\n",
    "from al_lib.helper_functions import _rnd_initial_sampling\n",
    "from al_lib.helper_functions import rmse_func\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "\n",
    "\n",
    "def active_learning(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    logging,\n",
    "    model_class=None,\n",
    "    model_params={},\n",
    "    selection_criterion=None,\n",
    "    n_iterations=None,\n",
    "    n_samples_per_it=None,\n",
    "    init_sample_size=None,\n",
    "    random_state=None,\n",
    "    n_jobs=None,\n",
    "    results_file=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform active learning with the given parameters.\n",
    "\n",
    "    Active Learning selects additional samples for the training set from\n",
    "    provided pool of samples. The selection is based on a selection criterion,\n",
    "    which can be selected from implemented criteria. The active\n",
    "    learning process is repeated for n_iterations. The model is retrained after\n",
    "    each iteration with the updated training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_i : pd.DataFrame\n",
    "        The features of the set (i = (train, test, val))\n",
    "    y_i : pd.Series\n",
    "        The target of the set (i = (train, test, val))\n",
    "    model_class : model class, either from sklearn or xgboost\n",
    "        The model to be used for the active learning process\n",
    "    model_params : dict\n",
    "        The parameters for the model\n",
    "    selection_criterion : function\n",
    "        The selection criterion to be used for the active learning process\n",
    "    n_iterations : int, optional (default=50)\n",
    "        The number of iterations for the active learning process\n",
    "    n_samples_per_it : int, optional (default=1)\n",
    "        The number of samples to be selected in each iteration\n",
    "    init_sample_size : int, optional (default=10)\n",
    "        The initial sample size for initial model\n",
    "    random_state : int, optional\n",
    "        The random state for the active learning process\n",
    "    n_jobs : int, optional\n",
    "        The number of kernels to be used for the active learning process\n",
    "    results_file : str, optional\n",
    "        The path to the file to store the results of the active learning process\n",
    "        If provided, the results are stored as a csv file from a pandas dataframe\n",
    "    **kwargs : dict\n",
    "        Additional keyword arguments to be used for the selection criterion\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple containing:\n",
    "        rmse_test : np.array\n",
    "            The RMSE of the model with the training set after each iteration\n",
    "        rmse_validation : np.array\n",
    "            The RMSE of the model with the validation set after each iteration\n",
    "        samples_selected : np.array\n",
    "            The samples selected in each iteration\n",
    "        rmse_full : float\n",
    "            The RMSE of the model trained with all training samples\n",
    "\n",
    "    \"\"\"\n",
    "    if \"n_fold\" in kwargs:\n",
    "        n_fold = kwargs[\"n_fold\"]\n",
    "    else:\n",
    "        n_fold = 3  # default value\n",
    "\n",
    "    # _validate_parameters(\n",
    "    #     X_train,\n",
    "    #     y_train,\n",
    "    #     model_class=None,\n",
    "    #     model_params={},\n",
    "    #     selection_criterion=None,\n",
    "    #     n_iterations=None,\n",
    "    #     n_samples_per_it=None,\n",
    "    #     init_sample_size=None,\n",
    "    # )\n",
    "\n",
    "    logging.info(f\"Size of X_train: {X_train.shape}\")\n",
    "    logging.info(f\"Size of y_train: {y_train.shape}\")\n",
    "    logging.info(f\"Model_class: {model_class}\")\n",
    "    logging.info(f\"Modelling parameters: {model_params}\")\n",
    "    logging.info(f\"Selection Criterion: {selection_criterion}\")\n",
    "    logging.info(f\"Key word arguments: {kwargs}\")\n",
    "\n",
    "    if n_samples_per_it is None:\n",
    "        n_samples_per_it = 1\n",
    "    if init_sample_size is None:\n",
    "        init_sample_size = 10\n",
    "    if n_iterations is None:\n",
    "        n_iterations = 50\n",
    "    if random_state is None:\n",
    "        random_state = 12345\n",
    "\n",
    "    # Initialize the model\n",
    "    model = model_class(**model_params)\n",
    "    # Initialize the active learning model\n",
    "    X_Pool = X_train\n",
    "    y_Pool = y_train\n",
    "\n",
    "    # prepare the output objects\n",
    "    rmse_test = np.zeros(n_iterations)\n",
    "    rmse_validation = np.zeros(n_iterations)\n",
    "    samples_selected = np.zeros(n_iterations)\n",
    "    selection_value_storage = np.zeros(n_iterations)\n",
    "\n",
    "    # initialize the learned set as a empty dataframe\n",
    "    X_Learned = pd.DataFrame()\n",
    "    y_Learned = pd.Series()\n",
    "\n",
    "    # Initialize the model\n",
    "    X_Learned, y_Learned, X_Pool, y_Pool = _rnd_initial_sampling(\n",
    "        X_Pool,\n",
    "        X_Learned,\n",
    "        y_Pool,\n",
    "        y_Learned,\n",
    "        init_sample_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    model.fit(X_Learned, y_Learned)\n",
    "\n",
    "    logging.info(f\"Initial model fitted with {init_sample_size} samples\")\n",
    "    logging.info(\"--Active Learning starts--\")\n",
    "\n",
    "    for it in range(n_iterations):\n",
    "        logging.info(f\"Active Learning with {selection_criterion} - iteration: {it}\")\n",
    "\n",
    "        y_pred_pool = model.predict(X_Pool)\n",
    "        y_pred_pool = pd.Series(y_pred_pool, index=X_Pool.index)\n",
    "\n",
    "        sample_id, selection_value = selection_criterion(\n",
    "            X_Pool=X_Pool,\n",
    "            y_Pool=y_Pool,\n",
    "            X_Learned=X_Learned,\n",
    "            y_Learned=y_Learned,\n",
    "            y_pred_pool=y_pred_pool,\n",
    "            n_fold=n_fold,\n",
    "            random_state=random_state,\n",
    "            logging=logging,\n",
    "            model=model,\n",
    "            n_jobs=n_jobs,\n",
    "            kwargs=kwargs,\n",
    "        )\n",
    "\n",
    "        samples_selected[it] = sample_id\n",
    "        selection_value_storage[it] = selection_value\n",
    "\n",
    "        # Update the Sample sets\n",
    "        x_new = X_Pool.loc[[sample_id]]\n",
    "        y_new = y_Pool.loc[[sample_id]]\n",
    "        X_Learned = pd.concat([X_Learned, x_new], ignore_index=True)\n",
    "        y_Learned = pd.concat([y_Learned, y_new], ignore_index=True)\n",
    "        X_Pool = X_Pool.drop(index=sample_id)\n",
    "        y_Pool = y_Pool.drop(index=sample_id)\n",
    "\n",
    "        # Update the Model\n",
    "        # retrain model on the new full data set and predict a new fit, if the n_samples_per_it is reached\n",
    "        if n_samples_per_it == None or n_samples_per_it == 1:\n",
    "            model.fit(X_Learned, y_Learned)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse_test[it] = rmse_func(y_test, y_pred)\n",
    "            y_pred_val = model.predict(X_val)\n",
    "            rmse_validation[it] = rmse_func(y_val, y_pred_val)\n",
    "        if it % n_samples_per_it == 0 or it != 0:\n",
    "            model.fit(X_Learned, y_Learned)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse_test[it] = rmse_func(y_test, y_pred)\n",
    "            y_pred_val = model.predict(X_val)\n",
    "            rmse_validation[it] = rmse_func(y_val, y_pred_val)\n",
    "\n",
    "    # write results into outputifle\n",
    "    if results_file is not None:\n",
    "        results = pd.DataFrame(\n",
    "            {\n",
    "                \"rmse_test\": rmse_test,\n",
    "                \"rmse_validation\": rmse_validation,\n",
    "                \"samples_selected\": samples_selected,\n",
    "                \"selection_value\": selection_value_storage,\n",
    "            }\n",
    "        )\n",
    "        results.to_csv(results_file, index=False, mode=\"a\")\n",
    "        logging.info(f\"Results written to {results_file}\")\n",
    "\n",
    "    # calc the rmse for the model including all training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse_full = rmse_func(y_test, y_pred)\n",
    "\n",
    "    # # plot the rmse over the iterations\n",
    "    # plt.plot(range(n_iterations)[1:], rmse_test[1:])\n",
    "    # # add a line for the model with all training samples\n",
    "    # plt.axhline(y=rmse_full, color=\"r\", linestyle=\"--\")\n",
    "    # selection_criterion_str = str(selection_criterion).split(\" \")[1]\n",
    "    # plt.title(\n",
    "    #     f\"RMSE over Iterations with {model_class} and\\n {selection_criterion} as selection criterion \\n {selection_criterion_str} as selection criterion\"\n",
    "    # )\n",
    "    # plt.xlabel(\"Iteration\")\n",
    "    # plt.ylabel(\"RMSE\")\n",
    "    # plt.show()\n",
    "    return (\n",
    "        rmse_test,\n",
    "        rmse_validation,\n",
    "        samples_selected,\n",
    "        selection_value_storage,\n",
    "        rmse_full,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_active_learning():\n",
    "    \"\"\"\n",
    "    Function to test the active learning function\n",
    "    \"\"\"\n",
    "    from al_lib.helper_functions import _create_test_data\n",
    "    from al_lib.helper_functions import _test_params_krr\n",
    "\n",
    "    # create test data\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = _create_test_data(logging=logging)\n",
    "    # Perform active Learning for n_iterations\n",
    "    n_iterations = 5\n",
    "    n_samples_per_it = 1\n",
    "    initial_sample_size = 10\n",
    "    model_params = _test_params_krr()\n",
    "    model_class = KRR\n",
    "    # Perform active learning\n",
    "    rmse_test, rmse_validation, samples_selected, selection_value_storage, rmse_full = (\n",
    "        active_learning(\n",
    "            X_train,\n",
    "            X_test,\n",
    "            X_val,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            y_val,\n",
    "            logging,\n",
    "            model_class=model_class,\n",
    "            model_params=model_params,\n",
    "            selection_criterion=_uncertainty_selection,\n",
    "            n_samples_per_it=n_samples_per_it,\n",
    "            n_iterations=n_iterations,\n",
    "            init_sample_size=initial_sample_size,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        rmse_test,\n",
    "        rmse_validation,\n",
    "        samples_selected,\n",
    "        selection_value_storage,\n",
    "        rmse_full,\n",
    "    )\n",
    "\n",
    "\n",
    "rmse_test, rmse_validation, samples_selected, selection_value_storage, rmse_full = (\n",
    "    test_active_learning()\n",
    ")\n",
    "\n",
    "assert (\n",
    "    rmse_test.shape[0] == rmse_validation.shape[0] == samples_selected.shape[0]\n",
    "), \"Shapes of output arrays not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform active learning twice and generate result plots\n",
    "\n",
    "\n",
    "def test_active_learning_twice():\n",
    "    \"\"\"\n",
    "    Function to test the active learning function\n",
    "    \"\"\"\n",
    "    from al_lib.helper_functions import _create_test_data\n",
    "    from al_lib.helper_functions import _test_params_krr\n",
    "\n",
    "    # create test data\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = _create_test_data(logging=logging)\n",
    "    # Perform active Learning for n_iterations\n",
    "    n_iterations = 15\n",
    "    n_samples_per_it = 1\n",
    "    initial_sample_size = 10\n",
    "    model_params = _test_params_krr()\n",
    "    model_class = KRR\n",
    "    # Perform active learning using a loop, store the results for each iteration\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "    for i in range(2):\n",
    "        random_state = 12345 + i\n",
    "\n",
    "        (\n",
    "            rmse_test,\n",
    "            rmse_validation,\n",
    "            samples_selected,\n",
    "            selection_value_storage,\n",
    "            rmse_full,\n",
    "        ) = active_learning(\n",
    "            X_train,\n",
    "            X_test,\n",
    "            X_val,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            y_val,\n",
    "            logging,\n",
    "            model_class=model_class,\n",
    "            model_params=model_params,\n",
    "            selection_criterion=_uncertainty_selection,\n",
    "            n_samples_per_it=n_samples_per_it,\n",
    "            n_iterations=n_iterations,\n",
    "            init_sample_size=initial_sample_size,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        results[f\"rmse_test_{i}\"] = rmse_test\n",
    "        results[f\"rmse_validation_{i}\"] = rmse_validation\n",
    "        results[f\"samples_selected_{i}\"] = samples_selected\n",
    "        results[f\"rmse_full_{i}\"] = rmse_full\n",
    "        results[f\"selection_value_{i}\"] = selection_value_storage\n",
    "\n",
    "    # id the range for the y axis\n",
    "    max_y_test = max([results[f\"rmse_test_{i}\"].max() for i in range(2)])\n",
    "    max_y_val = max([results[f\"rmse_validation_{i}\"].max() for i in range(2)])\n",
    "    max_y = max(max_y_test, max_y_val)\n",
    "\n",
    "    # plot the results\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot RMSE for each iteration\n",
    "    for i in range(2):\n",
    "        ax1.plot(\n",
    "            range(n_iterations), results[f\"rmse_test_{i}\"], label=f\"RMSE Sampling {i}\"\n",
    "        )\n",
    "        ax1.plot(\n",
    "            range(n_iterations),\n",
    "            results[f\"rmse_validation_{i}\"],\n",
    "            label=f\"RMSE Validation {i}\",\n",
    "        )\n",
    "        ax1.axhline(\n",
    "            y=results[f\"rmse_full_{i}\"][0],\n",
    "            color=\"r\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"RMSE Full {i}\",\n",
    "        )\n",
    "\n",
    "    ax1.set_title(\"RMSE over Iterations with KRR and Uncertainty Sampling\")\n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    ax1.set_ylabel(\"RMSE\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot selection values for each iteration\n",
    "    for i in range(2):\n",
    "        ax2.plot(\n",
    "            range(n_iterations),\n",
    "            results[f\"selection_value_{i}\"],\n",
    "            label=f\"Selection Value {i}\",\n",
    "        )\n",
    "\n",
    "    ax2.set_title(\"Selection Value for Uncertainty Sampling\")\n",
    "    ax2.set_xlabel(\"Iteration\")\n",
    "    ax2.set_ylabel(\"Selection Value\")\n",
    "    ax2.legend()\n",
    "\n",
    "    # Set the y-axis limit for the RMSE plot based on the maximum RMSE value across both iterations\n",
    "    max_y_test = max([results[f\"rmse_test_{i}\"].max() for i in range(2)])\n",
    "    max_y_val = max([results[f\"rmse_validation_{i}\"].max() for i in range(2)])\n",
    "    max_y = max(max_y_test, max_y_val)\n",
    "    ax1.set_ylim(0, max_y + 0.1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_active_learning_twice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Experiment\n",
    "\n",
    "The main experiment compares the performance of the various selection strategies statistically. To this end each selection strategy is performed multiple times for each model-class. The results can be compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the model and the model parameters for the models that are not available\n",
    "for model in models_available:\n",
    "    if models_available[model] == False:\n",
    "        # remove the model and the model parameters\n",
    "        try:\n",
    "            # remove the model from the models list\n",
    "            models.remove(model)\n",
    "        except KeyError:\n",
    "            logging.info(f\"Error deleting the parameters for model: {model}\")\n",
    "        try:\n",
    "            del globals()[f\"{model}\"]\n",
    "        except KeyError:\n",
    "            logging.info(f\"Error deleting the model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "\n",
    "models = [\"rf\", \"krr\", \"hgb\", \"pls\", \"xgb\"]\n",
    "models_list = [RF, KRR, HGB, PLS, XGB]\n",
    "\n",
    "params = [params_rf, params_krr, params_hgb, params_pls, params_xgb]\n",
    "\n",
    "model_params_list = [{model: param} for model, param in zip(models_list, params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_params_list:\n",
    "    # seperate the model class and the parameters\n",
    "    model_class = list(model.keys())[0]\n",
    "    model_params = model[model_class]\n",
    "    logging.info(f\"Model Class: {model_class}, with params: {model_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_criteria = [\n",
    "    {\n",
    "        \"criteria\": _random_selection,\n",
    "        \"crit_name\": \"random\",\n",
    "        \"kwargs\": {},\n",
    "    },  #'random_state': random_state}},\n",
    "    {\"criteria\": _gsx_selection, \"crit_name\": \"gsx\", \"kwargs\": {}},\n",
    "    {\"criteria\": _gsy_selection, \"crit_name\": \"gsy\", \"kwargs\": {}},\n",
    "    {\n",
    "        \"criteria\": _uncertainty_selection,\n",
    "        \"crit_name\": \"uncertainty\",\n",
    "        \"kwargs\": {\"n_fold\": 3},\n",
    "    },\n",
    "    {\"criteria\": _distance_weighing, \"crit_name\": \"idw\", \"kwargs\": {}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Experiment\n",
    "\n",
    "AL_RESULTS_PATH = f\"{RESULTS_PATH}al_result_tables/\"\n",
    "\n",
    "# number of active learning runs\n",
    "n_al_iterations = 10\n",
    "# Define the number of iterations for each active learning run\n",
    "n_iterations = 250\n",
    "\n",
    "# Define the number of samples to be queried in each iteration\n",
    "n_samples_per_it = 1\n",
    "\n",
    "# Define the initial sample size\n",
    "init_sample_size = 30\n",
    "\n",
    "# Define the random state\n",
    "random_state = 12345\n",
    "\n",
    "# Define the number of jobs\n",
    "n_jobs = 20\n",
    "\n",
    "# Define the output object\n",
    "\n",
    "selection_criteria = [\n",
    "    {\n",
    "        \"criteria\": _random_selection,\n",
    "        \"crit_name\": \"random\",\n",
    "        \"kwargs\": {},\n",
    "    },  #'random_state': random_state}},\n",
    "    {\"criteria\": _gsx_selection, \"crit_name\": \"gsx\", \"kwargs\": {}},\n",
    "    {\"criteria\": _gsy_selection, \"crit_name\": \"gsy\", \"kwargs\": {}},\n",
    "    {\n",
    "        \"criteria\": _uncertainty_selection,\n",
    "        \"crit_name\": \"uncertainty\",\n",
    "        \"kwargs\": {\"n_fold\": 3},\n",
    "    },\n",
    "    {\"criteria\": _distance_weighing, \"crit_name\": \"idw\", \"kwargs\": {}},\n",
    "]\n",
    "\n",
    "# perform the active learning process\n",
    "\n",
    "for model in model_params_list:\n",
    "    start = time.time()\n",
    "    model_class = list(model.keys())[0]\n",
    "    model_params = model[model_class]\n",
    "    results = pd.DataFrame()\n",
    "    logging.info(f\"Current model: {model}\")\n",
    "    for i in range(n_al_iterations):\n",
    "        random_state = random_state + i\n",
    "        logging.info(\n",
    "            f\"Active Learning iteration: {i} with random state: {random_state}\"\n",
    "        )\n",
    "        # generate the data sets from full data\n",
    "        X_train, X_test, X_val, y_train, y_test, y_val = _split_data(\n",
    "            X, y, random_state=random_state, logging=logging\n",
    "        )\n",
    "\n",
    "        for criteria in selection_criteria:\n",
    "            logging.info(\n",
    "                f\"Current criterion: {criteria['crit_name']} with kwargs: {criteria['kwargs']}\"\n",
    "            )\n",
    "            # extract the model name\n",
    "            model_name = str(model_class).split(\".\")[-1]\n",
    "            selection_criteria_name = criteria[\"crit_name\"]\n",
    "            results_file = f\"{AL_RESULTS_PATH}al_results_{models_available}_{criteria['crit_name']}.csv\"\n",
    "            kwargs = criteria.get(\"kwargs\", {})\n",
    "            (\n",
    "                rmse_test,\n",
    "                rmse_validation,\n",
    "                samples_selected,\n",
    "                selection_value_storage,\n",
    "                rmse_full,\n",
    "            ) = active_learning(\n",
    "                X_train,\n",
    "                X_test,\n",
    "                X_val,\n",
    "                y_train,\n",
    "                y_test,\n",
    "                y_val,\n",
    "                logging=logging,\n",
    "                model_class=model_class,\n",
    "                model_params=model_params,\n",
    "                selection_criterion=criteria[\"criteria\"],\n",
    "                n_iterations=n_iterations,\n",
    "                n_samples_per_it=n_samples_per_it,\n",
    "                init_sample_size=init_sample_size,\n",
    "                random_state=random_state,\n",
    "                n_jobs=n_jobs,\n",
    "                results_file=None,\n",
    "                **kwargs,\n",
    "            )\n",
    "            results[f\"rmse_test_{model_name}_{criteria['crit_name']}_{i}\"] = rmse_test\n",
    "            results[f\"rmse_val_{model_name}_{criteria['crit_name']}_{i}\"] = (\n",
    "                rmse_validation\n",
    "            )\n",
    "            results[f\"sample_sel_{model_name}_{criteria['crit_name']}_{i}\"] = (\n",
    "                samples_selected\n",
    "            )\n",
    "            results[f\"rmse_full_{model_name}_{criteria['crit_name']}_{i}\"] = rmse_full\n",
    "            results[f\"selection_value_{model_name}_{criteria['crit_name']}_{i}\"] = (\n",
    "                selection_value_storage\n",
    "            )\n",
    "    # store the results in the global variables\n",
    "    globals()[f\"{model_name}_al_results\"] = results\n",
    "    logging.info(f\"Results stored in global variable: {model_name}_al_results\")\n",
    "\n",
    "    # write the results to a csv file\n",
    "    results.to_csv(f\"{AL_RESULTS_PATH}al_results_{model_name}.csv\")\n",
    "    end = time.time()\n",
    "    logging.info(f\"Time taken for active learning with {model_name}: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the data\n",
    "\n",
    "from al_lib.results_vis import load_data\n",
    "from al_lib.results_vis import _seperate_results_test\n",
    "from al_lib.results_vis import _seperate_results_val\n",
    "from al_lib.results_vis import _plot_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "TABLES_PATH = RESULTS_PATH + \"al_result_tables/\"\n",
    "# load the data\n",
    "filename_krr = \"al_results_KernelRidge'>.csv\"\n",
    "\n",
    "data_krr = load_data(filename_krr, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_krr.columns:\n",
    "    # rename the columns\n",
    "    data_krr.rename(columns={col: col.replace(\"KernelRidge'>\", \"KRR\")}, inplace=True)\n",
    "\n",
    "data_krr.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_krr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "TABLES_PATH = RESULTS_PATH + \"al_result_tables/\"\n",
    "# load the data\n",
    "filename_pls = \"al_results_PLSRegression'>.csv\"\n",
    "\n",
    "data_pls = load_data(filename_pls, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_pls.columns:\n",
    "    # rename the columns\n",
    "    data_pls.rename(columns={col: col.replace(\"PLSRegression'>\", \"PLS\")}, inplace=True)\n",
    "\n",
    "data_pls.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_pls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# load the data\n",
    "filename_hgb = \"al_results_HistGradientBoostingRegressor'>.csv\"\n",
    "\n",
    "data_hgb = load_data(filename_hgb, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_hgb.columns:\n",
    "    # rename the columns\n",
    "    data_hgb.rename(\n",
    "        columns={col: col.replace(\"HistGradientBoostingRegressor'>\", \"HGB\")},\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "data_hgb.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_hgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# load the data\n",
    "filename_rf = \"al_results_RandomForestRegressor'>.csv\"\n",
    "\n",
    "data_rf = load_data(filename_rf, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_rf.columns:\n",
    "    # rename the columns\n",
    "    data_rf.rename(\n",
    "        columns={col: col.replace(\"RandomForestRegressor'>\", \"RF\")}, inplace=True\n",
    "    )\n",
    "\n",
    "data_rf.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# load the data\n",
    "filename_xgb = \"al_results_XGBRegressor'>.csv\"\n",
    "\n",
    "data_xgb = load_data(filename_xgb, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_xgb.columns:\n",
    "    # rename the columns\n",
    "    data_xgb.rename(columns={col: col.replace(\"XGBRegressor'>\", \"XGB\")}, inplace=True)\n",
    "\n",
    "data_xgb.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Additional Functions, necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_auc(results):  # test_rmse\n",
    "    \"\"\"\n",
    "    Calculate the Area under the curve for each selection criteria from the\n",
    "    post-processed results\n",
    "    Parameters:\n",
    "    results: list of dataframes\n",
    "    Returns:\n",
    "    aucs: list of tuples (AUC, std_rmse_mean_per_iteration, AUC_mean, auc_std)\n",
    "        each tuple contains the AUC, the standard deviation of the AUC, the mean of the AUC and the standard deviation of the RMSE\n",
    "        AUC: float, the area under the curve\n",
    "        std_rmse_mean_per_iteration: float, the standard deviation of the RMSE for each iteration\n",
    "        AUC_mean: float, the mean of the AUC\n",
    "        auc_std: float, the standard deviation of the AUC\n",
    "    \"\"\"\n",
    "    aucs = []\n",
    "    for i, rmse in enumerate(results):  # for each selection criteria\n",
    "\n",
    "        rmse_mean_per_iteration = rmse.mean(axis=1)  # axis = 1 -> row wise\n",
    "\n",
    "        # calc the std for each iteration\n",
    "        std_rmse_mean_per_iteration = rmse.std(axis=1).round(3)\n",
    "\n",
    "        # calculate the area under the curve\n",
    "        auc = sum(rmse_mean_per_iteration).__round__(1)\n",
    "\n",
    "        # calculate the mean of the AUC\n",
    "        auc_mean = np.mean(rmse_mean_per_iteration).__round__(1)\n",
    "\n",
    "        # calculate the standard deviation of the RMSE for each iteration\n",
    "        auc_std = rmse_mean_per_iteration.std().__round__(1)\n",
    "        aucs.append((auc, std_rmse_mean_per_iteration, auc_mean, auc_std))\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.results_vis import combined_auc_plot\n",
    "\n",
    "\n",
    "## main report function\n",
    "def report_al_results(results, model_name, filepath, selection_criteria):\n",
    "    test_rmse = _seperate_results_test(results=results, model_name=model_name)\n",
    "    val_rmse = _seperate_results_val(results=results, model_name=model_name)\n",
    "    # generate the test rmse plot\n",
    "    _plot_rmse(\n",
    "        test_rmse,\n",
    "        selection_criteria=selection_criteria,\n",
    "        model_name=model_name,\n",
    "        title=f\"Test RMSE for {model_name}\",\n",
    "        filepath=filepath + f\"test_rmse_{model_name}.png\",\n",
    "    )\n",
    "    # generate the validation rmse plot\n",
    "    _plot_rmse(\n",
    "        val_rmse,\n",
    "        selection_criteria=selection_criteria,\n",
    "        model_name=model_name,\n",
    "        title=f\"Validation RMSE for {model_name}\",\n",
    "        filepath=filepath + f\"val_rmse_{model_name}.png\",\n",
    "    )\n",
    "\n",
    "    # revised\n",
    "    mean_auc_test = _calculate_auc(test_rmse)\n",
    "    mean_auc_val = _calculate_auc(val_rmse)\n",
    "    combined_auc_plot(\n",
    "        mean_auc_test=mean_auc_test,\n",
    "        mean_auc_val=mean_auc_val,\n",
    "        model_name=model_name,\n",
    "        filepath=filepath,\n",
    "        selection_criteria=selection_criteria,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = FIGURE_PATH\n",
    "report_al_results(data_krr, \"KRR\", filepath, selection_criteria=selection_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_al_results(\n",
    "    data_pls, model_name=\"PLS\", filepath=filepath, selection_criteria=selection_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_al_results(\n",
    "    data_xgb, model_name=\"XGB\", filepath=filepath, selection_criteria=selection_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_al_results(\n",
    "    data_rf, model_name=\"RF\", filepath=filepath, selection_criteria=selection_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_al_results(data_hgb, \"HGB\", filepath, selection_criteria=selection_criteria)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
