{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# sys.path.clear()  # Clear all paths\n",
    "# sys.path.remove('./path/to/remove') # Remove a selected path\n",
    "\n",
    "# Basepath\n",
    "basepath = \"../\"  # Project directory\n",
    "sys.path.append(basepath)\n",
    "\n",
    "# Active Learning path\n",
    "AL_PATH = basepath + \"04_Active_Learning/\"\n",
    "\n",
    "# Data\n",
    "DATA_PATH = basepath + \"data/\"\n",
    "\n",
    "# Path to conda environment\n",
    "ENV_PATH = \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib\"\n",
    "\n",
    "# Resultspath\n",
    "RESULTS_PATH = AL_PATH + \"results/\"\n",
    "\n",
    "# Figure\n",
    "FIGURE_PATH = RESULTS_PATH + \"figures/\"\n",
    "\n",
    "# AL Scripts\n",
    "AL_SCRIPTS_PATH = basepath + \"al_scripts\"\n",
    "\n",
    "# Logging\n",
    "LOG_DIR = AL_PATH + \"logs/\"\n",
    "\n",
    "# Add the paths\n",
    "sys.path.extend(\n",
    "    {DATA_PATH, FIGURE_PATH, ENV_PATH, RESULTS_PATH, AL_SCRIPTS_PATH}\n",
    ")\n",
    "\n",
    "# remove a selected path\n",
    "# sys.path.remove('')\n",
    "\n",
    "sys.path  # Check if the path is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the logging specifications from file 'logging_config.py'\n",
    "from al_lib.logging_config import create_logger\n",
    "import datetime\n",
    "\n",
    "# Add data/time information\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the notebook name and the output name\n",
    "notebook_name = \"04_active_learning_pls.ipynb\"  # Is also used when saving the notebook\n",
    "output_name = f\"{notebook_name.split('.')[0]}_{date}.html\"\n",
    "\n",
    "# Specify logging location\n",
    "log_file_name = f\"{notebook_name.split('.')[0]}_{date}.log\"\n",
    "log_file_dir = f\"{LOG_DIR}\"\n",
    "log_file_path = f\"{LOG_DIR}/{log_file_name}\"\n",
    "# print(f\"Log file path: {log_file_path}\")\n",
    "\n",
    "# Get the logger\n",
    "# logger = None\n",
    "logging = create_logger(__name__, log_file_path=log_file_path)\n",
    "\n",
    "# Usage of the logger as follows:\n",
    "logging.info(\"Logging started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn of sklearn warnings\n",
    "from warnings import simplefilter\n",
    "import warnings\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*sklearn/cross_decomposition/_pls.py:336: UserWarning: y residual is constant*.\", category=UserWarning, append = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import PS20191107_2deriv_gegl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 2nd_deriv\n",
    "\n",
    "data_2nd_deriv_raw = pd.read_csv(\n",
    "    DATA_PATH + \"/PS20191107_2deriv_gegl.csv\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    sep=\";\",\n",
    "    decimal=\",\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# plot distribution of age in histogram\n",
    "plt.hist(data_2nd_deriv_raw[\"year\"], bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction of the dataset to exlude samples with age larger than >-5000\n",
    "\n",
    "# count the amount of samples with age larger than -5000\n",
    "logging.info(f\"Amount of samples with age older than -6000: {len(data_2nd_deriv_raw[data_2nd_deriv_raw['year'] < -6000])}\")\n",
    "# remove these samples\n",
    "data_2nd_deriv_raw = data_2nd_deriv_raw[data_2nd_deriv_raw['year'] > -6000]\n",
    "# plot distribution of age in histogram\n",
    "plt.hist(data_2nd_deriv_raw[\"year\"], bins=50)\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Distribution of age after removal of samples with year < -6000\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2nd_deriv = data_2nd_deriv_raw.rename(columns={\"Unnamed: 0\": \"Name\"})\n",
    "\n",
    "# Convert all columns of type 'object' to 'float' or 'int' if possible\n",
    "for column in data_2nd_deriv.columns:\n",
    "    # change datatype from the 'year' column to 'int\n",
    "    if column == \"year\":\n",
    "        data_2nd_deriv[column] = data_2nd_deriv[column].astype(\"int\")\n",
    "        print(f\"'{column}' has been converted to 'int'.\")\n",
    "        # skip the rest of the loop\n",
    "        continue\n",
    "    try:\n",
    "        data_2nd_deriv[column] = data_2nd_deriv[column].astype(\"float\")\n",
    "        # data_small.select_dtypes(include=['object']).astype('float')\n",
    "    except ValueError:\n",
    "        print(f\"'{column}' could not be converted. Continue with other column(s).\")\n",
    "    except TypeError:\n",
    "        print(f\"'{column}' could not be converted. Continue with other column(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dpsDeriv1200.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dps_deriv_1200 = pd.read_csv(\n",
    "    DATA_PATH + \"/dpsDeriv1200.csv\", sep=\",\", decimal=\".\", encoding=\"utf-8\"\n",
    ")\n",
    "data_dps_deriv_1200 = data_dps_deriv_1200.rename(columns=lambda x: x.replace(\"X\", \"\"))\n",
    "data_dps_deriv_1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generarl settings\n",
    "\n",
    "# Switch for the dataset\n",
    "# Select from (data_1200, data_full) or other if implemented\n",
    "data = data_dps_deriv_1200\n",
    "\n",
    "# Switch for testing mode (use only 10% of the data, among others)\n",
    "testing = True\n",
    "\n",
    "# Define a random state for randomized processes\n",
    "# random_state = np.random.RandomState(202375)\n",
    "random_state = 202375\n",
    "\n",
    "######################################################\n",
    "if testing == True:\n",
    "    n_jobs = 20\n",
    "    print(\"Testing mode for Cross Validation\")\n",
    "    print(\"consider Splitting the data for faster modelling\")\n",
    "else:\n",
    "    n_jobs = 40\n",
    "    print(\"Extensive mode for Cross Validation\")\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into feature and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.select_dtypes(\"float\")\n",
    "y = data[\"year\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "since not every regression method is able to estimate its prediction accuracy, a split of the data is retained as validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of columns with std = 0.0 in X\n",
    "logging.info(f\"Number of columns dropped, where std = 0.0 in X: {(X.std() == 0.0).sum()}\")\n",
    "\n",
    "# drop the columns with std = 0.0\n",
    "X = X.loc[:, X.std() != 0.0]\n",
    "logging.info(f\"Dimensions of X after dropping columns with std = 0.0: {X.shape}\")\n",
    "logging.info(f\"Dimensions of Y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain 10% of the data for validation\n",
    "(\n",
    "    X_remainder,\n",
    "    X_val,\n",
    "    y_remainder,\n",
    "    y_val,\n",
    ") = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
    "\n",
    "# split the remainder into training and test (30%) set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_remainder, y_remainder, test_size=0.3, random_state=random_state\n",
    ")\n",
    "logging.info(f\"Split of the dataset into Training, Test and Validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert the shapes and raise an error if they are not equal\n",
    "assert X_train.shape[0] + X_test.shape[0] + X_val.shape[0]== X.shape[0]\n",
    "assert y_train.shape[0] + y_test.shape[0] + y_val.shape[0]== y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage for each set\n",
    "\n",
    "# set the number of decimal places\n",
    "calc = lambda x: round(x / len(X), 2)\n",
    "\n",
    "# log the information\n",
    "logging.info(f\"Training set: {len(X_train)} ({calc(len(X_train))*100}%)\")\n",
    "logging.info(f\"Test set: {len(X_test)} ({calc(len(X_test))*100}%)\")\n",
    "logging.info(f\"Validation set:{len(X_val)} ({calc(len(X_val))*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Score metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# create a scorer which calculates Root Mean Squeared Error (RMSE)\n",
    "\n",
    "scoring = make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "# scoring = make_scorer(mean_squared_error, greater_is_better=False, squared=False)\n",
    "logging.info(f\"Scorer: {scoring}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of AL with IDW (Bemporad, 2023).\n",
    "\n",
    "This implementation tries to implement the IDW starting from 3.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select N samples for initial modelling via kmeans clustering\n",
    "# the distances to the cluster centers can be used to select the most\n",
    "# informative samples for the initial training set\n",
    "\n",
    "n_samples = len(X_train) // 100  # 1% of the training set\n",
    "# // floor devision - rounds to the nearest whole number\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# alternatively the number of initial clusters can be defined manually\n",
    "kmeans_clusters = n_samples\n",
    "\n",
    "# define the kmeans model\n",
    "kmeans = KMeans(n_clusters=n_samples, random_state=random_state)\n",
    "# run the kmeans model on the training set\n",
    "kmeans.fit(X_train)\n",
    "# get the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# calculate the distance of each sample to the cluster centers as\n",
    "# squared (scaled) euclidean distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distances = euclidean_distances(X_train, cluster_centers, squared=True)\n",
    "\n",
    "stepsize = 1\n",
    "\n",
    "# return the indizes of the samples that are closest to the cluster centers\n",
    "dist_sq_eu_argmin = np.argmin(distances, axis=1)\n",
    "\n",
    "# plot the distribution of the samples\n",
    "plt.hist(dist_sq_eu_argmin, bins=n_samples)\n",
    "#\n",
    "plt.title(\"Distribution of samples\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.savefig(FIGURE_PATH + \"/kmeans_cluster_distr.png\")\n",
    "plt.show()\n",
    "\n",
    "# plot the distances of the samples to the cluster centers\n",
    "plt.hist(np.min(distances, axis=1), bins=100)\n",
    "plt.title(\"Distribution of distances\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.savefig(FIGURE_PATH + \"/kmeans_cluster_dist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pca plot of the samples\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# store the explained variance ratio\n",
    "explained_variance_ratio = []\n",
    "\n",
    "for i in range(2, 14):\n",
    "    pca = PCA(n_components=i, random_state=random_state)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    explained_variance_ratio.append(pca.explained_variance_ratio_)\n",
    "    # retain the information if i == 2 for plotting 2 Dimensional PCA\n",
    "    if i == 2:\n",
    "        X_train_pca_2 = X_train_pca\n",
    "\n",
    "# calculate the sum of explained var for each pca component\n",
    "\n",
    "# show the table of explained variance ratio\n",
    "explained_variance_ratio = pd.DataFrame(explained_variance_ratio)\n",
    "\n",
    "explained_variance_ratio[\"explained_variance\"] = explained_variance_ratio.sum(axis=1)\n",
    "\n",
    "explained_variance_ratio.head(n=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the elbow plot\n",
    "\n",
    "# plot the sum of explained variance ratio against number of components\n",
    "plt.plot(range(2, 14), explained_variance_ratio[\"explained_variance\"])\n",
    "plt.title(\"Explained variance ratio by number of components\")\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Explained variance ratio\")\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distances from the cluster centers in 2D\n",
    "# be aware that the cluster centers were calculated in the original feature space\n",
    "\n",
    "plt.scatter(X_train_pca_2[:, 0], X_train_pca_2[:, 1], c=dist_sq_eu_argmin, alpha=0.5)\n",
    "# transparency of the mar\n",
    "\n",
    "# highlight the cluster centers\n",
    "plt.scatter(\n",
    "    pca.transform(cluster_centers)[:, 0], # transform the cluster centers to the pca space for pc 1\n",
    "    pca.transform(cluster_centers)[:, 1],\n",
    "    c=\"red\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"PCA plot samples and cluster centers\")\n",
    "plt.xlabel(\"Principial Component 1\")\n",
    "# reduce the size of markers on the x-axis\n",
    "plt.xticks(fontsize=8)\n",
    "plt.ylabel(\"Principial Component 2\")\n",
    "\n",
    "# add a legend explaining the color mapping\n",
    "plt.legend([\"Samples\", \"Cluster Centers\"])\n",
    "# save the plot\n",
    "plt.savefig(FIGURE_PATH + \"/pca_plot_generic.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_sq_eu_argmin is the index/name of the cluster center that is closest to each sample\n",
    "unique, counts = np.unique(dist_sq_eu_argmin, return_counts=True)\n",
    "\n",
    "# generation of a histogram over the sample distribution in the clusters\n",
    "plt.bar(unique, counts)\n",
    "plt.title(\"Number of Samples in each cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.savefig(FIGURE_PATH + \"/cluster_distribution.png\")\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the sample id with the smallest distance to any cluster center\n",
    "# the corresponding sample should be selected as the first sample\n",
    "id = np.argmin(np.min(distances, axis=1))\n",
    "id_max = np.argmax(np.max(distances, axis=1))\n",
    "print(\"Closest Sample: \", id, dist_sq_eu_argmin[id], np.min(dist_sq_eu_argmin[id]))\n",
    "print(\"Farthes Sample: \", id_max, dist_sq_eu_argmin[id_max], np.min(dist_sq_eu_argmin[id_max]))\n",
    "\n",
    "# pcaplot with the highlighted sample\n",
    "plt.scatter(X_train_pca_2[:, 0], X_train_pca_2[:, 1], c=dist_sq_eu_argmin, alpha=0.5)\n",
    "\n",
    "# highlight the cluster centers\n",
    "plt.scatter(pca.transform(cluster_centers)[:, 0], pca.transform(cluster_centers)[:, 1], c=\"red\", marker=\"x\")\n",
    "\n",
    "# highlight the selected sample\n",
    "plt.scatter(X_train_pca_2[id, 0], X_train_pca_2[id, 1], c=\"pink\", marker=\"o\")\n",
    "plt.scatter(X_train_pca_2[id_max, 0], X_train_pca_2[id_max, 1], c=\"black\", marker=\"o\")\n",
    "\n",
    "# Optional plotting possibilities\n",
    "# annotate the closest sample\n",
    "# plt.annotate(f\"Sample {id}\", (X_train_pca_2[id, 0], X_train_pca_2[id, 1]))\n",
    "# plt.annotate(f\"Sample {id_max}\", (X_train_pca_2[id_max, 0], X_train_pca_2[id_max, 1]))\n",
    "# center on the selected sample\n",
    "# plt.xlim(X_train_pca_2[id, 0] - 0.5, X_train_pca_2[id, 0] + 0.5)\n",
    "# plt.ylim(X_train_pca_2[id, 1] - 0.5, X_train_pca_2[id, 1] + 0.5)\n",
    "\n",
    "plt.title(\"PCA closest sample to cluster center\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend([\"Samples\", \"Cluster Center\", f\"Closest Sample (ID:{id})\", f\"Farthest Sample (ID:{id_max})\"])\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(FIGURE_PATH + \"/pca_plot_highligh_MinMaxDist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive the indizes of the samples that are closest to each cluster center\n",
    "kmeans_clusters = n_samples\n",
    "\n",
    "id = []\n",
    "initial_samples_sample_ids = []\n",
    "for i in range(kmeans_clusters):\n",
    "    id.append(np.where(dist_sq_eu_argmin == i)[0])\n",
    "\n",
    "# get the first sample of each cluster\n",
    "for i in range(kmeans_clusters):\n",
    "    initial_samples_sample_ids.append(id[i][0])\n",
    "initial_samples_sample_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning step (first step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "\n",
    "For each Active Learning we will start with an inital set of samples which allow to generate a model. Based on the employed strategy, additional samples will be added and the model updated. \n",
    "\n",
    "**Inital Models**  \n",
    "[X] random samples  \n",
    "[X] kmean cluster centers  \n",
    "\n",
    "**Sampling strategies**  \n",
    "[X] random sampling  \n",
    "[ ] euclidean distance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The Active Learning Process in my Workflow recieves the hyperparameters for the model inplementation from the previous process (RSCV and GSCV). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import optimal model parameters from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 03_1_rscv/rscv_results import pls_rscv_results.csv\n",
    "\n",
    "# Load the results from the RandomizedSearchCV\n",
    "pls_rscv_results = pd.read_csv(\"/home/fhwn.ac.at/202375/Zamberger_thesis_AL2024/03_Modelling/03_1_rscv/rscv_results/pls_rscv_results.csv\",\n",
    "    sep=\",\",\n",
    "    decimal=\".\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "# identify the best parameters via the rmse\n",
    "params_pls = eval(pls_rscv_results.loc[pls_rscv_results[\"RMSE\"].idxmin()].params)\n",
    "params_pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive the best parameters as a dictionary\n",
    "\n",
    "# params_pls = {'n_components': 45, 'max_iter':321}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inital model with random samples and pandas dataframes\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from al_lib.helper_functions import rmse_func\n",
    "\n",
    "pls = PLSRegression(**params_pls)\n",
    "\n",
    "X_Pool = X_train\n",
    "y_Pool = y_train\n",
    "n_samples = 8\n",
    "\n",
    "X_Learned = None\n",
    "y_Learned = None\n",
    "\n",
    "X_Pool, X_Learned = train_test_split(\n",
    "    X_Pool, test_size=n_samples, random_state=random_state\n",
    ")\n",
    "y_Pool, y_Learned = train_test_split(\n",
    "    y_Pool, test_size=n_samples, random_state=random_state\n",
    ")\n",
    "\n",
    "pls.fit(X_Learned, y_Learned)\n",
    "# calc initial rmse\n",
    "y_pred = pls.predict(X_test)\n",
    "rmse = rmse_func(y_test, y_pred)\n",
    "print(f\"Initial RMSE with random sampling: {rmse}, with {len(y_Learned)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning with random sampling\n",
    "\n",
    "To the initial model, additional samples will be added in a randomized way. This will be used later as baseline to compare strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active Learning (PLS) with random sampling\n",
    "\n",
    "pls = PLSRegression(**params_pls)\n",
    "\n",
    "X_Pool = X_train\n",
    "y_Pool = y_train\n",
    "n_samples = 8\n",
    "\n",
    "X_Learned = None\n",
    "y_Learned = None\n",
    "\n",
    "X_Pool, X_Learned, y_Pool, y_Learned = train_test_split(\n",
    "    X_Pool, y_Pool, test_size=n_samples, random_state=random_state\n",
    ")\n",
    "\n",
    "pls.fit(X_Learned, y_Learned)\n",
    "# calc initial rmse\n",
    "y_pred = pls.predict(X_test)\n",
    "rmse = rmse_func(y_test, y_pred)\n",
    "print(f\"Initial RMSE with random sampling: {rmse}, with {len(y_Learned)} samples\")\n",
    "\n",
    "# define the number of iterations\n",
    "n_iterations = 500\n",
    "\n",
    "# define the number of samples to be selected in each iteration\n",
    "n_samples_per_iteration = 1\n",
    "\n",
    "# to track the samples, we will generate lists, with all the indexes\n",
    "\n",
    "X_Learned_index = X_Learned.index\n",
    "# these should be the same as the y_Learned index\n",
    "y_Learned_index = y_Learned.index\n",
    "X_Pool_index = X_Pool.index\n",
    "y_Pool_index = y_Pool.index\n",
    "\n",
    "rmse_sampling = np.zeros(n_iterations)\n",
    "\n",
    "# Active Learning Loop\n",
    "for it in range(n_iterations):\n",
    "    # select a random sample from the pool by selecting a random index from X_Pool\n",
    "    random_sample_index = np.random.choice(X_Pool.index)\n",
    "\n",
    "    x_new = X_Pool.loc[[random_sample_index]]\n",
    "    y_new = y_Pool.loc[[random_sample_index]]\n",
    "    # add the sample to the learned set\n",
    "    X_Learned = pd.concat([X_Learned, x_new])\n",
    "    y_Learned = pd.concat([y_Learned, y_new])\n",
    "    # remove the sample from the pool\n",
    "    X_Pool = X_Pool.drop(index=random_sample_index)\n",
    "    y_Pool = y_Pool.drop(index=random_sample_index)\n",
    "    # retrain model on the new full data set and predict a new fit\n",
    "    pls.fit(X_Learned, y_Learned)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    rmse_sampling[it] = rmse_func(y_test, y_pred)\n",
    "    # print(f\"RMSE after iteration {it+1}: {rmse_sampling[it]}, with {len(y_Learned)} samples\")\n",
    "\n",
    "print(\n",
    "    f\"Final RMSE with random sampling: {rmse_sampling[-1]}, with {len(y_Learned)} samples\"\n",
    ")\n",
    "rmse_sampling_random_start = rmse_sampling\n",
    "\n",
    "# plot the rmse over the iterations\n",
    "plt.plot(range(n_iterations), rmse_sampling)\n",
    "plt.title(\n",
    "    \"Random Sampling with Random Samples as starting points \\n RMSE over iterations\"\n",
    ")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active Learning (PLS) with Cluster Centers as starting points\n",
    "\n",
    "pls = PLSRegression(**params_pls)\n",
    "\n",
    "X_Pool = X_train\n",
    "y_Pool = y_train\n",
    "\n",
    "X_Learned = None\n",
    "y_Learned = None\n",
    "\n",
    "# add the samples nearest to cluster centers to the learned set\n",
    "X_Learned = X_Pool.iloc[initial_samples_sample_ids]\n",
    "y_Learned = y_Pool.iloc[initial_samples_sample_ids]\n",
    "\n",
    "# remove the samples from the pool\n",
    "X_Pool = X_Pool.drop(X_Learned.index)\n",
    "y_Pool = y_Pool.drop(y_Learned.index)\n",
    "\n",
    "pls.fit(X_Learned, y_Learned)\n",
    "# calc initial rmse\n",
    "y_pred = pls.predict(X_test)\n",
    "rmse_init = rmse_func(y_test, y_pred)\n",
    "print(f\"Initial RMSE with random sampling: {rmse_init}, with {len(y_Learned)} samples\")\n",
    "\n",
    "# define the number of iterations\n",
    "n_iterations = 500\n",
    "\n",
    "# define the number of samples to be selected in each iteration\n",
    "n_samples_per_iteration = 1\n",
    "\n",
    "# to track the samples, we will generate lists, with all the indexes\n",
    "X_Learned_index = X_Learned.index\n",
    "y_Learned_index = y_Learned.index\n",
    "X_Pool_index = X_Pool.index\n",
    "y_Pool_index = y_Pool.index\n",
    "\n",
    "rmse_sampling = np.zeros(n_iterations)\n",
    "\n",
    "# Active Learning Loop\n",
    "for it in range(n_iterations):\n",
    "    # select a random sample from the pool by selecting a random index from X_Pool\n",
    "    random_sample_index = np.random.choice(X_Pool.index)\n",
    "    x_new = X_Pool.loc[[random_sample_index]]\n",
    "    y_new = y_Pool.loc[[random_sample_index]]\n",
    "    # add the sample to the learned set\n",
    "    X_Learned = pd.concat([X_Learned, x_new])\n",
    "    y_Learned = pd.concat([y_Learned, y_new])\n",
    "    # remove the sample from the pool\n",
    "    X_Pool = X_Pool.drop(index=random_sample_index)\n",
    "    y_Pool = y_Pool.drop(index=random_sample_index)\n",
    "    # retrain model on the new full data set and predict a new fit\n",
    "    pls.fit(X_Learned, y_Learned)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    rmse_sampling[it] = rmse_func(y_test, y_pred)\n",
    "    # print(f\"RMSE after iteration {it+1}: {rmse_sampling[it]}, with {len(y_Learned)} samples\")\n",
    "\n",
    "print(\n",
    "    f\"Final RMSE with random sampling: {rmse_sampling[-1]}, with {len(y_Learned)} samples\"\n",
    ")\n",
    "\n",
    "rmse_sampling_cluster_start = rmse_sampling\n",
    "\n",
    "# plot the rmse over the iterations\n",
    "plt.plot(range(n_iterations), rmse_sampling)\n",
    "plt.title(\n",
    "    \"Random Sampling with ClusterCenters as starting points \\n RMSE over iterations\"\n",
    ")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two plots\n",
    "# plot the rmse over the iterations\n",
    "plt.plot(range(n_iterations), rmse_sampling_cluster_start)\n",
    "plt.plot(range(n_iterations), rmse_sampling_random_start)\n",
    "plt.title(\n",
    "    \"Comparison Random vs Cluster start (with random sampling) \\n RMSE over iterations\"\n",
    ")\n",
    "plt.legend([\"Cluster Start\", \"Random Start\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning with Random sampling - Defining a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLS regression with random sampling as selection criterion\n",
    "\n",
    "def pls_random(X_train, y_train, n_iterations, params_pls, n_samples_per_it = None):\n",
    "    \"\"\"\n",
    "    Function to perform PLS regression with random sampling as selection criterion\n",
    "    \"\"\"\n",
    "    if n_samples_per_it is None:\n",
    "        n_samples_per_it = 1\n",
    "    # initialize the model\n",
    "    # Define the Input\n",
    "    # Data\n",
    "    X_Pool = X_train\n",
    "    y_Pool = y_train\n",
    "    # Model\n",
    "    pls = PLSRegression(**params_pls)\n",
    "\n",
    "    # Number of iterations of active learning\n",
    "    if n_iterations is None:\n",
    "        n_iterations = 50\n",
    "        \n",
    "    # Define the Output\n",
    "    # RMSE for each iteration\n",
    "    # Index of the samples selected in each iteration\n",
    "\n",
    "    rmse_sampling = np.zeros(n_iterations)\n",
    "    samples_selected = np.zeros(n_iterations)\n",
    "\n",
    "    # Active Learning Loop\n",
    "    for it in range(n_iterations):\n",
    "        # select a random sample from the pool by selecting a random index from X_Pool\n",
    "        random_sample_index = np.random.choice(X_Pool.index)\n",
    "        samples_selected[it] = random_sample_index\n",
    "\n",
    "        x_new = X_Pool.loc[[random_sample_index]]\n",
    "        y_new = y_Pool.loc[[random_sample_index]]\n",
    "\n",
    "        # if it = 0, initialize the learned set as empty\n",
    "        if it == 0:\n",
    "            X_Learned = pd.DataFrame()\n",
    "            y_Learned = pd.DataFrame()\n",
    "\n",
    "        # add the sample to the learned set\n",
    "        X_Learned = pd.concat([X_Learned, x_new])\n",
    "        y_Learned = pd.concat([y_Learned, y_new])\n",
    "        # remove the sample from the pool\n",
    "        X_Pool = X_Pool.drop(index=random_sample_index)\n",
    "        y_Pool = y_Pool.drop(index=random_sample_index)\n",
    "        # retrain model on the new full data set and predict a new fit, if the n_samples_per_it is reached\n",
    "        if n_samples_per_it == None:\n",
    "            pls.fit(X_Learned, y_Learned)\n",
    "            y_pred = pls.predict(X_test)\n",
    "            rmse_sampling[it] = rmse_func(y_test, y_pred)\n",
    "            # print(f\"RMSE after iteration {it+1}: {rmse_sampling[it]}, with {len(y_Learned)} samples\")\n",
    "        if it % n_samples_per_it == 0 and it != 0:\n",
    "            pls.fit(X_Learned, y_Learned)\n",
    "            y_pred = pls.predict(X_test)\n",
    "            rmse_sampling[it] = rmse_func(y_test, y_pred)\n",
    "            # print(f\"RMSE after iteration {it+1}: {rmse_sampling[it]}, with {len(y_Learned)} samples\")\n",
    "        # print(f\"RMSE after iteration {it+1}: {rmse_sampling[it]}, with {len(y_Learned)} samples\")\n",
    "\n",
    "    print(\n",
    "        f\"Final RMSE with random sampling: {round(rmse_sampling[-1], 3)}, with {len(y_Learned)} samples\"\n",
    "    )\n",
    "\n",
    "    # calc the rmse for the model including all training data\n",
    "    pls.fit(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    rmse_full = rmse_func(y_test, y_pred)\n",
    "    print(f\"RMSE with all training samples: {round(rmse_full, 3)} (with training on {len(y_train)} samples)\")\n",
    "\n",
    "    # plot the rmse over the iterations\n",
    "    plt.plot(range(n_iterations)[1:], rmse_sampling[1:])\n",
    "    # add a line for the model with all training samples\n",
    "    plt.axhline(y=rmse_full, color=\"r\", linestyle=\"--\")\n",
    "    plt.title(\n",
    "        \"Random Sampling with PLS Model\\n Random Samples as starting points \\n RMSE over iterations\"\n",
    "    )\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.show()\n",
    "\n",
    "    # create a plot of the rmse as accuracy over the iterations\n",
    "\n",
    "    #define max acc as rmse_full\n",
    "    max_acc = rmse_full\n",
    "    # calculate the accuracy for each iteration\n",
    "    rmse_acc = max_acc - rmse_sampling\n",
    "    # plot the accuracy over the iterations\n",
    "    plt.plot(range(n_iterations)[1:], rmse_acc[1:])\n",
    "    plt.axhline(y=0, color=\"r\", linestyle=\"--\") # rmse_full == max_acc\n",
    "    plt.xlim(1, n_iterations)\n",
    "    plt.ylim(np.min(rmse_acc) ,0+10)\n",
    "    plt.title(\n",
    "        \"Random Sampling with PLS Model\\n Random Samples as starting points \\n RMSE over iterations\"\n",
    "    )\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "    # return rmse_sampling, samples_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_random(X_Pool, y_Pool, 500, params_pls = params_pls) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSx\n",
    "\n",
    "The greedy sampling technique can be described as a max distance sampling. Therfor select samples which are most distant from each sample of the test set. The index specifies the 'direction' in which the samples are measured, X refers to the known Variables and y to the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLS regression with GSx as selection criterion\n",
    "\n",
    "# Number of iterations of active learning\n",
    "n_iterations = 500\n",
    "\n",
    "def pls_gsx(X_train, y_train, params_pls, n_iterations = None, n_samples_per_it = None, init_sample_size = None):\n",
    "    \"\"\"\n",
    "    Function to perform PLS regression with GSx as selection criterion\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    if n_samples_per_it is None:\n",
    "        n_samples_per_it = 1\n",
    "    if init_sample_size is None:\n",
    "        init_sample_size = 10\n",
    "    if n_iterations is None:\n",
    "        n_iterations = 50\n",
    "    # initialize the model\n",
    "    # Define the Input\n",
    "    # Data\n",
    "    X_Pool = X_train\n",
    "    y_Pool = y_train\n",
    "    # Model\n",
    "    pls = PLSRegression(**params_pls)\n",
    "\n",
    "    # Define the Output\n",
    "    # RMSE for each iteration\n",
    "    # Index of the sample selected (in each iteration)\n",
    "\n",
    "    rmse_sampling = np.zeros(n_iterations)\n",
    "    samples_selected = np.zeros(n_iterations)\n",
    "    #initialize the learned set as a empty dataframe\n",
    "    X_Learned = pd.DataFrame()\n",
    "    y_Learned = pd.Series()\n",
    "    # add initial samples to the learned set (random)\n",
    "    for _ in range(init_sample_size):\n",
    "        random_sample_index = np.random.choice(X_Pool.index)\n",
    "        x_new = X_Pool.loc[[random_sample_index]]\n",
    "        y_new = y_Pool.loc[[random_sample_index]]\n",
    "        X_Learned = pd.concat([X_Learned, x_new], ignore_index=True)\n",
    "        y_Learned = pd.concat([y_Learned, y_new], ignore_index=True)\n",
    "        X_Pool = X_Pool.drop(index=random_sample_index)\n",
    "        y_Pool = y_Pool.drop(index=random_sample_index)\n",
    "    assert all (y_Learned.index == X_Learned.index)\n",
    "\n",
    "    # Active Learning Loop\n",
    "    for it in range(n_iterations):\n",
    "        # Greedy Sampling by Euclidean Distance\n",
    "        # select the sample from X_Pool, where the euclidean distance to the samples in X_Pool is the largest\n",
    "        # this is done by calculating the euclidean distance between the samples in X_Pool and the samples in X_Learned\n",
    "        # the sample with the largest distance is selected\n",
    "        distances = euclidean_distances(X_Pool, X_Learned) # distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
    "        distances_df = pd.DataFrame(distances, index=X_Pool.index, columns=X_Learned.index)\n",
    "        # select the sample with the largest distance\n",
    "        sample_id = distances_df.sum(axis = 1).idxmax()\n",
    "        samples_selected[it] = sample_id\n",
    "        #retrieve the sample from the pool\n",
    "        x_new = X_Pool.loc[[sample_id]]\n",
    "        y_new = y_Pool.loc[[sample_id]]\n",
    "       \n",
    "        # add the sample to the learned set\n",
    "        X_Learned = pd.concat([X_Learned, x_new],ignore_index=True)\n",
    "        y_Learned = pd.concat([y_Learned, y_new],ignore_index=True)\n",
    "        # remove the sample from the pool\n",
    "        X_Pool = X_Pool.drop(index=sample_id)\n",
    "        y_Pool = y_Pool.drop(index=sample_id)\n",
    "        # retrain model on the new full data set and predict a new fit, if the n_samples_per_it is reached\n",
    "        if n_samples_per_it == None:\n",
    "            pls.fit(X_Learned, y_Learned)\n",
    "            y_pred = pls.predict(X_test)\n",
    "            rmse_sampling[it] = rmse_func(y_test, y_pred)\n",
    "            # print(f\"RMSE after iteration {it+1}: {rmse_sampling[it]}, with {len(y_Learned)} samples\")\n",
    "        if it % n_samples_per_it == 0 and it != 0:\n",
    "            pls.fit(X_Learned, y_Learned)\n",
    "            y_pred = pls.predict(X_test)\n",
    "            rmse_sampling[it] = rmse_func(y_test, y_pred)\n",
    "            # print(f\"RMSE after iteration {it+1}: {rmse_sampling[it]}, with {len(y_Learned)} samples\")\n",
    "        # print(f\"RMSE after iteration {it+1}: {rmse_sampling[it]}, with {len(y_Learned)} samples\")\n",
    "\n",
    "    print(\n",
    "        f\"Final RMSE with GSx sampling: {round(rmse_sampling[-1], 3)}, with {len(y_Learned)} samples\"\n",
    "    )\n",
    "    \n",
    "    gsx_rmse_sampling = rmse_sampling\n",
    "    # calc the rmse for the model including all training data\n",
    "    pls.fit(X_train, y_train)\n",
    "    y_pred = pls.predict(X_test)\n",
    "    rmse_full = rmse_func(y_test, y_pred)\n",
    "    print(f\"RMSE with all training samples: {round(rmse_full, 3)} (with training on {len(y_train)} samples)\")\n",
    "\n",
    "    # plot the rmse over the iterations\n",
    "    plt.plot(range(n_iterations)[1:], rmse_sampling[1:])\n",
    "    # add a line for the model with all training samples\n",
    "    plt.axhline(y=rmse_full, color=\"r\", linestyle=\"--\")\n",
    "    plt.title(\n",
    "        \"GSx Sampling with PLS Model\\n Random Samples as starting points \\n RMSE over iterations\"\n",
    "    )\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.show()\n",
    "    return gsx_rmse_sampling\n",
    "        \n",
    "gsx_rmse_sampling = pls_gsx(X_Pool, y_Pool, params_pls = params_pls, n_iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random sampling with GSx sampling\n",
    "\n",
    "# plot the rmse over the iterations\n",
    "plt.plot(range(n_iterations), rmse_sampling_cluster_start)\n",
    "plt.plot(range(n_iterations), rmse_sampling_random_start)\n",
    "plt.plot(range(n_iterations), gsx_rmse_sampling)\n",
    "plt.title(\n",
    "    \"Comparison Random vs Cluster start (with random sampling) \\n RMSE over iterations\"\n",
    ")\n",
    "plt.legend([\"Random(Cluster Start)\", \"Random(Random Start)\", \"GSx\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
