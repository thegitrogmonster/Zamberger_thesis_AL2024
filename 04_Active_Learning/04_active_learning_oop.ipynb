{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the active Learning strategies in the basic setting, while using an oop approach. The goal of this notebook is to\n",
    "* clearify the process\n",
    "* use algorithm settings accross multiple runs, without redefining them\n",
    "* create results for the main expriment for all selection criteria and model classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Define the PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "basepath = \"../\"  # Project directory\n",
    "sys.path.append(basepath)\n",
    "# AL Scripts\n",
    "AL_SCRIPTS_PATH = basepath + \"al_lib/\"\n",
    "\n",
    "sys.path.append({AL_SCRIPTS_PATH})\n",
    "\n",
    "from al_lib.active_learning_setting import ActiveLearningPaths\n",
    "\n",
    "PATHS = ActiveLearningPaths()\n",
    "(DATA_PATH, FIGURE_PATH, ENV_PATH, RESULTS_PATH, LOG_DIR) = PATHS\n",
    "sys.path.extend(PATHS)\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the logging specifications from file 'logging_config.py'\n",
    "from al_lib.logging_config import create_logger\n",
    "import datetime\n",
    "\n",
    "# Add data/time information\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# date = now.strftime(\"%Y-%m-%d\")\n",
    "log_file_name = f\"{date}_active_learning.log\"\n",
    "log_file_path = f\"{LOG_DIR}{log_file_name}\"\n",
    "\n",
    "# Create logger\n",
    "logging = create_logger(__name__, log_file_path = log_file_path)\n",
    "# Usage of the logger as follows:\n",
    "logging.info(\"Logging started\")\n",
    "logging.info(f\"log stored at: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn of sklearn warnings\n",
    "from warnings import simplefilter\n",
    "import warnings\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*y residual is constant*.\", category=UserWarning, append=False\n",
    ")\n",
    "# logging.warning(\"Warning \\\"y residual is constant\\\" turned off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datafile\n",
    "\n",
    "data_name = \"dpsDeriv1200.csv\"\n",
    "\n",
    "datafile = DATA_PATH + data_name\n",
    "\n",
    "from al_lib.helper_functions import import_dpsDeriv1200\n",
    "import pandas as pd\n",
    "\n",
    "data = import_dpsDeriv1200(datafile)\n",
    "logging.info(f\"Data loaded and preprocessed from {datafile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into feature and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.select_dtypes(\"float\")\n",
    "y = data[\"year\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "since not every regression method is able to estimate its prediction accuracy, a split of the data is retained as validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of columns with std = 0.0 in X\n",
    "logging.info(\n",
    "    f\"{(X.std() == 0.0).sum()} Columns dropped, where std = 0.0 in X\"\n",
    ")\n",
    "\n",
    "# drop the columns with std = 0.0\n",
    "X = X.loc[:, X.std() != 0.0]\n",
    "logging.info(\n",
    "    f\"X: {X.shape},y: {y.shape} Dimensions after dropping columns with std = 0.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational Settings\n",
    "random_state = 12345\n",
    "\n",
    "validation_size = 0.1\n",
    "test_size = 0.3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# retain 10% of the data for validation\n",
    "(\n",
    "    X_remainder,\n",
    "    X_val,\n",
    "    y_remainder,\n",
    "    y_val,\n",
    ") = train_test_split(X, y, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "# split the remainder into training and test (30%) set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_remainder, y_remainder, test_size=test_size, random_state=random_state\n",
    ")\n",
    "logging.info(f\"Split of the dataset into Train/Test/Validation set\")\n",
    "\n",
    "# assert the shapes for the sets and raise an error if they are not equal\n",
    "assert X_train.shape[0] + X_test.shape[0] + X_val.shape[0] == X.shape[0], \"Sum of samples in Train/Test/Validation set not equal to total samples\"\n",
    "assert X_train.shape[1] == X_test.shape[1] == X_val.shape[1] == X.shape[1], \"Number of features in Train/Test/Validation set not equal to total features\"\n",
    "assert y_train.shape[0] + y_test.shape[0] + y_val.shape[0] == y.shape[0], \"Sum of Sample-targets in Train/Test/Validation set not equal to total samples\"\n",
    "assert X_train.shape[0] == y_train.shape[0], \"Number of samples not equal to number of targets in Train set\"\n",
    "assert X_test.shape[0] == y_test.shape[0], \"Number of samples not equal to number of targets in Test set\"\n",
    "assert X_val.shape[0] == y_val.shape[0], \"Number of samples not equal to number of targets in Validation set\"\n",
    "\n",
    "logging.info(f\"Shapes of Train/Test/Validation set verified\")\n",
    "logging.info(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.helper_functions import calc_set_sizes\n",
    "\n",
    "calc_set_sizes(X_train , X_test, X_val, logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters and Model Methods\n",
    "\n",
    "The optimal model parameters according to the CV Results are used to to fit the individual models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Regressors\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "from sklearn.cross_decomposition import PLSRegression as PLS\n",
    "from xgboost import XGBRegressor as XGB\n",
    "\n",
    "# Define the regressors\n",
    "Regressors = [RF, KRR, HGB, MLP, PLS, XGB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the rscv model parameters from 03_Modelling/03_1_rscv\n",
    "\n",
    "rscv_results_dir = basepath + \"03_Modelling/03_1_rscv/rscv_results/\"\n",
    "gscv_results_dir = basepath + \"03_Modelling/03_2_gscv/gscv_results/\"\n",
    "\n",
    "# models tested\n",
    "models = [\"hgb\", \"krr\", \"mlp\", \"pls\", \"rf\", \"xgb\"]\n",
    "models_available = {}\n",
    "optimal_params = {}\n",
    "rmse_from_cv = {}\n",
    "\n",
    "for model in models:\n",
    "    try: \n",
    "        gscv_results = pd.read_csv(gscv_results_dir + f\"{model}_gscv_results.csv\")\n",
    "        # id the best parameters via min of RMSE\n",
    "        opt_run = gscv_results.loc[gscv_results[\"RMSE\"].idxmin()]\n",
    "        # extract the optimal parameters\n",
    "        optimal_params[model] = opt_run[\"params\"]\n",
    "        # convert the parameters to a dictionary\n",
    "        optimal_params[model] = eval(optimal_params[model])\n",
    "        # retrieve the optimal RMSE\n",
    "        rmse_from_cv[model] = opt_run[\"RMSE\"]\n",
    "        # convert the RMSE to a float, round, and assign integer value\n",
    "        rmse_from_cv[model] = round(float(rmse_from_cv[model]), 0)\n",
    "        models_available[model] = True\n",
    "        logging.info(f\"Loaded the gscv results: {model} from {gscv_results_dir + f'{model}_gscv_results.csv'}\")\n",
    "    except FileNotFoundError:\n",
    "        try: # try to load the results of the rscv as a dataframe\n",
    "            rscv_results = pd.read_csv(rscv_results_dir + f\"{model}_rscv_results.csv\")\n",
    "            # id the best parameters via min of RMSE\n",
    "            opt_run = rscv_results.loc[rscv_results[\"RMSE\"].idxmin()]\n",
    "            # extract the optimal parameters\n",
    "            optimal_params[model] = opt_run[\"params\"]\n",
    "            # convert the parameters to a dictionary\n",
    "            optimal_params[model] = eval(optimal_params[model])\n",
    "            # retrieve the optimal RMSE\n",
    "            rmse_from_cv[model] = opt_run[\"RMSE\"]\n",
    "            # convert the RMSE to a float, round, and assign integer value\n",
    "            rmse_from_cv[model] = round(float(rmse_from_cv[model]), 0)\n",
    "            models_available[model] = True\n",
    "            logging.info(f\"Loaded the rscv results: {model} from {rscv_results_dir + f'{model}_rscv_results.csv'}\")\n",
    "        except FileNotFoundError:\n",
    "            models_available[model] = False\n",
    "            logging.error(f\"Error loading the rscv results: {model} from {rscv_results_dir + f'{model}_rscv_results.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the model parameters which are used to perform active learning. These are important, since they potentially influence the performance of the AL-processes in an influental manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_available:\n",
    "    if models_available[model]:\n",
    "        logging.info(f\"Optimal parameters (GSCV) for {model} with RMSE {rmse_from_cv[model]}: {optimal_params[model]}\")\n",
    "    else:\n",
    "        logging.info(f\"Optimal parameters (RSCV) for {model} with RMSE {rmse_from_cv[model]}: {optimal_params[model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate object with the optimized parameters to hand over to the Regressors\n",
    "for key in optimal_params.keys():\n",
    "    # generate a global variable with the optimal parameters\n",
    "    globals()[f\"params_{key}\"] = optimal_params[key]\n",
    "\n",
    "\n",
    "for key in optimal_params.keys():\n",
    "    logging.info(f\"Optimal parameters for {key}: {optimal_params[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Setup\n",
    "\n",
    "The Basic Active Learning Experiment follows the specifications:\n",
    "\n",
    "* Implementation of each Sampling Strategy in a Modular fashion\n",
    "* Selecting the inital samples randomly\n",
    "* Refitting the model after each selected sample\n",
    "* Runing the experiment n-fold with differing random states\n",
    "* Calculation of mean performance with confidence intervalls\n",
    "* Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.selection_criteria import (_random_selection,\n",
    "                                       _gsx_selection, \n",
    "                                       _gsy_selection, \n",
    "                                       _uncertainty_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the active Learning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.helper_functions import _validate_parameters\n",
    "from al_lib.helper_functions import _rnd_initial_sampling\n",
    "from al_lib.helper_functions import rmse_func\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "\n",
    "def active_learning(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    logging,\n",
    "    model_class=None,\n",
    "    model_params={},\n",
    "    selection_criterion=None,\n",
    "    n_iterations=None,\n",
    "    n_samples_per_it=None,\n",
    "    init_sample_size=None,\n",
    "    random_state=None,\n",
    "    n_jobs=None,\n",
    "    results_file=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform active learning with the given parameters. \n",
    "    \n",
    "    Active Learning selects additional samples for the training set from \n",
    "    provided pool of samples. The selection is based on a selection criterion, \n",
    "    which can be selected from implemented criteria. The active \n",
    "    learning process is repeated for n_iterations. The model is retrained after\n",
    "    each iteration with the updated training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_i : pd.DataFrame\n",
    "        The features of the set (i = (train, test, val))\n",
    "    y_i : pd.Series\n",
    "        The target of the set (i = (train, test, val))\n",
    "    model_class : model class, either from sklearn or xgboost\n",
    "        The model to be used for the active learning process\n",
    "    model_params : dict\n",
    "        The parameters for the model\n",
    "    selection_criterion : function\n",
    "        The selection criterion to be used for the active learning process\n",
    "    n_iterations : int, optional (default=50)\n",
    "        The number of iterations for the active learning process\n",
    "    n_samples_per_it : int, optional (default=1)\n",
    "        The number of samples to be selected in each iteration\n",
    "    init_sample_size : int, optional (default=10)\n",
    "        The initial sample size for initial model\n",
    "    random_state : int, optional\n",
    "        The random state for the active learning process\n",
    "    n_jobs : int, optional\n",
    "        The number of kernels to be used for the active learning process\n",
    "    results_file : str, optional\n",
    "        The path to the file to store the results of the active learning process\n",
    "        If provided, the results are stored as a csv file from a pandas dataframe\n",
    "    **kwargs : dict\n",
    "        Additional keyword arguments to be used for the selection criterion\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple containing:\n",
    "        rmse_test : np.array\n",
    "            The RMSE of the model with the training set after each iteration\n",
    "        rmse_validation : np.array\n",
    "            The RMSE of the model with the validation set after each iteration\n",
    "        samples_selected : np.array\n",
    "            The samples selected in each iteration\n",
    "        rmse_full : float\n",
    "            The RMSE of the model trained with all training samples\n",
    "\n",
    "    \"\"\"\n",
    "    if 'n_fold' in kwargs:\n",
    "        n_fold = kwargs['n_fold']\n",
    "    else:\n",
    "        n_fold = 3 # default value\n",
    "\n",
    "    # _validate_parameters(\n",
    "    #     X_train,\n",
    "    #     y_train,\n",
    "    #     model_class=None,\n",
    "    #     model_params={},\n",
    "    #     selection_criterion=None,\n",
    "    #     n_iterations=None,\n",
    "    #     n_samples_per_it=None,\n",
    "    #     init_sample_size=None,\n",
    "    # )\n",
    "\n",
    "    logging.info(f\"Size of X_train: {X_train.shape}\")\n",
    "    logging.info(f\"Size of y_train: {y_train.shape}\")\n",
    "    logging.info(f\"Model_class: {model_class}\")\n",
    "    logging.info(f\"Modelling parameters: {model_params}\")\n",
    "    logging.info(f\"Selection Criterion: {selection_criterion}\")\n",
    "    logging.info(f\"Key word arguments: {kwargs}\")\n",
    "\n",
    "    if n_samples_per_it is None:\n",
    "        n_samples_per_it = 1\n",
    "    if init_sample_size is None:\n",
    "        init_sample_size = 10\n",
    "    if n_iterations is None:\n",
    "        n_iterations = 50\n",
    "    if random_state is None:\n",
    "        random_state = 12345\n",
    "\n",
    "    # Initialize the model\n",
    "    model = model_class(**model_params)\n",
    "    # Initialize the active learning model\n",
    "    X_Pool = X_train\n",
    "    y_Pool = y_train\n",
    "\n",
    "    # prepare the output objects\n",
    "    rmse_test = np.zeros(n_iterations)\n",
    "    rmse_validation = np.zeros(n_iterations)\n",
    "    samples_selected = np.zeros(n_iterations)\n",
    "    selection_value_storage = np.zeros(n_iterations)\n",
    "\n",
    "    # initialize the learned set as a empty dataframe\n",
    "    X_Learned = pd.DataFrame()\n",
    "    y_Learned = pd.Series()\n",
    "    \n",
    "    # Initialize the model\n",
    "    X_Learned, y_Learned, X_Pool, y_Pool = _rnd_initial_sampling(X_Pool, X_Learned, y_Pool, y_Learned, init_sample_size, random_state)\n",
    "    model.fit(X_Learned, y_Learned)\n",
    "\n",
    "    logging.info(f\"Initial model fitted with {init_sample_size} samples\")\n",
    "    logging.info(\"--Active Learning starts--\")\n",
    "\n",
    "    for it in range(n_iterations):\n",
    "        logging.info(f\"Active Learning with {selection_criterion} - iteration: {it}\")\n",
    "\n",
    "        y_pred_pool = model.predict(X_Pool)\n",
    "        # convert y_pred_pool to a \n",
    "        y_pred_pool = pd.Series(y_pred_pool, index=X_Pool.index)\n",
    "\n",
    "        # uncertainty_criteria_(X_Pool, y_Pool, X_Learned, y_Learned, y_pred_pool, n_fold)\n",
    "        sample_id, selection_value = selection_criterion(X_Pool = X_Pool, \n",
    "                                        y_Pool = y_Pool,\n",
    "                                        X_Learned = X_Learned,\n",
    "                                        y_Learned = y_Learned,\n",
    "                                        y_pred_pool = y_pred_pool,\n",
    "                                        n_fold = n_fold, \n",
    "                                        random_state = random_state, \n",
    "                                        logging = logging, \n",
    "                                        model = model,\n",
    "                                        n_jobs = n_jobs,\n",
    "                                        kwargs = kwargs)\n",
    "\n",
    "        samples_selected[it] = sample_id\n",
    "        selection_value_storage[it] = selection_value\n",
    "        logging.info(f\"Sample_id: {sample_id} with selection value {selection_value}\")\n",
    "\n",
    "        # Update the Sample sets\n",
    "        x_new = X_Pool.loc[[sample_id]]\n",
    "        y_new = y_Pool.loc[[sample_id]]\n",
    "        X_Learned = pd.concat([X_Learned, x_new], ignore_index=True)\n",
    "        y_Learned = pd.concat([y_Learned, y_new], ignore_index=True)\n",
    "        X_Pool = X_Pool.drop(index=sample_id)\n",
    "        y_Pool = y_Pool.drop(index=sample_id)\n",
    "\n",
    "        # Update the Model\n",
    "        # retrain model on the new full data set and predict a new fit, if the n_samples_per_it is reached\n",
    "        if n_samples_per_it == None or n_samples_per_it == 1:\n",
    "            model.fit(X_Learned, y_Learned)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse_test[it] = rmse_func(y_test, y_pred)\n",
    "            y_pred_val = model.predict(X_val)\n",
    "            rmse_validation[it] = rmse_func(y_val, y_pred_val)\n",
    "        if it % n_samples_per_it == 0 or it != 0:\n",
    "            model.fit(X_Learned, y_Learned)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse_test[it] = rmse_func(y_test, y_pred)\n",
    "            y_pred_val = model.predict(X_val)\n",
    "            rmse_validation[it] = rmse_func(y_val, y_pred_val)\n",
    "\n",
    "    # write results into outputifle\n",
    "    if results_file is not None:\n",
    "        results = pd.DataFrame(\n",
    "            {\n",
    "                \"rmse_test\": rmse_test,\n",
    "                \"rmse_validation\": rmse_validation,\n",
    "                \"samples_selected\": samples_selected,\n",
    "                \"selection_value\": selection_value_storage,\n",
    "            }\n",
    "        )\n",
    "        results.to_csv(results_file, index=False, mode = \"a\")\n",
    "        logging.info(f\"Results written to {results_file}\")\n",
    "\n",
    "    # calc the rmse for the model including all training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse_full = rmse_func(y_test, y_pred)\n",
    "\n",
    "    # # plot the rmse over the iterations\n",
    "    # plt.plot(range(n_iterations)[1:], rmse_test[1:])\n",
    "    # # add a line for the model with all training samples\n",
    "    # plt.axhline(y=rmse_full, color=\"r\", linestyle=\"--\")\n",
    "    # selection_criterion_str = str(selection_criterion).split(\" \")[1]\n",
    "    # plt.title(\n",
    "    #     f\"RMSE over Iterations with {model_class} and\\n {selection_criterion} as selection criterion \\n {selection_criterion_str} as selection criterion\"\n",
    "    # )\n",
    "    # plt.xlabel(\"Iteration\")\n",
    "    # plt.ylabel(\"RMSE\")\n",
    "    # plt.show()\n",
    "    return rmse_test, rmse_validation, samples_selected, rmse_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_active_learning():\n",
    "    \"\"\"\n",
    "    Function to test the active learning function\n",
    "    \"\"\"\n",
    "    from al_lib.helper_functions import _create_test_data\n",
    "    from al_lib.helper_functions import _test_params_krr\n",
    "\n",
    "    # create test data\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = _create_test_data(logging=logging)\n",
    "    # Perform active Learning for n_iterations\n",
    "    n_iterations = 5\n",
    "    n_samples_per_it = 1\n",
    "    initial_sample_size = 10\n",
    "    model_params=_test_params_krr()\n",
    "    model_class=KRR\n",
    "    # Perform active learning\n",
    "    rmse_test, rmse_validation, samples_selected, rmse_full = active_learning(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        X_val,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        y_val,\n",
    "        logging,\n",
    "        model_class=model_class,\n",
    "        model_params=model_params,\n",
    "        selection_criterion=_uncertainty_selection,\n",
    "        n_samples_per_it=n_samples_per_it,\n",
    "        n_iterations=n_iterations,\n",
    "        init_sample_size=initial_sample_size,\n",
    "    )\n",
    "\n",
    "    return rmse_test, rmse_validation, samples_selected, rmse_full\n",
    "\n",
    "rmse_test, rmse_validation, samples_selected, rmse_full = test_active_learning()\n",
    "\n",
    "assert rmse_test.shape[0] == rmse_validation.shape[0] == samples_selected.shape[0], \"Shapes of output arrays not equal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2): \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform active learning twice and generate result plots\n",
    "\n",
    "def test_active_learning_twice():\n",
    "    \"\"\"\n",
    "    Function to test the active learning function\n",
    "    \"\"\"\n",
    "    from al_lib.helper_functions import _create_test_data\n",
    "    from al_lib.helper_functions import _test_params_krr\n",
    "\n",
    "    # create test data\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = _create_test_data(logging=logging)\n",
    "    # Perform active Learning for n_iterations\n",
    "    n_iterations = 15\n",
    "    n_samples_per_it = 1\n",
    "    initial_sample_size = 10\n",
    "    model_params=_test_params_krr()\n",
    "    model_class=KRR\n",
    "    # Perform active learning using a loop, store the results for each iteration\n",
    "\n",
    "    # the rsults will be stored in a df with the following columns:\n",
    "    # for each n_al_iterations: rmse_test_(i), rmse_validation_(i), samples_selected_(i), rmse_full_(i)\n",
    "    results = pd.DataFrame()\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    for i in range(2):\n",
    "        rmse_test, rmse_validation, samples_selected, rmse_full = active_learning(\n",
    "            X_train,\n",
    "            X_test,\n",
    "            X_val,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            y_val,\n",
    "            logging,\n",
    "            model_class=model_class,\n",
    "            model_params=model_params,\n",
    "            selection_criterion=_uncertainty_selection,\n",
    "            n_samples_per_it=n_samples_per_it,\n",
    "            n_iterations=n_iterations,\n",
    "            init_sample_size=initial_sample_size,\n",
    "        )\n",
    "        results[f\"rmse_test_{i}\"] = rmse_test\n",
    "        results[f\"rmse_validation_{i}\"] = rmse_validation\n",
    "        results[f\"samples_selected_{i}\"] = samples_selected\n",
    "        results[f\"rmse_full_{i}\"] = rmse_full\n",
    "\n",
    "    # plot the results of both iterations\n",
    "    # fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "    # for i in range(2):\n",
    "    #     ax[i].plot(range(n_iterations), results[f\"rmse_test_{i}\"], label=\"RMSE Sampling\")\n",
    "    #     ax[i].plot(range(n_iterations), results[f\"rmse_validation_{i}\"], label=\"RMSE Validation\")\n",
    "    #     ax[i].axhline(y=results[f\"rmse_full_{i}\"][0], color=\"r\", linestyle=\"--\", label=\"RMSE Full\")\n",
    "    #     ax[i].set_title(f\"RMSE over Iterations with KRR and Uncertainty Sampling - Iteration {i}\")\n",
    "    #     ax[i].set_xlabel(\"Iteration\")\n",
    "    #     ax[i].set_ylabel(\"RMSE\")\n",
    "    #     ax[i].legend()\n",
    "    # plt.show()\n",
    "    # plot the results of both iterations in a single plot\n",
    "        \n",
    "    \n",
    "        ax.plot(range(n_iterations), (results[f\"rmse_test_{i}\"]-i//10), label=f\"RMSE Sampling {i}\")\n",
    "        ax.plot(range(n_iterations), results[f\"rmse_validation_{i}\"], label=f\"RMSE Validation {i}\")\n",
    "        ax.axhline(y=results[f\"rmse_full_{i}\"][0], color=\"r\", linestyle=\"--\", label=\"RMSE Full\")\n",
    "        ax.set_title(f\"RMSE over Iterations with KRR and Uncertainty Sampling\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(\"RMSE\")\n",
    "        ax.legend()\n",
    "    # id the range for the y axis\n",
    "    ax.set_ylim(0, 1.1 * results[f\"rmse_test_{i}\"].max())\n",
    "    plt.show()\n",
    "\n",
    "test_active_learning_twice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Experiment\n",
    "\n",
    "The main experiment compares the performance of the various selection strategies statistically. To this end each selection strategy is performed multiple times for each model-class. The results can be compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the model and the model parameters for the models that are not available\n",
    "for model in models_available:\n",
    "    if models_available[model] == False:\n",
    "        # remove the model and the model parameters\n",
    "        try: \n",
    "            #remove the model from the models list\n",
    "            models.remove(model)\n",
    "        except KeyError:\n",
    "            logging.info(f\"Error deleting the parameters for model: {model}\")\n",
    "        try:\n",
    "            del globals()[f\"{model}\"]\n",
    "        except KeyError:\n",
    "            logging.info(f\"Error deleting the model: {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "# remove the models that are not available\n",
    "model_names_list = [\"hgb\", \"krr\", \"mlp\", \"pls\", \"rf\", \"xgb\"]\n",
    "models_list = [HGB, KRR, MLP, PLS, RF, XGB]\n",
    "\n",
    "params = [params_hgb, params_krr, params_mlp, params_pls, params_rf, params_xgb]\n",
    "\n",
    "model_params_list = [{model: param} for model, param in zip(models_list, params)]\n",
    "model_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_params_list:\n",
    "    #seperate the model class and the parameters\n",
    "    model_class = list(model.keys())[0]\n",
    "    model_params = model[model_class]\n",
    "    print(type(model_class), type(model_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Experiment - Test for single model\n",
    "\n",
    "AL_RESULTS_PATH = f\"{RESULTS_PATH}al_result_tables/\"\n",
    "\n",
    "# number of active learning runs\n",
    "n_al_iterations = 20\n",
    "# Define the number of iterations for each active learning run\n",
    "n_iterations = 200\n",
    "\n",
    "# Define the number of samples to be queried in each iteration\n",
    "n_samples_per_it = 1\n",
    "\n",
    "# Define the initial sample size\n",
    "init_sample_size = 10\n",
    "\n",
    "# Define the random state\n",
    "random_state = 12345\n",
    "\n",
    "# Define the number of jobs\n",
    "n_jobs = 20\n",
    "\n",
    "# Define the output object\n",
    "\n",
    "selection_criteria = [\n",
    "    {'criteria':_random_selection, 'crit_name': 'random', 'kwargs': {}}, #'random_state': random_state}}, \n",
    "    {'criteria':_gsx_selection, 'crit_name': 'gsx', 'kwargs': {}}, \n",
    "    {'criteria':_gsy_selection, 'crit_name': 'gsy', 'kwargs': {}}, \n",
    "    {'criteria':_uncertainty_selection,'crit_name': 'uncertainty', 'kwargs': {'n_fold': 3}},\n",
    "                      ]\n",
    "\n",
    "#prepare the results dataframe\n",
    "# for model in models:\n",
    "#     for criteria in selection_criteria:\n",
    "#         # create a global variable with the model and criterion\n",
    "#         globals()[f\"{model}_{criteria}\"] = pd.DataFrame(\n",
    "#             index=range(n_al_iterations), columns=range(n_iterations)\n",
    "#         )\n",
    "\n",
    "# perform the active learning process\n",
    "\n",
    "for model in model_params_list:\n",
    "    model_class = list(model.keys())[0]\n",
    "    model_params = model[model_class]\n",
    "    results = pd.DataFrame()\n",
    "    logging.info(f\"Current model: {model}\")\n",
    "    for i in range(n_al_iterations):\n",
    "        logging.info(f\"Active Learning iteration: {i}\")\n",
    "        random_state = random_state + i\n",
    "        for criteria in selection_criteria:\n",
    "            logging.info(f\"Current criterion: {criteria['criteria']}\")\n",
    "            logging.info(f\"Current criterion: {criteria['crit_name']}\")\n",
    "            logging.info(f\"Current criterion kwargs: {criteria['kwargs']}\")\n",
    "            # extract the model name\n",
    "            model_name = str(model_class).split(\".\")[-1]\n",
    "            selection_criteria_name = criteria['crit_name']\n",
    "            results_file = f\"{AL_RESULTS_PATH}al_results_{models_available}_{criteria['crit_name']}.csv\"\n",
    "            kwargs = criteria.get('kwargs', {})\n",
    "            rmse_test, rmse_validation, samples_selected, rmse_full = active_learning(\n",
    "                X_train,\n",
    "                X_test,\n",
    "                X_val,\n",
    "                y_train,\n",
    "                y_test,\n",
    "                y_val,\n",
    "                logging = logging,\n",
    "                model_class=model_class,\n",
    "                model_params=model_params,\n",
    "                selection_criterion=criteria['criteria'],\n",
    "                n_iterations=n_iterations,\n",
    "                n_samples_per_it=n_samples_per_it,\n",
    "                init_sample_size=init_sample_size,\n",
    "                random_state=random_state,\n",
    "                n_jobs=n_jobs,\n",
    "                results_file=None,\n",
    "                **kwargs,\n",
    "            )\n",
    "            results[f\"rmse_test_{model_name}_{criteria['crit_name']}_{i}\"] = rmse_test\n",
    "            results[f\"rmse_val_{model_name}_{criteria['crit_name']}_{i}\"] = rmse_validation\n",
    "            results[f\"sample_sel_{model_name}_{criteria['crit_name']}_{i}\"] = samples_selected\n",
    "            results[f\"rmse_full_{model_name}_{criteria['crit_name']}_{i}\"] = rmse_full\n",
    "    # store the results in the global variables\n",
    "    globals()[f\"{model_name}_al_results\"] = results\n",
    "    # write the results to a csv file\n",
    "results.to_csv(f\"{AL_RESULTS_PATH}al_results_{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _seperate_results(results, model, selection_criteria): \n",
    "\n",
    "    results_random = pd.DataFrame()\n",
    "    results_gsx = pd.DataFrame()\n",
    "    results_gsy = pd.DataFrame()\n",
    "    results_uncertainty = pd.DataFrame()\n",
    "\n",
    "    model_name = str(model).split(\" \")[1]\n",
    "    results_random = pd.concat(\n",
    "        [\n",
    "            results_random,\n",
    "            globals()[f\"{model_name}_al_results_loaded\"].filter(\n",
    "                regex=f\"rmse_test_{model_name}_random\"\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    results_gsx = pd.concat(\n",
    "        [\n",
    "            results_gsx,\n",
    "            globals()[f\"{model_name}_al_results_loaded\"].filter(\n",
    "                regex=f\"rmse_test_{model_name}_gsx\"\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    results_gsy = pd.concat(\n",
    "        [\n",
    "            results_gsy,\n",
    "            globals()[f\"{model_name}_al_results_loaded\"].filter(\n",
    "                regex=f\"rmse_test_{model_name}_gsy\"\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    results_uncertainty = pd.concat(\n",
    "        [\n",
    "            results_uncertainty,\n",
    "            globals()[f\"{model_name}_al_results_loaded\"].filter(\n",
    "                regex=f\"rmse_test_{model_name}_uncertainty\"\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    return results_random, results_gsx, results_gsy, results_uncertainty\n",
    "\n",
    "model_krr = models[1] #results for krr\n",
    "results_random, results_gsx, results_gsy, results_uncertainty = _seperate_results(results, model_krr, selection_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "\n",
    "for model in models:\n",
    "    results = globals()[f\"{model}_al_results\"]\n",
    "    results_random, results_gsx, results_gsy, results_uncertainty = _seperate_results(results, model, selection_criteria)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    # clean up the model name\n",
    "    model_name = str(model).split(\" \")[1]\n",
    "    model_name = model_name.replace(\"'sklearn.ensemble._forest.RandomForestRegressor'>\", 'RF')\n",
    "    model_name = model_name.replace(\"'sklearn.kernel_ridge.KernelRidge'>\", 'KRR')\n",
    "    model_name = model_name.replace(\"'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>\", 'HGB')\n",
    "    model_name = model_name.replace(\"'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>\", 'MLP')\n",
    "    model_name = model_name.replace(\"'sklearn.cross_decomposition._pls.PLSRegression'>\", 'PLS')\n",
    "    model_name = model_name.replace(\"'xgboost.sklearn.XGBRegressor'>\", 'XGB')\n",
    "\n",
    "    ax.plot(\n",
    "        range(n_iterations),\n",
    "        results_random.mean(axis=1),\n",
    "        label=f\"{model_name} Random\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        range(n_iterations),\n",
    "        results_gsx.mean(axis=1),\n",
    "        label=f\"{model_name} GSX\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(n_iterations),\n",
    "        results_gsy.mean(axis=1),\n",
    "        label=f\"{model_name} GSY\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(n_iterations),\n",
    "        results_uncertainty.mean(axis=1),\n",
    "        label=f\"{model_name} Uncertainty\",\n",
    "    )\n",
    "    # calculate the std for each selection criterion\n",
    "    std_random = results_random.std(axis=1)\n",
    "    std_gsx = results_gsx.std(axis=1)\n",
    "    std_gsy = results_gsy.std(axis=1)\n",
    "    std_uncertainty = results_uncertainty.std(axis=1)\n",
    "    # plt the std as a shaded area\n",
    "    ax.fill_between(range(n_iterations), results_random.mean(axis=1) - std_random, results_random.mean(axis=1) + std_random, alpha=0.1,)\n",
    "    ax.fill_between(range(n_iterations), results_gsx.mean(axis=1) - std_gsx, results_gsx.mean(axis=1) + std_gsx, alpha=0.1)\n",
    "    ax.fill_between(range(n_iterations), results_gsy.mean(axis=1) - std_gsy, results_gsy.mean(axis=1) + std_gsy, alpha=0.1)\n",
    "    ax.fill_between(range(n_iterations), results_uncertainty.mean(axis=1) - std_uncertainty, results_uncertainty.mean(axis=1) + std_uncertainty, alpha=0.1)\n",
    "    ax.set_title(f\"RMSE over Iterations with {model_name} and different Selection Criteria\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}al_results_{model_name}.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
