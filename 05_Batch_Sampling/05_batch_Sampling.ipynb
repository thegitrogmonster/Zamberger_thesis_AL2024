{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the active Learning strategies in the batch sampling setting, while using an oop approach. The goal of this notebook is to\n",
    "\n",
    "* implement batch sampling for the KRR Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Define the PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "basepath = \"../\"  # Project directory\n",
    "sys.path.append(basepath)\n",
    "# AL Scripts\n",
    "AL_SCRIPTS_PATH = basepath + \"al_lib/\"\n",
    "\n",
    "sys.path.append({AL_SCRIPTS_PATH})\n",
    "\n",
    "from al_lib.active_learning_setting import ActiveLearningBatchSamplingPaths\n",
    "\n",
    "PATHS = ActiveLearningBatchSamplingPaths()\n",
    "(DATA_PATH, FIGURE_PATH, ENV_PATH, RESULTS_PATH, LOG_DIR) = PATHS\n",
    "\n",
    "sys.path.extend(PATHS)\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the logging specifications from file 'logging_config.py'\n",
    "from al_lib.logging_config import create_logger\n",
    "import datetime\n",
    "\n",
    "# Add data/time information\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# date = now.strftime(\"%Y-%m-%d\")\n",
    "log_file_name = f\"{date}_active_learning_batch.log\"\n",
    "log_file_path = f\"{LOG_DIR}{log_file_name}\"\n",
    "\n",
    "# Create logger\n",
    "logging = create_logger(__name__, log_file_path = log_file_path)\n",
    "# Usage of the logger as follows:\n",
    "logging.info(\"Logging started\")\n",
    "logging.info(f\"log stored at: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn of sklearn warnings\n",
    "from warnings import simplefilter\n",
    "import warnings\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*y residual is constant*.\", category=UserWarning, append=False\n",
    ")\n",
    "# logging.warning(\"Warning \\\"y residual is constant\\\" turned off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datafile\n",
    "\n",
    "data_name = \"dpsDeriv1200.csv\"\n",
    "\n",
    "datafile = DATA_PATH + data_name\n",
    "\n",
    "from al_lib.helper_functions import import_dpsDeriv1200\n",
    "\n",
    "data = import_dpsDeriv1200(datafile)\n",
    "logging.info(f\"Data loaded and preprocessed from {datafile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    pass\n",
    "end = time.time()\n",
    "print(f\"Time taken for 100 passes: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into feature and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.select_dtypes(\"float\")\n",
    "y = data[\"year\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "since not every regression method is able to estimate its prediction accuracy, a split of the data is retained as validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of columns with std = 0.0 in X\n",
    "logging.info(\n",
    "    f\"{(X.std() == 0.0).sum()} Columns dropped, where std = 0.0 in X\"\n",
    ")\n",
    "\n",
    "# drop the columns with std = 0.0\n",
    "X = X.loc[:, X.std() != 0.0]\n",
    "logging.info(\n",
    "    f\"X: {X.shape},y: {y.shape} Dimensions after dropping columns with std = 0.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational Settings\n",
    "random_state = 12345\n",
    "\n",
    "validation_size = 0.1\n",
    "test_size = 0.3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# retain 10% of the data for validation\n",
    "(\n",
    "    X_remainder,\n",
    "    X_val,\n",
    "    y_remainder,\n",
    "    y_val,\n",
    ") = train_test_split(X, y, test_size=validation_size, random_state=random_state)\n",
    "\n",
    "# split the remainder into training and test (30%) set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_remainder, y_remainder, test_size=test_size, random_state=random_state\n",
    ")\n",
    "logging.info(f\"Split of the dataset into Train/Test/Validation set\")\n",
    "\n",
    "# assert the shapes for the sets and raise an error if they are not equal\n",
    "assert (\n",
    "    X_train.shape[0] + X_test.shape[0] + X_val.shape[0] == X.shape[0]\n",
    "), \"Sum of samples in Train/Test/Validation set not equal to total samples\"\n",
    "assert (\n",
    "    X_train.shape[1] == X_test.shape[1] == X_val.shape[1] == X.shape[1]\n",
    "), \"Number of features in Train/Test/Validation set not equal to total features\"\n",
    "assert (\n",
    "    y_train.shape[0] + y_test.shape[0] + y_val.shape[0] == y.shape[0]\n",
    "), \"Sum of Sample-targets in Train/Test/Validation set not equal to total samples\"\n",
    "assert (\n",
    "    X_train.shape[0] == y_train.shape[0]\n",
    "), \"Number of samples not equal to number of targets in Train set\"\n",
    "assert (\n",
    "    X_test.shape[0] == y_test.shape[0]\n",
    "), \"Number of samples not equal to number of targets in Test set\"\n",
    "assert (\n",
    "    X_val.shape[0] == y_val.shape[0]\n",
    "), \"Number of samples not equal to number of targets in Validation set\"\n",
    "\n",
    "logging.info(f\"Shapes of Train/Test/Validation set verified\")\n",
    "logging.info(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.helper_functions import calc_set_sizes\n",
    "\n",
    "calc_set_sizes(X_train , X_test, X_val, logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters and Model Methods\n",
    "\n",
    "The optimal model parameters according to the CV Results are used to to fit the individual models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Regressors\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "from sklearn.cross_decomposition import PLSRegression as PLS\n",
    "from xgboost import XGBRegressor as XGB\n",
    "\n",
    "# Define the regressors\n",
    "Regressors = [KRR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the rscv model parameters from 03_Modelling/03_1_rscv\n",
    "\n",
    "rscv_results_dir = basepath + \"03_Modelling/03_1_rscv/rscv_results/\"\n",
    "gscv_results_dir = basepath + \"03_Modelling/03_2_gscv/gscv_results/\"\n",
    "\n",
    "# models tested\n",
    "models = [\"hgb\", \"krr\", \"mlp\", \"pls\", \"rf\", \"xgb\"]\n",
    "from al_lib.helper_functions import _get_optimal_params_from_cv\n",
    "\n",
    "optimal_params, rmse_from_cv, models_available = _get_optimal_params_from_cv(models, gscv_results_dir, rscv_results_dir, logging=logging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the model parameters which are used to perform active learning. These are important, since they potentially influence the performance of the AL-processes in an influental manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_available:\n",
    "    if models_available[model]:\n",
    "        logging.info(f\"Optimal parameters (GSCV) for {model} with RMSE {rmse_from_cv[model]}: {optimal_params[model]}\")\n",
    "    else:\n",
    "        logging.info(f\"Optimal parameters (RSCV) for {model} with RMSE {rmse_from_cv[model]}: {optimal_params[model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate object with the optimized parameters to hand over to the Regressors\n",
    "for key in optimal_params.keys():\n",
    "    # generate a global variable with the optimal parameters\n",
    "    globals()[f\"params_{key}\"] = optimal_params[key]\n",
    "\n",
    "\n",
    "for key in optimal_params.keys():\n",
    "    logging.info(f\"Optimal parameters for {key}: {optimal_params[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Setup\n",
    "\n",
    "The Basic Active Learning Experiment follows the specifications:\n",
    "\n",
    "* Implementation of each Sampling Strategy in a Modular fashion\n",
    "* Selecting the inital samples randomly\n",
    "* Refitting the model after each selected sample\n",
    "* Runing the experiment n-fold with differing random states\n",
    "* Calculation of mean performance with confidence intervalls\n",
    "* Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.selection_criteria import (\n",
    "    _random_selection,\n",
    "    _gsx_selection,\n",
    "    _gsy_selection,\n",
    "    _uncertainty_selection,\n",
    "    _distance_weighing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the active Learning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.helper_functions import _validate_parameters\n",
    "from al_lib.helper_functions import _rnd_initial_sampling\n",
    "from al_lib.helper_functions import rmse_func\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "\n",
    "\n",
    "def active_learning_batch(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    logging,\n",
    "    model_class=None,\n",
    "    model_params={},\n",
    "    selection_criterion=None,\n",
    "    n_iterations=None,\n",
    "    n_samples_per_it=None,\n",
    "    init_sample_size=None,\n",
    "    random_state=None,\n",
    "    n_jobs=None,\n",
    "    results_file=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform active learning with the given parameters in a BATCH mode.\n",
    "\n",
    "    Active Learning selects additional samples for the training set from\n",
    "    provided pool of samples. The selection is based on a selection criterion,\n",
    "    which can be selected from implemented criteria. The active\n",
    "    learning process is repeated for n_iterations. The model is retrained after\n",
    "    each iteration with the updated training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_i : pd.DataFrame\n",
    "        The features of the set (i = (train, test, val))\n",
    "    y_i : pd.Series\n",
    "        The target of the set (i = (train, test, val))\n",
    "    model_class : model class, either from sklearn or xgboost\n",
    "        The model to be used for the active learning process\n",
    "    model_params : dict\n",
    "        The parameters for the model\n",
    "    selection_criterion : function\n",
    "        The selection criterion to be used for the active learning process\n",
    "    n_iterations : int, optional (default=50)\n",
    "        The number of iterations for the active learning process\n",
    "    n_samples_per_it : int, optional (default=1)\n",
    "        The number of samples to be selected in each iteration\n",
    "    init_sample_size : int, optional (default=10)\n",
    "        The initial sample size for initial model\n",
    "    random_state : int, optional\n",
    "        The random state for the active learning process\n",
    "    n_jobs : int, optional\n",
    "        The number of kernels to be used for the active learning process\n",
    "    results_file : str, optional\n",
    "        The path to the file to store the results of the active learning process\n",
    "        If provided, the results are stored as a csv file from a pandas dataframe\n",
    "    **kwargs : dict\n",
    "        Additional keyword arguments to be used for the selection criterion\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple containing:\n",
    "        rmse_test : np.array\n",
    "            The RMSE of the model with the training set after each iteration\n",
    "        rmse_validation : np.array\n",
    "            The RMSE of the model with the validation set after each iteration\n",
    "        samples_selected : np.array\n",
    "            The samples selected in each iteration\n",
    "        rmse_full : float\n",
    "            The RMSE of the model trained with all training samples\n",
    "\n",
    "    \"\"\"\n",
    "    if \"n_fold\" in kwargs:\n",
    "        n_fold = kwargs[\"n_fold\"]\n",
    "    else:\n",
    "        n_fold = 3  # default value\n",
    "\n",
    "    # _validate_parameters(\n",
    "    #     X_train,\n",
    "    #     y_train,\n",
    "    #     model_class=None,\n",
    "    #     model_params={},\n",
    "    #     selection_criterion=None,\n",
    "    #     n_iterations=None,\n",
    "    #     n_samples_per_it=None,\n",
    "    #     init_sample_size=None,\n",
    "    # )\n",
    "\n",
    "    logging.info(f\"Size of X_train: {X_train.shape}\")\n",
    "    logging.info(f\"Size of y_train: {y_train.shape}\")\n",
    "    logging.info(f\"Model_class: {model_class}\")\n",
    "    logging.info(f\"Modelling parameters: {model_params}\")\n",
    "    logging.info(f\"Selection Criterion: {selection_criterion}\")\n",
    "    logging.info(f\"Key word arguments: {kwargs}\")\n",
    "\n",
    "    if n_samples_per_it is None:\n",
    "        n_samples_per_it = 1\n",
    "    if init_sample_size is None:\n",
    "        init_sample_size = 10\n",
    "    if n_iterations is None:\n",
    "        n_iterations = 50\n",
    "    if random_state is None:\n",
    "        random_state = 12345\n",
    "\n",
    "    # Initialize the model\n",
    "    model = model_class(**model_params)\n",
    "    # Initialize the active learning model\n",
    "    X_Pool = X_train\n",
    "    y_Pool = y_train\n",
    "\n",
    "    # prepare the output objects\n",
    "    rmse_test = np.zeros(n_iterations)\n",
    "    rmse_validation = np.zeros(n_iterations)\n",
    "    samples_selected = np.zeros(n_iterations)\n",
    "    selection_value_storage = np.zeros(n_iterations)\n",
    "\n",
    "    # initialize the learned set as a empty dataframe\n",
    "    X_Learned = pd.DataFrame()\n",
    "    y_Learned = pd.Series()\n",
    "\n",
    "    # Initialize the model\n",
    "    X_Learned, y_Learned, X_Pool, y_Pool = _rnd_initial_sampling(\n",
    "        X_Pool, X_Learned, y_Pool, y_Learned, init_sample_size, \n",
    "        random_state = random_state\n",
    "    )\n",
    "    model.fit(X_Learned, y_Learned)\n",
    "\n",
    "    logging.info(f\"Initial model fitted with {init_sample_size} samples\")\n",
    "    logging.info(\"--Active Learning starts--\")\n",
    "\n",
    "    for it in range(n_iterations):\n",
    "        logging.info(f\"Active Learning with {selection_criterion} - iteration: {it}\")\n",
    "\n",
    "        y_pred_pool = model.predict(X_Pool)\n",
    "        y_pred_pool = pd.Series(y_pred_pool, index=X_Pool.index)\n",
    "\n",
    "        sample_id, selection_value = selection_criterion(\n",
    "            X_Pool=X_Pool,\n",
    "            y_Pool=y_Pool,\n",
    "            X_Learned=X_Learned,\n",
    "            y_Learned=y_Learned,\n",
    "            y_pred_pool=y_pred_pool,\n",
    "            n_fold=n_fold,\n",
    "            random_state=random_state,\n",
    "            logging=logging,\n",
    "            model=model,\n",
    "            n_jobs=n_jobs,\n",
    "            kwargs=kwargs,\n",
    "        )\n",
    "\n",
    "        samples_selected[it] = sample_id\n",
    "        selection_value_storage[it] = selection_value\n",
    "        logging.info(f\"Sample_id: {sample_id} with selection value {selection_value}\")\n",
    "\n",
    "        # Update the Sample sets\n",
    "        x_new = X_Pool.loc[[sample_id]]\n",
    "        y_new = y_Pool.loc[[sample_id]]\n",
    "        X_Learned = pd.concat([X_Learned, x_new], ignore_index=True)\n",
    "        y_Learned = pd.concat([y_Learned, y_new], ignore_index=True)\n",
    "        X_Pool = X_Pool.drop(index=sample_id)\n",
    "        y_Pool = y_Pool.drop(index=sample_id)\n",
    "\n",
    "        # Update the Model\n",
    "        # retrain model on the new full data set and predict a new fit, if the n_samples_per_it is reached\n",
    "        if n_samples_per_it == None or n_samples_per_it == 1:\n",
    "            model.fit(X_Learned, y_Learned)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse_test[it] = rmse_func(y_test, y_pred)\n",
    "            y_pred_val = model.predict(X_val)\n",
    "            rmse_validation[it] = rmse_func(y_val, y_pred_val)\n",
    "        if it % n_samples_per_it == 0 or it != 0:\n",
    "            model.fit(X_Learned, y_Learned)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse_test[it] = rmse_func(y_test, y_pred)\n",
    "            y_pred_val = model.predict(X_val)\n",
    "            rmse_validation[it] = rmse_func(y_val, y_pred_val)\n",
    "\n",
    "    # write results into outputifle\n",
    "    if results_file is not None:\n",
    "        results = pd.DataFrame(\n",
    "            {\n",
    "                \"rmse_test\": rmse_test,\n",
    "                \"rmse_validation\": rmse_validation,\n",
    "                \"samples_selected\": samples_selected,\n",
    "                \"selection_value\": selection_value_storage,\n",
    "            }\n",
    "        )\n",
    "        results.to_csv(results_file, index=False, mode=\"a\")\n",
    "        logging.info(f\"Results written to {results_file}\")\n",
    "\n",
    "    # calc the rmse for the model including all training data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse_full = rmse_func(y_test, y_pred)\n",
    "\n",
    "    # # plot the rmse over the iterations\n",
    "    # plt.plot(range(n_iterations)[1:], rmse_test[1:])\n",
    "    # # add a line for the model with all training samples\n",
    "    # plt.axhline(y=rmse_full, color=\"r\", linestyle=\"--\")\n",
    "    # selection_criterion_str = str(selection_criterion).split(\" \")[1]\n",
    "    # plt.title(\n",
    "    #     f\"RMSE over Iterations with {model_class} and\\n {selection_criterion} as selection criterion \\n {selection_criterion_str} as selection criterion\"\n",
    "    # )\n",
    "    # plt.xlabel(\"Iteration\")\n",
    "    # plt.ylabel(\"RMSE\")\n",
    "    # plt.show()\n",
    "    return (\n",
    "        rmse_test,\n",
    "        rmse_validation,\n",
    "        samples_selected,\n",
    "        selection_value_storage,\n",
    "        rmse_full,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_active_learning_batch():\n",
    "    \"\"\"\n",
    "    Function to test the active learning function\n",
    "    \"\"\"\n",
    "    from al_lib.helper_functions import _create_test_data\n",
    "    from al_lib.helper_functions import _test_params_krr\n",
    "\n",
    "    # create test data\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = _create_test_data(logging=logging)\n",
    "    # Perform active Learning for n_iterations\n",
    "    n_iterations = 5\n",
    "    n_samples_per_it = 3\n",
    "    initial_sample_size = 10\n",
    "    model_params=_test_params_krr()\n",
    "    model_class=KRR\n",
    "    # Perform active learning\n",
    "    rmse_test, rmse_validation, samples_selected, selection_value_storage, rmse_full = active_learning_batch(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        X_val,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        y_val,\n",
    "        logging,\n",
    "        model_class=model_class,\n",
    "        model_params=model_params,\n",
    "        selection_criterion=_uncertainty_selection,\n",
    "        n_samples_per_it=n_samples_per_it,\n",
    "        n_iterations=n_iterations,\n",
    "        init_sample_size=initial_sample_size,\n",
    "    )\n",
    "\n",
    "    return rmse_test, rmse_validation, samples_selected, selection_value_storage, rmse_full\n",
    "\n",
    "rmse_test, rmse_validation, samples_selected, selection_value_storage, rmse_full = test_active_learning_batch()\n",
    "\n",
    "assert rmse_test.shape[0] == rmse_validation.shape[0] == samples_selected.shape[0], \"Shapes of output arrays not equal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform active learning twice and generate result plots\n",
    "\n",
    "def test_active_learning_twice():\n",
    "    \"\"\"\n",
    "    Function to test the active learning function\n",
    "    \"\"\"\n",
    "    from al_lib.helper_functions import _create_test_data\n",
    "    from al_lib.helper_functions import _test_params_krr\n",
    "\n",
    "    # create test data\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = _create_test_data(logging=logging)\n",
    "    # Perform active Learning for n_iterations\n",
    "    n_iterations = 15\n",
    "    n_samples_per_it = 2\n",
    "    initial_sample_size = 10\n",
    "    model_params=_test_params_krr()\n",
    "    model_class=KRR\n",
    "    # Perform active learning using a loop, store the results for each iteration\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    for i in range(2):\n",
    "        rmse_test, rmse_validation, samples_selected, selection_value_storage, rmse_full = active_learning_batch(\n",
    "            X_train,\n",
    "            X_test,\n",
    "            X_val,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            y_val,\n",
    "            logging,\n",
    "            model_class=model_class,\n",
    "            model_params=model_params,\n",
    "            selection_criterion=_uncertainty_selection,\n",
    "            n_samples_per_it=n_samples_per_it,\n",
    "            n_iterations=n_iterations,\n",
    "            init_sample_size=initial_sample_size,\n",
    "        )\n",
    "        results[f\"rmse_test_{i}\"] = rmse_test\n",
    "        results[f\"rmse_validation_{i}\"] = rmse_validation\n",
    "        results[f\"samples_selected_{i}\"] = samples_selected\n",
    "        results[f\"rmse_full_{i}\"] = rmse_full\n",
    "        results[f\"selection_value_{i}\"] = selection_value_storage\n",
    "\n",
    "        ax1.plot(range(n_iterations), (results[f\"rmse_test_{i}\"]-i//10), label=f\"RMSE Sampling {i}\")\n",
    "        ax1.plot(range(n_iterations), results[f\"rmse_validation_{i}\"], label=f\"RMSE Validation {i}\")\n",
    "        ax1.axhline(y=results[f\"rmse_full_{i}\"][0], color=\"r\", linestyle=\"--\", label=\"RMSE Full\")\n",
    "        ax1.set_title(f\"RMSE over Iterations with KRR and Uncertainty Sampling\")\n",
    "        ax1.set_xlabel(\"Iteration\")\n",
    "        ax1.set_ylabel(\"RMSE\")\n",
    "        ax1.legend()\n",
    "\n",
    "        # prepare the results to plot the selection values\n",
    "        ax2.plot(range(n_iterations), results[f\"selection_value_{i}\"], label=f\"Selection Value {i}\")\n",
    "        ax2.set_title(f\"Selection Value for Uncertainty Sampling\")\n",
    "        ax2.set_xlabel(\"Iteration\")\n",
    "        ax2.set_ylabel(\"Selection Value\")\n",
    "        ax2.legend()\n",
    "    # id the range for the y axis\n",
    "    max_y_test = max([results[f\"rmse_test_{i}\"].max() for i in range(2)])\n",
    "    max_y_val = max([results[f\"rmse_validation_{i}\"].max() for i in range(2)])\n",
    "    max_y  = max(max_y_test, max_y_val)\n",
    "    ax1.set_ylim(0, max_y+0.1)\n",
    "    plt.show()\n",
    "\n",
    "test_active_learning_twice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Experiment\n",
    "\n",
    "The main experiment compares the performance of the various selection strategies statistically. To this end each selection strategy is performed multiple times for each model-class. The results can be compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the model and the model parameters for the models that are not available\n",
    "for model in models_available:\n",
    "    if models_available[model] == False:\n",
    "        # remove the model and the model parameters\n",
    "        try: \n",
    "            #remove the model from the models list\n",
    "            models.remove(model)\n",
    "        except KeyError:\n",
    "            logging.info(f\"Error deleting the parameters for model: {model}\")\n",
    "        try:\n",
    "            del globals()[f\"{model}\"]\n",
    "        except KeyError:\n",
    "            logging.info(f\"Error deleting the model: {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "# remove the models that are not available\n",
    "# models = [\"hgb\", \"krr\", \"pls\", \"rf\", \"xgb\"]\n",
    "models = [\"rf\", \"krr\", \"hgb\", \"pls\", \"xgb\"]\n",
    "models_list = [RF, KRR, HGB, PLS, XGB]\n",
    "\n",
    "params = [params_rf, params_krr, params_hgb, params_pls, params_xgb]\n",
    "\n",
    "model_params_list = [{model: param} for model, param in zip(models_list, params)]\n",
    "model_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_params_list:\n",
    "    #seperate the model class and the parameters\n",
    "    model_class = list(model.keys())[0]\n",
    "    model_params = model[model_class]\n",
    "    print(model_class, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Experiment\n",
    "\n",
    "AL_RESULTS_PATH = f\"{RESULTS_PATH}al_batch_tables/\"\n",
    "\n",
    "# number of active learning runs\n",
    "n_al_iterations = 10\n",
    "# Define the number of iterations for each active learning run\n",
    "n_iterations = 250\n",
    "\n",
    "# Define the number of samples to be queried in each iteration\n",
    "n_samples_per_it = 5\n",
    "\n",
    "# Define the initial sample size\n",
    "init_sample_size = 30\n",
    "\n",
    "# Define the random state\n",
    "random_state = 12345\n",
    "\n",
    "# Define the number of jobs\n",
    "n_jobs = 20\n",
    "\n",
    "# Define the output object\n",
    "\n",
    "selection_criteria = [\n",
    "    {\n",
    "        \"criteria\": _random_selection,\n",
    "        \"crit_name\": \"random\",\n",
    "        \"kwargs\": {},\n",
    "    },\n",
    "    {\"criteria\": _gsx_selection, \"crit_name\": \"gsx\", \"kwargs\": {}},\n",
    "    {\"criteria\": _gsy_selection, \"crit_name\": \"gsy\", \"kwargs\": {}},\n",
    "    {\n",
    "        \"criteria\": _uncertainty_selection,\n",
    "        \"crit_name\": \"uncertainty\",\n",
    "        \"kwargs\": {\"n_fold\": 3},\n",
    "    },\n",
    "    {\"criteria\": _distance_weighing, \"crit_name\": \"idw\", \"kwargs\": {}},\n",
    "]\n",
    "\n",
    "# prepare the results dataframe\n",
    "# for model in models:\n",
    "#     for criteria in selection_criteria:\n",
    "#         # create a global variable with the model and criterion\n",
    "#         globals()[f\"{model}_{criteria}\"] = pd.DataFrame(\n",
    "#             index=range(n_al_iterations), columns=range(n_iterations)\n",
    "#         )\n",
    "\n",
    "# perform the active learning process\n",
    "\n",
    "for model in model_params_list:\n",
    "    start = time.time()\n",
    "    model_class = list(model.keys())[0]\n",
    "    model_params = model[model_class]\n",
    "    results = pd.DataFrame()\n",
    "    logging.info(f\"Current model: {model}\")\n",
    "    for i in range(n_al_iterations):\n",
    "        logging.info(f\"Active Learning iteration: {i}\")\n",
    "        random_state = random_state + i\n",
    "        for criteria in selection_criteria:\n",
    "            logging.info(\n",
    "                f\"Current criterion: {criteria['crit_name']} with kwargs: {criteria['kwargs']}\"\n",
    "            )\n",
    "            # extract the model name\n",
    "            model_name = str(model_class).split(\".\")[-1]\n",
    "            selection_criteria_name = criteria[\"crit_name\"]\n",
    "            results_file = f\"{AL_RESULTS_PATH}al_batch_{models_available}_{criteria['crit_name']}.csv\"\n",
    "            kwargs = criteria.get(\"kwargs\", {})\n",
    "            (\n",
    "                rmse_test,\n",
    "                rmse_validation,\n",
    "                samples_selected,\n",
    "                selection_value_storage,\n",
    "                rmse_full,\n",
    "            ) = active_learning_batch(\n",
    "                X_train,\n",
    "                X_test,\n",
    "                X_val,\n",
    "                y_train,\n",
    "                y_test,\n",
    "                y_val,\n",
    "                logging=logging,\n",
    "                model_class=model_class,\n",
    "                model_params=model_params,\n",
    "                selection_criterion=criteria[\"criteria\"],\n",
    "                n_iterations=n_iterations,\n",
    "                n_samples_per_it=n_samples_per_it,\n",
    "                init_sample_size=init_sample_size,\n",
    "                random_state=random_state,\n",
    "                n_jobs=n_jobs,\n",
    "                results_file=None,\n",
    "                **kwargs,\n",
    "            )\n",
    "            results[f\"rmse_test_{model_name}_{criteria['crit_name']}_{i}\"] = rmse_test\n",
    "            results[f\"rmse_val_{model_name}_{criteria['crit_name']}_{i}\"] = (\n",
    "                rmse_validation\n",
    "            )\n",
    "            results[f\"sample_sel_{model_name}_{criteria['crit_name']}_{i}\"] = (\n",
    "                samples_selected\n",
    "            )\n",
    "            results[f\"rmse_full_{model_name}_{criteria['crit_name']}_{i}\"] = rmse_full\n",
    "            results[f\"selection_value_{model_name}_{criteria['crit_name']}_{i}\"] = (\n",
    "                selection_value_storage\n",
    "            )\n",
    "\n",
    "    # store the results in the global variables\n",
    "    globals()[f\"{model_name}_al_results\"] = results\n",
    "    globals()[f\"{model_name}_results\"] = results\n",
    "    logging.info(f\"Results stored in global variable: {model_name}_al_results\")\n",
    "\n",
    "    # write the results to a csv file\n",
    "    results.to_csv(f\"{AL_RESULTS_PATH}al_results_{model_name}.csv\")\n",
    "    end = time.time()\n",
    "    logging.info(f\"Time taken for active learning with {model_name}: {end-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the data\n",
    "\n",
    "from al_lib.results_vis import load_data  \n",
    "from al_lib.results_vis import _seperate_results_test\n",
    "from al_lib.results_vis import _seperate_results_val\n",
    "from al_lib.results_vis import _plot_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "TABLES_PATH = RESULTS_PATH + \"al_batch_tables/\"\n",
    "# load the data\n",
    "filename_krr = \"al_results_KernelRidge'>.csv\"\n",
    "\n",
    "data_krr = load_data(filename_krr, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_krr.columns:\n",
    "    # rename the columns\n",
    "    data_krr.rename(columns={col: col.replace(\"KernelRidge'>\", \"KRR\")}, inplace=True)\n",
    "\n",
    "data_krr.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_krr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "TABLES_PATH = RESULTS_PATH + \"al_batch_tables/\"\n",
    "# load the data\n",
    "filename_pls = \"al_results_PLSRegression'>.csv\"\n",
    "\n",
    "data_pls = load_data(filename_pls, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_pls.columns:\n",
    "    # rename the columns\n",
    "    data_pls.rename(columns={col: col.replace(\"PLSRegression'>\", \"PLS\")}, inplace=True)\n",
    "\n",
    "data_pls.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_pls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# load the data\n",
    "filename_hgb = \"al_results_HistGradientBoostingRegressor'>.csv\"\n",
    "\n",
    "data_hgb = load_data(filename_hgb, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_hgb.columns:\n",
    "    # rename the columns\n",
    "    data_hgb.rename(columns={col: col.replace(\"HistGradientBoostingRegressor'>\", \"HGB\")}, inplace=True)\n",
    "\n",
    "data_hgb.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_hgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# load the data\n",
    "filename_rf = \"al_results_RandomForestRegressor'>.csv\"\n",
    "\n",
    "data_rf = load_data(filename_rf, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_rf.columns:\n",
    "    # rename the columns\n",
    "    data_rf.rename(columns={col: col.replace(\"RandomForestRegressor'>\", \"RF\")}, inplace=True)\n",
    "\n",
    "data_rf.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "# load the data\n",
    "filename_xgb = \"al_results_XGBRegressor'>.csv\"\n",
    "\n",
    "data_xgb = load_data(filename_xgb, TABLES_PATH)\n",
    "\n",
    "# rename the columns for better readability\n",
    "for col in data_xgb.columns:\n",
    "    # rename the columns\n",
    "    data_xgb.rename(columns={col: col.replace(\"XGBRegressor'>\", \"XGB\")}, inplace=True)\n",
    "\n",
    "data_xgb.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data_xgb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define functions to perform the necessary operations\n",
    "\n",
    "_seperate_results_test (al_lib)\n",
    "_seperate_results_val (al_lib)\n",
    "_plot_rmse (al_lib)\n",
    "_calculate_auc\n",
    "_combined_auc_plot\n",
    "_plot_selection_value_development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only necessary, if not the full notebook is run\n",
    "\n",
    "selection_criteria = [\n",
    "    {\n",
    "        \"criteria\": _random_selection,\n",
    "        \"crit_name\": \"random\",\n",
    "        \"kwargs\": {},\n",
    "    },  #'random_state': random_state}},\n",
    "    {\"criteria\": _gsx_selection, \"crit_name\": \"gsx\", \"kwargs\": {}},\n",
    "    {\"criteria\": _gsy_selection, \"crit_name\": \"gsy\", \"kwargs\": {}},\n",
    "    {\n",
    "        \"criteria\": _uncertainty_selection,\n",
    "        \"crit_name\": \"uncertainty\",\n",
    "        \"kwargs\": {\"n_fold\": 3},\n",
    "    },{\"criteria\": _distance_weighing, \"crit_name\": \"idw\", \"kwargs\": {}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract test RMSE\n",
    "\n",
    "model_name = \"KRR\"\n",
    "filepath = FIGURE_PATH + \"test_rmse_krr.png\"\n",
    "sep_res_test = _seperate_results_test(data_krr, model_name = model_name)\n",
    "plot = _plot_rmse(\n",
    "    sep_res_test,\n",
    "    selection_criteria=selection_criteria,\n",
    "    model_name=model_name,\n",
    "    title=f\"Test RMSE for {model_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract validation RMSE\n",
    "\n",
    "model_name = \"KRR\"\n",
    "filepath = FIGURE_PATH + \"val_rmse_krr.png\"\n",
    "sep_res_val = _seperate_results_val(data_krr, model_name = model_name)\n",
    "plot = _plot_rmse(\n",
    "    sep_res_val,\n",
    "    selection_criteria=selection_criteria,\n",
    "    model_name=model_name,\n",
    "    title=f\"Validation RMSE for {model_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_auc(results):  # test_rmse\n",
    "    \"\"\"\n",
    "    Calculate the Area under the curve for each selection criteria from the\n",
    "    post-processed results\n",
    "    Parameters:\n",
    "    results: list of dataframes\n",
    "    Returns:\n",
    "    aucs: list of tuples (AUC, std_rmse_mean_per_iteration, AUC_mean, auc_std)\n",
    "        each tuple contains the AUC, the standard deviation of the AUC, the mean of the AUC and the standard deviation of the RMSE\n",
    "        AUC: float, the area under the curve\n",
    "        std_rmse_mean_per_iteration: float, the standard deviation of the RMSE for each iteration\n",
    "        AUC_mean: float, the mean of the AUC\n",
    "        auc_std: float, the standard deviation of the AUC\n",
    "    \"\"\"\n",
    "    aucs = []\n",
    "    for i, rmse in enumerate(results):  # for each selection criteria\n",
    "\n",
    "        rmse_mean_per_iteration = rmse.mean(axis=1)  # axis = 1 -> row wise\n",
    "\n",
    "        # calc the std for each iteration\n",
    "        std_rmse_mean_per_iteration = rmse.std(axis=1).round(3)\n",
    "\n",
    "        # calculate the area under the curve\n",
    "        auc = sum(rmse_mean_per_iteration).__round__(1)\n",
    "\n",
    "        # calculate the mean of the AUC\n",
    "        auc_mean = np.mean(rmse_mean_per_iteration).__round__(1)\n",
    "\n",
    "        # calculate the standard deviation of the RMSE for each iteration\n",
    "        auc_std = rmse_mean_per_iteration.std().__round__(1)\n",
    "        aucs.append((auc, std_rmse_mean_per_iteration, auc_mean, auc_std))\n",
    "    return aucs\n",
    "\n",
    "def _plot_auc(aucs, model_name, filepath, title=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    for i, auc in enumerate(aucs):\n",
    "        ax.bar(\n",
    "            selection_criteria[i][\"crit_name\"],\n",
    "            auc[2],\n",
    "            yerr=auc[3],\n",
    "            label=selection_criteria[i][\"crit_name\"],\n",
    "            capsize=5,\n",
    "        )\n",
    "    if title == None:\n",
    "        title = f\"AUC/iteration for {model_name}\"\n",
    "    else:\n",
    "        title = title\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"AUC\")\n",
    "    if filepath != None:\n",
    "        plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_krr = _calculate_auc(sep_res_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.results_vis import combined_auc_plot\n",
    "## main report function \n",
    "def report_al_batch_results(results, model_name, filepath , selection_criteria):\n",
    "    test_rmse = _seperate_results_test(results=results, model_name=model_name)\n",
    "    val_rmse = _seperate_results_val(results = results, model_name=model_name)\n",
    "    # generate the test rmse plot\n",
    "    _plot_rmse(\n",
    "        test_rmse,\n",
    "        selection_criteria=selection_criteria,\n",
    "        model_name=model_name,\n",
    "        title=f\"Test RMSE for {model_name}\",\n",
    "        filepath=filepath + f\"test_rmse_{model_name}.png\",\n",
    "    )\n",
    "    # generate the validation rmse plot\n",
    "    _plot_rmse(\n",
    "        val_rmse,\n",
    "        selection_criteria=selection_criteria,\n",
    "        model_name=model_name,\n",
    "        title=f\"Validation RMSE for {model_name}\",\n",
    "        filepath=filepath + f\"val_rmse_{model_name}.png\",\n",
    "    )\n",
    "\n",
    "    # revised\n",
    "    mean_auc_test = _calculate_auc(test_rmse)\n",
    "    mean_auc_val = _calculate_auc(val_rmse)\n",
    "    combined_auc_plot(\n",
    "        mean_auc_test = mean_auc_test, mean_auc_val = mean_auc_val, model_name=model_name, filepath=filepath, selection_criteria=selection_criteria\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = FIGURE_PATH \n",
    "report_al_batch_results(data_krr, model_name = \"KRR\", filepath = filepath, selection_criteria = selection_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_al_batch_results(\n",
    "    data_pls, model_name=\"PLS\", filepath=filepath, selection_criteria=selection_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_al_batch_results(\n",
    "    data_rf, model_name=\"RF\", filepath=filepath, selection_criteria=selection_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_al_batch_results(\n",
    "    data_xgb, model_name=\"XGB\", filepath=filepath, selection_criteria=selection_criteria\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
