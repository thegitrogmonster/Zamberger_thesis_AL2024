{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSCV Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.clear()\n",
    "\n",
    "# Basepath\n",
    "basepath = \"../\"  # Project directory\n",
    "sys.path.append(basepath)\n",
    "\n",
    "# Data\n",
    "DATA_PATH = basepath + \"data\"\n",
    "\n",
    "# Results path\n",
    "RESULTS_PATH = basepath + \"03_Modelling/03_2_gscv/gscv_results/\"\n",
    "\n",
    "# Figure path\n",
    "FIGURE_PATH = basepath + \"03_Modelling/03_2_gscv/gscv_figures/\"\n",
    "\n",
    "# Path to environment\n",
    "ENV_PATH = \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib\"\n",
    "\n",
    "# Modelpath\n",
    "MODEL_PATH = basepath + \"models\"\n",
    "\n",
    "# Logging\n",
    "LOG_DIR = basepath + \"03_Modelling/03_2_gscv/\"\n",
    "\n",
    "# Active Learning library\n",
    "AL_PATH = basepath + \"al_lib\"\n",
    "\n",
    "# Add the paths\n",
    "sys.path.extend(\n",
    "    {DATA_PATH, FIGURE_PATH, ENV_PATH, MODEL_PATH, RESULTS_PATH, LOG_DIR, AL_PATH}\n",
    ")\n",
    "sys.path  # Check if the path is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the logging specifications from file 'logging_config.py'\n",
    "from al_lib.logging_config import create_logger\n",
    "import datetime\n",
    "\n",
    "# Add data/time information\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the notebook name and the output name\n",
    "notebook_name = \"03_1_rscv.ipynb\"  # Is also used when saving the notebook\n",
    "output_name = f\"{notebook_name.split('.')[0]}_{date}.html\"\n",
    "\n",
    "# Specify logging location\n",
    "log_file_name = f\"{notebook_name.split('.')[0]}_{date}.log\"\n",
    "log_file_dir = f\"{LOG_DIR}\"\n",
    "log_file_path = f\"{LOG_DIR}/{log_file_name}\"\n",
    "# print(f\"Log file path: {log_file_path}\")\n",
    "\n",
    "# Get the logger\n",
    "# logger = None\n",
    "logging = create_logger(__name__, log_file_path=log_file_path)\n",
    "\n",
    "# Usage of the logger as follows:\n",
    "logging.info(\"Logging started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import PS20191107_2deriv_gegl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 2nd_deriv\n",
    "\n",
    "data_2nd_deriv_raw = pd.read_csv(\n",
    "    DATA_PATH + \"/PS20191107_2deriv_gegl.csv\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    sep=\";\",\n",
    "    decimal=\",\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "data_2nd_deriv = data_2nd_deriv_raw.rename(columns={\"Unnamed: 0\": \"Name\"})\n",
    "\n",
    "# Convert all columns of type 'object' to 'float' or 'int' if possible\n",
    "for column in data_2nd_deriv.columns:\n",
    "    # change datatype from the 'year' column to 'int\n",
    "    if column == \"year\":\n",
    "        data_2nd_deriv[column] = data_2nd_deriv[column].astype(\"int\")\n",
    "        print(f\"'{column}' has been converted to 'int'.\")\n",
    "        # skip the rest of the loop\n",
    "        continue\n",
    "    try:\n",
    "        data_2nd_deriv[column] = data_2nd_deriv[column].astype(\"float\")\n",
    "        # data_small.select_dtypes(include=['object']).astype('float')\n",
    "    except ValueError:\n",
    "        print(f\"'{column}' could not be converted. Continue with other column(s).\")\n",
    "    except TypeError:\n",
    "        print(f\"'{column}' could not be converted. Continue with other column(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2nd_deriv_raw.shape  # for quality control purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2nd_deriv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch for the dataset\n",
    "# Select from (data_small, data_full, data_2nd_deriv) or other if implemented\n",
    "data_raw = data_2nd_deriv\n",
    "data_raw.dataset_name = \"data_2nd_deriv\"\n",
    "logging.info(f\"Dataset: {data_raw.dataset_name}\")\n",
    "logging.info(f\"Size of the dataset: {data_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the CV\n",
    "\n",
    "# Switch for testing mode (use only 10% of the data, among others)\n",
    "testing = False\n",
    "\n",
    "# Define a random state for randomized processes\n",
    "random_state = np.random.RandomState(202375)\n",
    "\n",
    "if testing == True:\n",
    "    nfolds = 3\n",
    "    NoTrials = 5\n",
    "    n_jobs = 20\n",
    "    save_model = False\n",
    "    data = data_raw.sample(frac=0.15, random_state=random_state)\n",
    "    logging.info(f\"Size of the dataset reduced: {data.shape}\")\n",
    "else:\n",
    "    nfolds = 10\n",
    "    NoTrials = 15\n",
    "    n_jobs = 30\n",
    "    save_model = True\n",
    "    data = data_raw\n",
    "    logging.info(f\"Size of the dataset not reduced: {data.shape}\")\n",
    "\n",
    "# Log the modelling parameters\n",
    "logging.info(\n",
    "    f\"Testing for Cross Validation: {testing}, nfolds: {nfolds}, NoTrials: {NoTrials}, n_jobs: {n_jobs}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To apply the models we need to split the data into the variables and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into target and features\n",
    "# The goal is to predict the year column of the dataset using the spectral data\n",
    "X = data.select_dtypes(\"float\")\n",
    "y = data[\"year\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of columns with std = 0.0 in X\n",
    "logging.info(f\"Number of columns dropped, where std = 0.0 in X: {(X.std() == 0.0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns with std = 0.0\n",
    "X = X.loc[:, X.std() != 0.0]\n",
    "X.shape, y.shape\n",
    "logging.info(f\"Dimensions of X after dropping columns with std = 0.0: {X.shape}\")\n",
    "logging.info(f\"Dimensions of Y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "logging.info(f\"random split with testsize {test_size} into training and test sets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "# assert the shapes and raise an error if they are not equal\n",
    "assert X_train.shape[0] + X_test.shape[0] == X.shape[0]\n",
    "assert y_train.shape[0] + y_test.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Score metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# create a scorer which calculates Root Mean Squeared Error (RMSE)\n",
    "\n",
    "scoring = make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "# scoring = make_scorer(mean_squared_error, greater_is_better=False, squared=False)\n",
    "logging.info(f\"Scorer: {scoring}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Randomized Search Crossvalidation (RSCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Definition\n",
    "\n",
    "Randomized Search CV is usefull for the efficient exploration of a large parameter space. The results can consequently be used to design a fine grid for the Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Hyperparameter distributions for the RandomizedSearchCV\n",
    "from al_lib.rscv_parameters import (\n",
    "    rf_rscv_parameters,\n",
    "    pls_rscv_parameters,\n",
    "    krr_rscv_parameters,\n",
    "    mlp_rscv_parameters,\n",
    "    xgb_rscv_parameters,\n",
    "    hgb_rscv_parameters,\n",
    ")\n",
    "\n",
    "# to update the import without restarting the kernel, uncoment and modify the following line\n",
    "# del <model>_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "models = {\n",
    "    \"rf\": RandomForestRegressor(),\n",
    "    \"pls\": PLSRegression(),\n",
    "    \"krr\": KRR(),\n",
    "    \"mlp\": MLP(),\n",
    "    \"xgb\": XGBRegressor(),\n",
    "    \"hgb\": HGB(),\n",
    "}\n",
    "\n",
    "# Prepare objects to store the results\n",
    "# Template:\n",
    "# rf_rscv_results = pd.DataFrame(columns=[\"model\", \"MAE\", \"RMSE\", \"params\"])\n",
    "for model in models.keys():\n",
    "    globals()[f\"{model}_rscv_results\"] = pd.DataFrame(\n",
    "        columns=[\"model\", \"MAE\", \"RMSE\", \"params\"]\n",
    "    )\n",
    "    print(f\"{model}_rscv_parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.helper_functions import rmse_func as rmse\n",
    "from al_lib.helper_functions import report_model\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    ")  # also imports the neg_root_mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "# create a scorer which calculates Root Mean Squeared Error (RMSE)\n",
    "\n",
    "\n",
    "def rscv(\n",
    "    features,\n",
    "    target,\n",
    "    model,\n",
    "    param_distributions,\n",
    "    results_file,\n",
    "    random_state,\n",
    "    NoTrials=5,\n",
    "    nfolds=4,\n",
    "    n_jobs=5,\n",
    "    scoring=scoring, #\n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        features (_type_): _description_\n",
    "        target (_type_): _description_\n",
    "        model (_type_): _description_\n",
    "        param_distributions (_type_): _description_\n",
    "        results_file (_type_): _description_\n",
    "        random_state (_type_): _description_\n",
    "        NoTrials (int, optional): _description_. Defaults to 5.\n",
    "        nfolds (int, optional): _description_. Defaults to 4.\n",
    "        n_jobs (int, optional): _description_. Defaults to 5.\n",
    "        scoring (_type_, optional): _description_. Defaults to scoring.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # log the args\n",
    "    logging.info(\n",
    "        f\"Features: {features.shape}, Target: {target.shape}, Model: {model}, Param_distributions: {param_distributions}, Results File: {results_file} Random_state: {random_state}, NoTrials: {NoTrials}, nfolds: {nfolds}, n_jobs: {n_jobs}, Scoring: {scoring}\"\n",
    "    )\n",
    "\n",
    "    # prepare the result object 1\n",
    "    rscv_results = pd.DataFrame(columns=[\"model\", \"MAE\", \"RMSE\", \"params\"])\n",
    "\n",
    "    # define the train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.3, random_state=random_state\n",
    "    )\n",
    "    # create the result objects 2\n",
    "    rscv_rmse_inner = np.zeros(NoTrials)\n",
    "    rscv_rmse_outer = np.zeros(NoTrials)\n",
    "\n",
    "    for i in range(NoTrials):\n",
    "        logging.info(f\"Trial: {i} out of {NoTrials}\")\n",
    "        # split for nested cross-validation\n",
    "        inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=i)\n",
    "        outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=i)\n",
    "\n",
    "        # non-nested parameter search and scoring\n",
    "        rscv = RandomizedSearchCV(\n",
    "            model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=10,\n",
    "            cv=inner_cv,\n",
    "            random_state=random_state,\n",
    "            scoring=scoring,\n",
    "\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "\n",
    "        # fit\n",
    "        rscv.fit(X_train, y_train)\n",
    "        # make predictions to later estimate the generalization error\n",
    "        y_pred = cvp(rscv, X_test, y_test, cv=outer_cv, n_jobs=n_jobs)\n",
    "        all_predictions = np.zeros((len(y_test), NoTrials))\n",
    "        all_predictions[:, i] = y_pred\n",
    "        # calculate the RMSE for the inner and outer CV\n",
    "        rscv_rmse_inner[i] = rscv.best_score_\n",
    "\n",
    "        # calculate the RMSE for the outer CV\n",
    "        # rscv_rmse_outer[i] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        rscv_rmse_outer[i] = rmse(y_test, y_pred)\n",
    "        # store the results\n",
    "        rscv_results.loc[i, \"model\"] = rscv.estimator\n",
    "        rscv_results.loc[i, \"MAE\"] = mean_absolute_error(y_test, y_pred)\n",
    "        rscv_results.loc[i, \"RMSE\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        rscv_results.at[i, \"params\"] = rscv.best_params_\n",
    "        report_model(rscv)\n",
    "\n",
    "    # write results into outputifle\n",
    "    rscv_results.to_csv(results_file, index=False, mode=\"a\")\n",
    "\n",
    "    return rscv_results\n",
    "    # the goal of the rscv is to find the optimal hyperparameters\n",
    "    # for further investigation we want to store\n",
    "    # the 10 best model parameters and their scores\n",
    "    # both the inner and outer cv scores, as well as the score difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor - RSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf_rscv_results_file = f\"{RESULTS_PATH}rf_rscv_results.csv\"\n",
    "logging.info(f\"Results file: {rf_rscv_results_file}\")\n",
    "\n",
    "rscv(\n",
    "    features=X,\n",
    "    target=y,\n",
    "    model=rf,\n",
    "    param_distributions=rf_rscv_parameters,\n",
    "    results_file=rf_rscv_results_file,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# import the optimal model parameters\n",
    "rf_results = pd.read_csv(rf_rscv_results_file)\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_rf = rf_results.loc[rf_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_rf = dict(eval(optimal_params_str_rf))\n",
    "\n",
    "rf_opt = RandomForestRegressor(**optimal_params_rf)\n",
    "\n",
    "y_pred_rf = rf_opt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "title_str = (\n",
    "    f\"Random Forest: Actual vs. Predicted Values \\n params:\"\n",
    "    + optimal_params_str_rf\n",
    "    + f\"\\n RMSE = {root_mean_squared_error(y_test, y_pred_rf):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = f\"{FIGURE_PATH}avp_rf.png\"\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "rf_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_rf, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS Regressor - RSCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_decomposition\n",
    "\n",
    "pls = cross_decomposition.PLSRegression()\n",
    "pls_rscv_results_file = f\"{RESULTS_PATH}/pls_rscv_results.csv\"\n",
    "\n",
    "\n",
    "rscv(\n",
    "    features=X,\n",
    "    target=y,\n",
    "    model=pls,\n",
    "    param_distributions=pls_rscv_parameters,\n",
    "    results_file=pls_rscv_results_file,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# import the optimal model parameters\n",
    "pls_results = pd.read_csv(pls_rscv_results_file)\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_pls = pls_results.loc[pls_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_pls = dict(eval(optimal_params_str_pls))\n",
    "# fit the data with the optimal model parameters\n",
    "pls_opt = cross_decomposition.PLSRegression(**optimal_params_pls)\n",
    "\n",
    "y_pred_pls = pls_opt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "title_str = (\n",
    "    f\"PLS: Actual vs. Predicted Values \\n params:\"\n",
    "    + optimal_params_str_pls\n",
    "    + f\"\\n RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_pls)):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = (f\"{FIGURE_PATH}/avp_pls.png\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "pls_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_pls, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKR Regressor - RSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "\n",
    "krr = KRR()\n",
    "krr_rscv_results_file = f\"{RESULTS_PATH}/krr_rscv_results.csv\"\n",
    "\n",
    "rscv(\n",
    "    features=X,\n",
    "    target=y,\n",
    "    model=krr,\n",
    "    param_distributions=krr_rscv_parameters,\n",
    "    results_file=krr_rscv_results_file,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "\n",
    "# import the optimal model parameters\n",
    "krr_results = pd.read_csv(krr_rscv_results_file)\n",
    "\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_krr = krr_results.loc[krr_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_krr = dict(eval(optimal_params_str_krr))\n",
    "krr_opt = KRR(**optimal_params_krr)\n",
    "\n",
    "y_pred_krr = krr_opt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# break the optimal_params_str_krr string into more lines\n",
    "\n",
    "title_str = (\n",
    "    f\"KRR: Actual vs. Predicted Values \\n params:\"\n",
    "    + optimal_params_str_krr\n",
    "    + f\"\\n RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_krr)):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path_krr = (f\"{FIGURE_PATH}/avp_krr.png\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "krr_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_krr, param_dict, fig_path_krr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Regressor - RSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlp\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "\n",
    "mlp = MLP()\n",
    "mlp_rscv_results_file = f\"{RESULTS_PATH}/mlp_rscv_results.csv\"\n",
    "\n",
    "rscv_mpl = rscv(\n",
    "    features=X,\n",
    "    target=y,\n",
    "    model=mlp,\n",
    "    param_distributions=mlp_rscv_parameters,\n",
    "    results_file=mlp_rscv_results_file,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "\n",
    "# import the optimal model parameters\n",
    "mlp_results = pd.read_csv(mlp_rscv_results_file)\n",
    "\n",
    "# select the (optimal) model parameters with the lowest RMSE\n",
    "optimal_params_str_mlp = mlp_results.loc[mlp_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_mlp = dict(eval(optimal_params_str_mlp))\n",
    "\n",
    "# fit the data with the optimal model parameters\n",
    "mlp_opt = MLP(**optimal_params_mlp)\n",
    "\n",
    "y_pred_mlp = mlp_opt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# break the optimal_params_str_mlp string into more lines\n",
    "optimal_params_str_mlp_break = optimal_params_str_mlp.replace(\", \", \",\\n\")\n",
    "\n",
    "title_str = (\n",
    "    f\"MLP: Actual vs. Predicted Values \\n params:\"\n",
    "    + optimal_params_str_mlp_break\n",
    "    + f\"\\n RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_mlp)):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path_mlp = (f\"{FIGURE_PATH}/avp_mlp.png\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "mlp_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_mlp, param_dict, fig_path_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb_rscv_results_file = f\"{RESULTS_PATH}/xgb_rscv_results.csv\"\n",
    "\n",
    "rscv_xgb = rscv(\n",
    "    features=X,\n",
    "    target=y,\n",
    "    model=xgb,\n",
    "    param_distributions=xgb_rscv_parameters,\n",
    "    results_file=xgb_rscv_results_file,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "\n",
    "# import the optimal model parameters\n",
    "xgb_results = pd.read_csv(xgb_rscv_results_file)\n",
    "\n",
    "# round the results to 4 decimal places\n",
    "xgb_results = xgb_results.round(4)\n",
    "\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_xgb = xgb_results.loc[xgb_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_xgb = dict(eval(optimal_params_str_xgb))\n",
    "\n",
    "# fit the data with the optimal model parameters\n",
    "xgb_opt = XGBRegressor(**optimal_params_xgb)\n",
    "\n",
    "y_pred_xgb = xgb_opt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# break the optimal_params_str_ string into more lines\n",
    "optimal_params_str_xgb_break = optimal_params_str_xgb.replace(\", \", \",\\n\")\n",
    "\n",
    "\n",
    "title_str = (\n",
    "    f\"XGB: Actual vs. Predicted Values \\n params:\"\n",
    "    + optimal_params_str_xgb_break\n",
    "    + f\"\\n RMSE = {np.sqrt(mean_squared_error(y_test, y_pred_xgb)):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = (f\"{FIGURE_PATH}/avp_xgb.png\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "xgb_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_xgb, param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HGB\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "\n",
    "hbg = HGB()\n",
    "hgb_rscv_results_file = f\"{RESULTS_PATH}/hgb_rscv_results.csv\"\n",
    "\n",
    "rscv_hgb = rscv(\n",
    "    features=X,\n",
    "    target=y,\n",
    "    model=hbg,\n",
    "    param_distributions=hgb_rscv_parameters,\n",
    "    results_file=hgb_rscv_results_file,\n",
    "    random_state=random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "hgb_results = pd.read_csv(hgb_rscv_results_file)\n",
    "\n",
    "# select the model parameters with the lowest RMSE\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_hgb = hgb_results.loc[hgb_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_hgb = dict(eval(optimal_params_str_hgb))\n",
    "\n",
    "# fit the data with the optimal model parameters\n",
    "hgb_opt = HGB(**optimal_params_hgb)\n",
    "\n",
    "y_pred_hgb = hgb_opt.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# break the optimal_params_str_ string into more lines\n",
    "optimal_params_str_hgb_break = optimal_params_str_hgb.replace(\", \", \",\\n\")\n",
    "\n",
    "\n",
    "title_str = (\n",
    "    f\"HGB: Actual vs. Predicted Values \\n params:\"\n",
    "    + optimal_params_str_hgb_break\n",
    "    + f\"\\n RMSE = {root_mean_squared_error(y_test, y_pred_hgb):.2f}\"\n",
    ")\n",
    "fig_path = (f\"{FIGURE_PATH}/avp_xgb.png\")\n",
    "param_dict = {\"title\": title_str}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "xgb_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_xgb, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "In this section the goal is to document the packages which where used during the execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package informations\n",
    "from sklearn import show_versions\n",
    "\n",
    "show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Add data/time information\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Create the output name from the notebookname\n",
    "\n",
    "output_name = f\"{notebook_name.split('.')[0]}_{date}.html\"\n",
    "\n",
    "\n",
    "# Function to convert the notebook to HTML\n",
    "def convert_notebook_to_html(notebook_name, output_name):\n",
    "    # Use subprocess to call the jupyter nbconvert command\n",
    "    subprocess.call([\"jupyter\", \"nbconvert\", \"--to\", \"html\", notebook_name])\n",
    "    # Rename the output file\n",
    "    os.rename(notebook_name.split(\".\")[0] + \".html\", output_name)\n",
    "\n",
    "\n",
    "# Wait for a short period to ensure all cells have finished executing\n",
    "time.sleep(5)  # Adjust the sleep duration as needed\n",
    "\n",
    "# Convert the notebook to HTML\n",
    "convert_notebook_to_html(notebook_name, output_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
