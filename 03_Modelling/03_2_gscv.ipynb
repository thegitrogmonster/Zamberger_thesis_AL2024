{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSCV Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.clear()\n",
    "\n",
    "# Basepath\n",
    "basepath = \"../\"  # Project directory\n",
    "sys.path.append(basepath)\n",
    "\n",
    "# Data\n",
    "DATA_PATH = basepath + \"data\"\n",
    "\n",
    "# Results path\n",
    "RESULTS_PATH = basepath + \"03_Modelling/03_2_gscv/gscv_results/\"\n",
    "\n",
    "# Figure path\n",
    "FIGURE_PATH = basepath + \"03_Modelling/03_2_gscv/gscv_figures/\"\n",
    "\n",
    "# Path to environment\n",
    "ENV_PATH = \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib\"\n",
    "\n",
    "# Modelpath\n",
    "MODEL_PATH = basepath + \"models\"\n",
    "\n",
    "# Logging\n",
    "LOG_DIR = basepath + \"03_Modelling/03_2_gscv/\"\n",
    "\n",
    "# Active Learning library\n",
    "AL_PATH = basepath + \"al_lib\"\n",
    "\n",
    "# Add the paths\n",
    "sys.path.extend(\n",
    "    {DATA_PATH, FIGURE_PATH, ENV_PATH, MODEL_PATH, RESULTS_PATH, LOG_DIR, AL_PATH}\n",
    ")\n",
    "sys.path  # Check if the path is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the logging specifications from file 'logging_config.py'\n",
    "from al_lib.logging_config import create_logger\n",
    "import datetime\n",
    "\n",
    "# Add data/time information\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the notebook name and the output name\n",
    "notebook_name = \"03_2_gscv.ipynb\"  # Can also used when saving the notebook \n",
    "\n",
    "# Specify logging location\n",
    "log_file_name = f\"{notebook_name.split('.')[0]}_{date}.log\"\n",
    "log_file_path = f\"{LOG_DIR}/{log_file_name}\"\n",
    "\n",
    "# Get the logger\n",
    "logging = create_logger(__name__, log_file_path=log_file_path)\n",
    "\n",
    "# Usage of the logger as follows:\n",
    "logging.info(\"Logging started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dpsDeriv1200.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dps_deriv_1200 = pd.read_csv(\n",
    "    DATA_PATH + \"/dpsDeriv1200.csv\", sep=\",\", decimal=\".\", encoding=\"utf-8\"\n",
    ")\n",
    "data_dps_deriv_1200 = data_dps_deriv_1200.rename(columns=lambda x: x.replace(\"X\", \"\"))\n",
    "data_dps_deriv_1200 = data_dps_deriv_1200.rename(columns={\"Unnamed: 0\": \"Samplename\"})\n",
    "data_dps_deriv_1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch for the dataset\n",
    "# Select from (data_small, data_full, data_2nd_deriv, data_dps_deriv_1200) or other if implemented\n",
    "data_raw = data_dps_deriv_1200\n",
    "data_raw.dataset_name = \"data_dps_deriv_1200\"\n",
    "logging.info(f\"Dataset: {data_raw.dataset_name}\")\n",
    "logging.info(f\"Size of the dataset: {data_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the CV\n",
    "\n",
    "# Switch for testing mode (use only 10% of the data, among others)\n",
    "testing = False\n",
    "\n",
    "# Define a random state for randomized processes\n",
    "random_state = np.random.RandomState(202375)\n",
    "\n",
    "if testing == True:\n",
    "    nfolds = 5\n",
    "    NoTrials = 5\n",
    "    n_jobs = 20\n",
    "    save_model = False\n",
    "    # data = data_raw.sample(frac=0.15, random_state=random_state)\n",
    "    data = data_raw\n",
    "    # logging.info(f\"Size of the dataset reduced: {data.shape}\")\n",
    "else:\n",
    "    nfolds = 10\n",
    "    NoTrials = 15\n",
    "    n_jobs = 20\n",
    "    save_model = True\n",
    "    data = data_raw\n",
    "    logging.info(f\"Size of the dataset not reduced: {data.shape}\")\n",
    "\n",
    "# Log the modelling parameters\n",
    "logging.info(\n",
    "    f\"Testing for Cross Validation: {testing}, nfolds: {nfolds}, NoTrials: {NoTrials}, n_jobs: {n_jobs}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To apply the models we need to split the data into the variables and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into target and features\n",
    "# The goal is to predict the year column of the dataset using the spectral data\n",
    "X = data.select_dtypes(\"float\")\n",
    "y = data[\"year\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of columns with std = 0.0 in X\n",
    "logging.info(f\"Number of columns dropped, where std = 0.0 in X: {(X.std() == 0.0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns with std = 0.0\n",
    "X = X.loc[:, X.std() != 0.0]\n",
    "X.shape, y.shape\n",
    "logging.info(f\"Dimensions of X after dropping columns with std = 0.0: {X.shape}\")\n",
    "logging.info(f\"Dimensions of Y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "logging.info(f\"random split with testsize {test_size} into training and test sets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "# assert the shapes and raise an error if they are not equal\n",
    "assert X_train.shape[0] + X_test.shape[0] == X.shape[0]\n",
    "assert y_train.shape[0] + y_test.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Score metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# create a scorer which calculates Root Mean Squeared Error (RMSE)\n",
    "\n",
    "scoring = make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "# scoring = make_scorer(mean_squared_error, greater_is_better=False, squared=False)\n",
    "logging.info(f\"Scorer: {scoring}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Grid Search Crossvalidation (GSCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Definition\n",
    "\n",
    "Grid Search CV is usefull for the extensive testing of a defined parameter space. The results can consequently be used with confidence in their local validity. To create the local grid we will explore the space between the three most successful parameters in the rscv approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the parameters on basis of the rscv results\n",
    "\n",
    "Of special intrest are the min and max value for numerical variables and the optimal value according to rscv. \n",
    "For categorial variables, we  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSCV_DIR = basepath + \"03_Modelling/03_1_rscv/rscv_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_and_parse_csv(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file containing RSCV results, parses the parameters,\n",
    "    and returns a DataFrame with the parsed parameters.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    # drop rows where the entry in the 'params' column is 'params'\n",
    "    df = df[df['params'] != 'params']\n",
    "    # Parse the parameters\n",
    "    df['params'] = df['params'].apply(eval)\n",
    "    # split the params column into separate columns\n",
    "    df_params = pd.DataFrame(df['params'].to_list(), index=df.index)\n",
    "    df = df.drop(columns=['params'], axis = 1)\n",
    "    # merge the dataframes\n",
    "    df = pd.concat([df, df_params], axis=1)\n",
    "    logging.info (f\"Loaded and parsed {file_path} successfully.\")\n",
    "    # change the data types of the columns 'RMSE' and 'MAE' to float\n",
    "    df['RMSE'] = df['RMSE'].astype('float')\n",
    "    df['MAE'] = df['MAE'].astype('float')\n",
    "    cl_types = df.dtypes\n",
    "\n",
    "    for w,v in cl_types.items():\n",
    "        logging.info(f\"datatype of values in Column {w} : {v}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "pls_path = RSCV_DIR + 'pls_rscv_results.csv'\n",
    "rscv_results_pls = _read_and_parse_csv(pls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_grid(rscv_results, grid_file = None):\n",
    "    \"\"\"\n",
    "    Creates a grid from the RSCV results for GridSearchCV.\n",
    "    It spans the hyperparameter space for the top 5 models, and includes \n",
    "    the specific values for the optimal model. \n",
    "    For categorical hyperparameters each unique entry is integrated. \n",
    "    \"\"\"\n",
    "\n",
    "    # transform the rscv_results into a dataframe\n",
    "\n",
    "    # Select the 5 best performing models based on RMSE\n",
    "    best_models = rscv_results.nsmallest(5, \"RMSE\")\n",
    "    \n",
    "    # Initialize a empty grid\n",
    "    grid = {}\n",
    "    model_name = str(rscv_results['model'].iloc[0]).removeprefix('<class').removesuffix('>').removesuffix(\"()\")\n",
    "    # Iterate over each column (parameter) except 'model', 'MAE', and 'RMSE'\n",
    "    for col in rscv_results.columns.drop([\"model\", \"MAE\", \"RMSE\"]):\n",
    "        # Extract the minimum and maximum values for numerical columns\n",
    "        param_space = []\n",
    "        if rscv_results[col].dtype in ['float64']:\n",
    "            min_val = best_models[col].min()\n",
    "            max_val = best_models[col].max()\n",
    "            opt_val = best_models[col].iloc[0]\n",
    "            # Create an equidistant grid around the min and max values\n",
    "            param_space.append(np.linspace(min_val, max_val, 5))\n",
    "            # if the optimal value is not in the grid, add it\n",
    "            if opt_val not in param_space[-1]:\n",
    "                param_space.append([opt_val])\n",
    "        if rscv_results[col].dtype in ['int64']:\n",
    "            min_val = best_models[col].min()\n",
    "            max_val = best_models[col].max()\n",
    "            opt_val = best_models[col].iloc[0]\n",
    "            # Create an equidistant grid around the min and max values\n",
    "            param_space.append(np.linspace(min_val, max_val, 5))\n",
    "            if opt_val not in param_space[-1]:\n",
    "                param_space.append([opt_val])\n",
    "            #round the values and convert to int\n",
    "            param_space = [np.round(val).astype(int) for val in param_space]\n",
    "            # ensure there are no duplicates\n",
    "        # For categorical columns, use all unique values\n",
    "        else:\n",
    "            unique_vals = best_models[col].unique()\n",
    "            param_space.append(unique_vals)\n",
    "        # Store the parameter space in the grid\n",
    "            # key = column name \n",
    "            # value = parameter space\n",
    "        param_space = [np.unique(val) for val in param_space]\n",
    "        grid[col] = param_space[0]\n",
    "    if grid_file is not None:\n",
    "        # create the file\n",
    "        with open(grid_file, 'w') as file:\n",
    "            # write the gridname to the file \n",
    "            file.write(f\"gscv_parameters = {str(grid)}\")\n",
    "            logging.info(f\"Grid saved as {grid_file}\")\n",
    "            file.close()\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 14:50:02 - INFO - PLS model instantiated successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'copy': array([ True]),\n",
       " 'max_iter': array([201, 266, 330, 394, 459]),\n",
       " 'n_components': array([12, 13, 14, 15]),\n",
       " 'scale': array([False]),\n",
       " 'tol': array([0.30006406, 0.32350801, 0.34695195, 0.37039589, 0.39383984])}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the grid extraction methods for the pls\n",
    "\n",
    "def _test_grid(): \n",
    "    \"\"\" Test the grid extraction method for PLS.\n",
    "    Returns 'None' and a logging entry if the test is successful.\n",
    "    \"\"\"\n",
    "    grid_pls = _create_grid(rscv_results_pls)\n",
    "    # instantiate the pls model with the first entry in the grid dict\n",
    "    mock_parms = {key: value[0] for key, value in grid_pls.items()}\n",
    "    # create a PLS model\n",
    "    try: \n",
    "        PLSRegression(**mock_parms)\n",
    "        logging.info(f\"PLS model instantiated successfully.\")\n",
    "    except:\n",
    "        logging.error(f\"PLS model could not be instantiated.\")\n",
    "    return grid_pls\n",
    "\n",
    "_test_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changed to markdown\n",
    "\n",
    "models = {\n",
    "    \"rf\": RandomForestRegressor(),\n",
    "    \"pls\": PLSRegression(),\n",
    "    \"krr\": KRR(),\n",
    "    \"mlp\": MLP(),\n",
    "    \"xgb\": XGBRegressor(),\n",
    "    \"hgb\": HGB(),\n",
    "}\n",
    "\n",
    "# Prepare objects to store the results\n",
    "for model in models.keys():\n",
    "    globals()[f\"{model}_gscv_results\"] = pd.DataFrame(\n",
    "        columns=[\"model\", \"MAE\", \"RMSE\", \"params\"]\n",
    "    )\n",
    "    print(f\"{model}_gscv_parameters successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from al_lib.helper_functions import rmse_func as rmse\n",
    "from al_lib.helper_functions import report_model\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    ")  # also imports the neg_root_mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def gscv(\n",
    "    features,\n",
    "    target,\n",
    "    model,\n",
    "    param_grid,\n",
    "    results_file,\n",
    "    NoTrials=5,\n",
    "    nfolds=4,\n",
    "    n_jobs=5,\n",
    "    scoring=scoring, \n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        features (_type_): _description_\n",
    "        target (_type_): _description_\n",
    "        model (_type_): _description_\n",
    "        param_distributions (_type_): _description_\n",
    "        results_file (_type_): _description_\n",
    "        random_state (_type_): _description_\n",
    "        NoTrials (int, optional): _description_. Defaults to 5.\n",
    "        nfolds (int, optional): _description_. Defaults to 4.\n",
    "        n_jobs (int, optional): _description_. Defaults to 5.\n",
    "        scoring (_type_, optional): _description_. Defaults to scoring.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # log the args\n",
    "    logging.info(\n",
    "        f\"Features: {features.shape}, Target: {target.shape}, Model: {model}, Param_distributions: {param_grid}, Results File: {results_file} Random_state: {random_state}, NoTrials: {NoTrials}, nfolds: {nfolds}, n_jobs: {n_jobs}, Scoring: {scoring}\"\n",
    "    )\n",
    "\n",
    "    # prepare the result object\n",
    "    gscv_results = pd.DataFrame(columns=[\"model\", \"MAE\", \"RMSE\", \"params\"])\n",
    "\n",
    "    # define the train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.3, random_state=random_state\n",
    "    )\n",
    "    # create the result objects 2\n",
    "    gscv_rmse_inner = np.zeros(NoTrials)\n",
    "    gscv_rmse_outer = np.zeros(NoTrials)\n",
    "\n",
    "    for i in range(NoTrials):\n",
    "        logging.info(f\"Trial: {i} out of {NoTrials}\")\n",
    "        # split for nested cross-validation\n",
    "        inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=i)\n",
    "        outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=i)\n",
    "\n",
    "        # non-nested parameter search and scoring\n",
    "        gscv = GridSearchCV(\n",
    "            model,\n",
    "            param_grid=param_grid,\n",
    "            cv=inner_cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs,\n",
    "                            )\n",
    "\n",
    "        # fit\n",
    "        gscv.fit(X_train, y_train)\n",
    "        # make predictions to later estimate the generalization error\n",
    "        y_pred = cvp(gscv, X_test, y_test, cv=outer_cv, n_jobs=n_jobs)\n",
    "        all_predictions = np.zeros((len(y_test), NoTrials))\n",
    "        all_predictions[:, i] = y_pred\n",
    "        # calculate the RMSE for the inner and outer CV\n",
    "        gscv_rmse_inner[i] = gscv.best_score_\n",
    "        # calculate the RMSE for the outer CV\n",
    "        gscv_rmse_outer[i] = rmse(y_test, y_pred)\n",
    "        # store the results\n",
    "        gscv_results.loc[i, \"model\"] = gscv.estimator\n",
    "        gscv_results.loc[i, \"MAE\"] = mean_absolute_error(y_test, y_pred)\n",
    "        gscv_results.loc[i, \"RMSE\"] = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        gscv_results.at[i, \"params\"] = gscv.best_params_\n",
    "        report_model(gscv)\n",
    "\n",
    "    # write results into outputifle\n",
    "    gscv_results.to_csv(results_file, index=False, mode=\"a\")\n",
    "\n",
    "    return gscv_results\n",
    "    # the goal of the rscv is to find the optimal hyperparameters\n",
    "    # for further investigation we want to store\n",
    "    # the 10 best model parameters and their scores\n",
    "    # both the inner and outer cv scores, as well as the score difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor - GSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "# Import parameters\n",
    "rf_path = RSCV_DIR + \"rf_rscv_results.csv\"\n",
    "rscv_results_rf = _read_and_parse_csv(rf_path)\n",
    "# save the grid as a txt.file\n",
    "grid_file_rf = RESULTS_PATH + \"rf_grid.txt\"\n",
    "grid_rf = _create_grid(rscv_results_rf, grid_file_rf)\n",
    "logging.info(f\"Grid for RF {grid_rf}\")\n",
    "\n",
    "# Define the results file\n",
    "rf_gscv_results_file = f\"{RESULTS_PATH}rf_gscv_results.csv\"\n",
    "logging.info(f\"Results file: {rf_gscv_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing GSCV for RF\n",
    "\n",
    "rf_gscv_results = gscv(\n",
    "    features = X,\n",
    "    target = y,\n",
    "    model = rf, \n",
    "    param_grid = grid_rf,\n",
    "    results_file = rf_gscv_results_file,\n",
    "    NoTrials=2,\n",
    "    nfolds=2,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring=scoring, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# import the optimal model parameters\n",
    "rf_results = pd.read_csv(rf_gscv_results_file)\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_rf = rf_results.loc[rf_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_rf = dict(eval(optimal_params_str_rf))\n",
    "\n",
    "rf_opt = RandomForestRegressor(**optimal_params_rf, random_state=random_state)\n",
    "\n",
    "y_pred_rf = rf_opt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "title_str_params = optimal_params_str_rf.replace(\"np.int64\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "title_str = (\n",
    "    f\"Random Forest: Actual vs. Predicted Values \\n params:\"\n",
    "    + title_str_params\n",
    "    + f\"\\n RMSE = {root_mean_squared_error(y_test, y_pred_rf):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = f\"{FIGURE_PATH}avp_rf.png\"\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 8)\n",
    "rf_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_rf, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS Regressor - GSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = PLSRegression()\n",
    "# Import parameters\n",
    "pls_path = RSCV_DIR + \"pls_rscv_results.csv\"\n",
    "rscv_results_pls = _read_and_parse_csv(pls_path)\n",
    "# save the grid as a txt.file\n",
    "grid_file_pls = RESULTS_PATH + \"pls_grid.txt\"\n",
    "grid_pls = _create_grid(rscv_results_pls, grid_file_pls)\n",
    "logging.info(f\"Grid for PLS: {grid_pls}\")\n",
    "\n",
    "# Define the results file\n",
    "pls_gscv_results_file = f\"{RESULTS_PATH}pls_gscv_results.csv\"\n",
    "logging.info(f\"Results file: {pls_gscv_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pls_gscv_results = gscv(\n",
    "    features = X,\n",
    "    target = y,\n",
    "    model = pls, \n",
    "    param_grid = grid_pls,\n",
    "    results_file = pls_gscv_results_file,\n",
    "    NoTrials=2,\n",
    "    nfolds=2,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring=scoring, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# import the optimal model parameters\n",
    "pls_results = pd.read_csv(pls_gscv_results_file)\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_pls = pls_results.loc[pls_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_pls = dict(eval(optimal_params_str_pls))\n",
    "\n",
    "pls_opt = PLSRegression(**optimal_params_pls)\n",
    "\n",
    "y_pred_pls = pls_opt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "title_str_params = optimal_params_str_pls.replace(\"np.int64\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "title_str = (\n",
    "    f\"PLS Regression: Actual vs. Predicted Values \\n params:\"\n",
    "    + title_str_params\n",
    "    + f\"\\n RMSE = {root_mean_squared_error(y_test, y_pred_pls):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = f\"{FIGURE_PATH}avp_pls.png\"\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 8)\n",
    "rf_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_pls, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KRR - GSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "krr = KRR()\n",
    "# Import parameters\n",
    "krr_path = RSCV_DIR + 'krr_rscv_results.csv'\n",
    "rscv_results_krr = _read_and_parse_csv(krr_path)\n",
    "# save the grid as a txt.file\n",
    "grid_file_krr = RESULTS_PATH + \"krr_grid.txt\"\n",
    "grid_krr = _create_grid(rscv_results_krr, grid_file_krr)\n",
    "logging.info(f\"Grid for krr: {grid_krr}\")\n",
    "\n",
    "# Define the results file\n",
    "krr_gscv_results_file = f\"{RESULTS_PATH}krr_gscv_results.csv\"\n",
    "logging.info(f\"Results file: {krr_gscv_results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr_gscv_results = gscv(\n",
    "    features = X,\n",
    "    target = y,\n",
    "    model = krr, \n",
    "    param_grid = grid_krr,\n",
    "    results_file = krr_gscv_results_file,\n",
    "    NoTrials=2,\n",
    "    nfolds=2,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring=scoring, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# import the optimal model parameters\n",
    "krr_results = pd.read_csv(krr_gscv_results_file)\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_krr = krr_results.loc[krr_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_krr = dict(eval(optimal_params_str_krr))\n",
    "\n",
    "krr_opt = KRR(**optimal_params_krr)\n",
    "\n",
    "y_pred_krr = krr_opt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "title_str_params = optimal_params_str_krr.replace(\"np.int64\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"np.float64\", \"\")\n",
    "\n",
    "title_str = (\n",
    "    f\"Kernel-Ridge Regression: Actual vs. Predicted Values \\n params:\"\n",
    "    + title_str_params\n",
    "    + f\"\\n RMSE = {root_mean_squared_error(y_test, y_pred_krr):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = f\"{FIGURE_PATH}avp_krr.png\"\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 8)\n",
    "krr_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_krr, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp Regressor - GSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlp\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "\n",
    "# Import parameters\n",
    "mlp_path = RSCV_DIR + 'mlp_rscv_results.csv'\n",
    "rscv_results_mlp = _read_and_parse_csv(mlp_path)\n",
    "\n",
    "# handle the NaN values in the mlp results['max_iter'] column\n",
    "    # replace the NaN values with the median of the column\n",
    "rscv_results_mlp['max_iter'] = rscv_results_mlp['max_iter'].fillna(rscv_results_mlp['max_iter'].median())\n",
    "    # change the data type of the column to int\n",
    "rscv_results_mlp['max_iter'] = rscv_results_mlp['max_iter'].astype(int)\n",
    "\n",
    "# save the grid as a txt.file\n",
    "grid_file_mlp = RESULTS_PATH + \"mlp_grid.txt\"\n",
    "grid_mlp = _create_grid(rscv_results_mlp, grid_file_mlp)\n",
    "logging.info(f\"Grid for mlp {grid_mlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlp\n",
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "mlp_gscv_results_file = f\"{RESULTS_PATH}/mlp_gscv_results.csv\"\n",
    "logging.info(f\"Results file: {mlp_gscv_results_file}\")\n",
    "\n",
    "mlp_gscv_results = gscv(\n",
    "    features = X,\n",
    "    target = y,\n",
    "    model = mlp, \n",
    "    param_grid = grid_mlp,\n",
    "    results_file = mlp_gscv_results_file,\n",
    "    NoTrials=2,\n",
    "    nfolds=2,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring=scoring, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "\n",
    "# import parameters\n",
    "mlp_results = pd.read_csv(mlp_gscv_results_file)\n",
    "optimal_params_str_mlp = mlp_results.loc[mlp_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_mlp = dict(eval(optimal_params_str_mlp))\n",
    "\n",
    "mlp_opt = MLP(**optimal_params_mlp)\n",
    "\n",
    "y_pred_mlp = mlp_opt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "title_str_params = optimal_params_str_mlp.replace(\"np.int64\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"np.float64\", \"\")\n",
    "\n",
    "title_str = (\n",
    "    f\"MLP: Actual vs. Predicted Values \\n params:\"\n",
    "    + title_str_params\n",
    "    + f\"\\n RMSE = {root_mean_squared_error(y_test, y_pred_mlp):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = f\"{FIGURE_PATH}avp_mlp.png\"\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 8)\n",
    "mlp_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_mlp, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HGB - GSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HGB\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "\n",
    "hgb_path = RSCV_DIR + 'hgb_rscv_results.csv'\n",
    "rscv_results_hgb = _read_and_parse_csv(hgb_path)\n",
    "\n",
    "# save the grid as a txt.file\n",
    "grid_file_hgb = RESULTS_PATH + \"hgb_grid.txt\"\n",
    "grid_hgb = _create_grid(rscv_results_hgb, grid_file_hgb)\n",
    "logging.info(f\"Grid for hgb {grid_hgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb = HGB()\n",
    "\n",
    "hgb_gscv_results_file = f\"{RESULTS_PATH}/hgb_gscv_results.csv\"\n",
    "logging.info(f\"Results file: {hgb_gscv_results_file}\")\n",
    "\n",
    "hgb_gscv_results = gscv(\n",
    "    features = X,\n",
    "    target = y,\n",
    "    model = hgb, \n",
    "    param_grid = grid_hgb,\n",
    "    results_file = hgb_gscv_results_file,\n",
    "    NoTrials=2,\n",
    "    nfolds=2,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring=scoring, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# import the optimal model parameters\n",
    "hgb_results = pd.read_csv(hgb_gscv_results_file)\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_hgb = hgb_results.loc[hgb_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_hgb = dict(eval(optimal_params_str_hgb))\n",
    "\n",
    "hgb_opt = hgb(**optimal_params_hgb)\n",
    "\n",
    "y_pred_hgb = hgb_opt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "title_str_params = optimal_params_str_hgb.replace(\"np.int64\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"np.float64\", \"\")\n",
    "\n",
    "title_str = (\n",
    "    f\"Kernel-Ridge Regression: Actual vs. Predicted Values \\n params:\"\n",
    "    + title_str_params\n",
    "    + f\"\\n RMSE = {root_mean_squared_error(y_test, y_pred_hgb):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = f\"{FIGURE_PATH}avp_hgb.png\"\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 8)\n",
    "hgb_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_hgb, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB - GSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# Import the XGB results\n",
    "xgb_path = RSCV_DIR + 'xgb_rscv_results.csv'\n",
    "rscv_results_xgb = _read_and_parse_csv(xgb_path)\n",
    "#save the grid as a txt.file\n",
    "grid_file_xgb = RESULTS_PATH + \"xgb_grid.txt\"\n",
    "\n",
    "grid_xgb = _create_grid(rscv_results_xgb, grid_file_xgb)\n",
    "logging.info(f\"Grid for XGB: {grid_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gscv_results_file = f\"{RESULTS_PATH}/xgb_gscv_results.csv\"\n",
    "logging.info(f\"Results file: {xgb_gscv_results_file}\")\n",
    "\n",
    "xgb_gscv_results = gscv(\n",
    "    features = X,\n",
    "    target = y,\n",
    "    model = xgb, \n",
    "    param_grid = grid_xgb,\n",
    "    results_file = xgb_gscv_results_file,\n",
    "    NoTrials=2,\n",
    "    nfolds=2,\n",
    "    n_jobs=n_jobs,\n",
    "    scoring=scoring, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the actual vs. predicted plot\n",
    "from al_lib.helper_functions import plot_actual_vs_pred\n",
    "\n",
    "# import the optimal model parameters\n",
    "xgb_results = pd.read_csv(xgb_rscv_results_file)\n",
    "# select the model parameters with the lowest RMSE\n",
    "optimal_params_str_xgb = xgb_results.loc[xgb_results[\"RMSE\"].idxmin(), \"params\"]\n",
    "optimal_params_xgb = dict(eval(optimal_params_str_xgb))\n",
    "\n",
    "xgb_opt = xgb(**optimal_params_xgb)\n",
    "\n",
    "y_pred_xgb = xgb_opt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "title_str_params = optimal_params_str_xgb.replace(\"np.int64\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"np.float64\", \"\")\n",
    "\n",
    "title_str = (\n",
    "    f\"Kernel-Ridge Regression: Actual vs. Predicted Values \\n params:\"\n",
    "    + title_str_params\n",
    "    + f\"\\n RMSE = {root_mean_squared_error(y_test, y_pred_xgb):.2f}\"\n",
    ")\n",
    "\n",
    "param_dict = {\"title\": title_str}\n",
    "fig_path = f\"{FIGURE_PATH}avp_xgb.png\"\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(10, 8)\n",
    "xgb_avp_plot = plot_actual_vs_pred(ax, y_test, y_pred_xgb, param_dict, fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "In this section the goal is to document the packages which where used during the execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package informations\n",
    "from sklearn import show_versions\n",
    "\n",
    "show_versions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
