{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the Initial Model \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Notebook is develped to identify and specify the models, which will be used to apply the Active Learning strategies on. At least two models will be created, as described in the initial Research Proposal: \n",
    "1. PLS-Regression-Model \n",
    "2. Random-Forest-Regression-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperation\n",
    "\n",
    "To work in python, various libraries are needed. So the neccessary libraries are imported in the next cell. \n",
    "\n",
    "The code is developed inspired by the machine learining course by [Peter Sykacek](peter.sykacek[at]boku.ac.at) in the winter of 2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python312.zip',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/lib-dynload',\n",
       " '',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib/python3.12/site-packages',\n",
       " './',\n",
       " './server_files/ml_group/course.lib',\n",
       " './results/03_modeling_results',\n",
       " './figures/03_modeling_figures',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib',\n",
       " './data',\n",
       " './models',\n",
       " './models',\n",
       " './',\n",
       " './server_files/ml_group/course.lib',\n",
       " './results/03_modeling_results',\n",
       " './figures/03_modeling_figures',\n",
       " '/home/fhwn.ac.at/202375/.conda/envs/thesis/lib',\n",
       " './data',\n",
       " './models']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# sys.path.clear()\n",
    "\n",
    "# Basepath\n",
    "basepath=\"./\" # Project directory\n",
    "sys.path.append(basepath)\n",
    "sys.path.append(basepath+\"server_files/ml_group/course.lib\")\n",
    "\n",
    "# Data\n",
    "DATA_PATH = basepath + \"data\"\n",
    "\n",
    "#Figure\n",
    "FIGURE_PATH = basepath + \"figures/03_modeling_figures\"\n",
    "\n",
    "# Modelpath\n",
    "MODEL_PATH = basepath + \"models\"\n",
    "\n",
    "# Path to environment\n",
    "\n",
    "ENV_PATH = \"/home/fhwn.ac.at/202375/.conda/envs/thesis/lib\"\n",
    "\n",
    "# Resultspath\n",
    "RESULTS_PATH = basepath + \"results/03_modeling_results\"\n",
    "\n",
    "# Add the paths\n",
    "sys.path.extend({DATA_PATH, FIGURE_PATH, MODEL_PATH, ENV_PATH, RESULTS_PATH})\n",
    "sys.path # Check if the path is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## timing the full notebook\n",
    "import time\n",
    "nb_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Path to store the ML Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the path to save the ml models\n",
    "\n",
    "MODEL_PATH = basepath + \"models\"\n",
    "sys.path.append(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ml_lib as mlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### turn off convergence warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate various models an import of the respective functions from preexisting packages is neccessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch Crossvalidation\n",
    "\n",
    "[sklearn GSCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "Exhaustive search over specified parameter values for an estimator.\n",
    "Important members are fit, predict.\n",
    "\n",
    "* GridSearchCV implements a \"fit\" and a \"score\" method.\n",
    "* It also implements \"score_samples\", \"predict\", \"predict_proba\", \"decision_function\", \"transform\" and \"inverse_transform\" if they are implemented in the estimator used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV as GSCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Parameter Optimization\n",
    "\n",
    "[sklearn RandomizedSearchCV](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)  \n",
    "\n",
    " RandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "\n",
    "A budget can be chosen independent of the number of parameters and possible values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold cross-validator.\n",
    "[sklearn KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n",
    "\n",
    "Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).\n",
    "\n",
    "Each fold is then used once as a validation while the k - 1 remaining folds form the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge\n",
    "\n",
    "[sklearn KRR](https://scikit-learn.org/stable/modules/kernel_ridge.html#kernel-ridge-regression)\n",
    "\n",
    "Kernel ridge regression (KRR) [M2012] combines Ridge regression and classification (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge as KRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inspection\n",
    "\n",
    "[sklearn cv_results_](https://scikit-learn.org/stable/modules/grid_search.html#analyzing-results-with-the-cv-results-attribute)\n",
    "\n",
    "\"The cv_results_ attribute contains useful information for analyzing the results of a search. It can be converted to a pandas dataframe with df = pd.DataFrame(est.cv_results_).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the sample data will be imported. \n",
    "\n",
    "Currently 2 Datasets are of interest for us: \n",
    "1. PS20191107_gegl.csv\n",
    "2. dps1200.csv\n",
    "\n",
    "The differences are that the first is a dataframe containing the data unmodified and full. It was used to generate the later, which contains only selected sections of the spectra. The Wavelengths of this dataset were selected by discarding Wavelengths, based on critieria ???\n",
    "\n",
    "**TODO**: Research the criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS20191107 (Full Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/PS20191107_gegl.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_full \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasepath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/PS20191107_gegl.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m data_full\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/thesis/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/PS20191107_gegl.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_full = pd.read_csv(basepath+\"data/PS20191107_gegl.csv\", \n",
    "                            sep=\";\", decimal=\",\", encoding=\"utf-8\")\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive basic characteristics for each variable\n",
    "data_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full.groupby('type')[['year']].agg(['max', 'mean', 'min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset dps1200.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = pd.read_csv(basepath+\"data/dps1200.csv\", \n",
    "                            sep=\",\", decimal=\".\", encoding=\"utf-8\")\n",
    "data_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the column headers\n",
    "\n",
    "# data_1200.rename(lambda x: x[1:], axis='columns')\n",
    "data_small = data_small.rename(columns=lambda x: x.replace('X', ''))\n",
    "data_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small.describe()\n",
    "# describe() gives some basic statistics for numeric columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small.describe(include=\"object\")\n",
    "# describe() gives some basic statistics for numeric columns, \n",
    "# categorial columns are included with the option include=\"object\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the CV\n",
    "\n",
    "# Switch for the dataset\n",
    "    # Select from (data_1200, data_full) or other if implemented\n",
    "data = data_small\n",
    "\n",
    "# Switch for testing mode (use only 10% of the data, among others)\n",
    "testing = True\n",
    "\n",
    "# Define a random state for randomized processes\n",
    "random_state = np.random.RandomState(202375)\n",
    "\n",
    "# Define a metric for model evaluation\n",
    "cv_scorer = 'neg_mean_squared_error'\n",
    "\n",
    "######################################################\n",
    "if testing == True:\n",
    "    nfolds = 3\n",
    "    NoTrials = 2\n",
    "    n_jobs = 20\n",
    "    save_model = False\n",
    "    print(\"Testing mode for Cross Validation\")\n",
    "    print(\"Splitting the data for faster modelling\")\n",
    "    data = data.sample(frac=0.1)\n",
    "else:\n",
    "    nfolds = 10\n",
    "    NoTrials = 15\n",
    "    n_jobs = 40\n",
    "    save_model = True\n",
    "    print(\"Extensive mode for Cross Validation\")\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.select_dtypes('float')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['year']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "During this Project, we will generate statistical model with a random fraction of the dataset. The remainder will be retained to be used as test values to estimate the accuracy of the model and potentially detect overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (RSCV)\n",
    "Implemented: \n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF Define the parameters for the CV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rf = RandomForestRegressor() # default criterion to evaluate the quality of the split is the ”squared_error”\n",
    "\n",
    "param_distribs = {'n_estimators': randint(low=3, high=150), # for hyperparameter with discrete values \n",
    "                  'min_samples_split': randint(low=2, high=20), \n",
    "                  'max_depth': randint(low=1, high=20), \n",
    "                  'min_samples_leaf': randint(low=1, high=10),\n",
    "                  }\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "rf_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "rf_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    rf_rscv = RandomizedSearchCV(\n",
    "        rf, # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=1, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    rf_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    rf_rscv_rmse1[i] = np.sqrt(-rf_rscv.best_score_)\n",
    "    y_pred_rf = cvp(rf_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    rf_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_rf))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"Execution time: {execution_time} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal Parameters according to RSCV:', rf_rscv.best_params_)\n",
    "print('best score', rf_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF with optimal parameters\n",
    "\n",
    "extract the best parameters and run the RF Regression with the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "rf_opt = RandomForestRegressor(**rf_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "rf_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "y_pred_rf = rf_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS (RSCV)\n",
    "Implemented:\n",
    "\n",
    "\n",
    "*TODO*\n",
    "\n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter search should be limited in regards to the numebr of components to keep:\n",
    "# \"Should be in [1, min(n_samples, n_features, n_targets)]\" sklearn\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the CV\n",
    "from sklearn import cross_decomposition\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pls = cross_decomposition.PLSRegression()\n",
    "\n",
    "param_distribs = {'n_components': randint(low=1, high=90), #  should be in [1, min(n_samples, n_features, n_targets = 90)].\n",
    "                  'max_iter': randint(low=2, high=700), \n",
    "                  }\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "pls_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "pls_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    pls_rscv = RandomizedSearchCV(\n",
    "        pls, # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=0, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    pls_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    pls_rscv_rmse1[i] = np.sqrt(-pls_rscv.best_score_)\n",
    "    y_pred_pls = cvp(pls_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    pls_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_pls))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"Execution time: {execution_time} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal Parameters according to RSCV:', pls_rscv.best_params_)\n",
    "print('best score' ,pls_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLS with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "pls_opt = cross_decomposition.PLSRegression(**pls_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "pls_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "\n",
    "y_pred_pls = pls_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_pls = np.sqrt(mean_squared_error(y_test, y_pred_pls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_pls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KRR with RBF (RSCV)\n",
    "Implemented:\n",
    "\n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters  \n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRR with RBF \n",
    "\n",
    "# alpha: Regularization strength\n",
    "\n",
    "param_distribs = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3], \n",
    "                  \"gamma\": np.logspace(-2, 2, 7)}\n",
    "\n",
    "# param_distribs = {\"alpha\": np.logspace(0.0001, 0.1), \n",
    "#                  \"gamma\": np.logspace(0.0001, 0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-2, 2, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge as KRR\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "krr_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "krr_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    krr_rscv = RandomizedSearchCV(\n",
    "        KRR(kernel='rbf'), # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=1, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    krr_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    krr_rscv_rmse1[i] = np.sqrt(-krr_rscv.best_score_)\n",
    "    y_pred_krr = cvp(krr_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    krr_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_krr))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/ 60\n",
    "print(f\"Execution time: {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal Parameters according to RSCV:', krr_rscv.best_params_)\n",
    "print('best score' ,krr_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR (rbf) with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "krr_opt = KRR(**krr_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "krr_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "\n",
    "y_pred_krr = krr_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_krr = np.sqrt(mean_squared_error(y_test, y_pred_krr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_krr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (RSCV)\n",
    "\n",
    "This Method, the multi-layer perceptron creates a neural network, where neurons are organized in three or more layers (1 input-, n hidden-, and 1 output-layer). The MLP is based on a threshold logic unit (TLU, sometimes linear threshold unit LTU). A TLU recieves input from its connections and calculates 'weights' from the sum of all inputs and calculates a step function. Common step functions are the *Heaviside step function* or *sign function*. \n",
    "\n",
    "To compute the outputs of a single fully connnected layer the following eq. can be used \n",
    "\n",
    "(citation: A. Géron, Hands-on machine learning with Scikit-Learn and TensorFlow concepts, tools, and techniques to build intelligent systems, 2nd ed. O’Reilly Media, Inc., 2019. p.283)\n",
    "‌\n",
    "\n",
    "$$h_{W,b}(X) = \\phi(WX + b)$$\n",
    "\n",
    "Implemented: \n",
    "\n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters \n",
    "\n",
    "*TODO* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter Distribution for mlp\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_distribs = {\"hidden_layer_sizes\": randint(low=50, high=200), # number of neurons in each layer\n",
    "                  \"activation\": ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                  \"solver\": ['lbfgs','sgd', 'adam'],\n",
    "                  'alpha': uniform(loc=0.0001, scale=0.1),\n",
    "                  'early_stopping': [True, False],  \n",
    "                  'validation_fraction': uniform(loc=0.1, scale=0.1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor as MLP\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "mlp_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "mlp_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    mlp_rscv = RandomizedSearchCV(\n",
    "        MLP(), # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=0, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    mlp_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    mlp_rscv_rmse1[i] = np.sqrt(-mlp_rscv.best_score_)\n",
    "    y_pred_mlp = cvp(mlp_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    mlp_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_mlp))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/ 60\n",
    "print(f\"Execution time: {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal Parameters according to RSCV:', mlp_rscv.best_params_)\n",
    "print('best score' ,mlp_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "mlp_opt = MLP(**mlp_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "mlp_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "y_pred_mlp = mlp_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (RSCV)\n",
    "\n",
    "Implemented:\n",
    "\n",
    "- import\n",
    "\n",
    "*TODO*\n",
    "\n",
    "\n",
    "- parameter distribution  \n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgboost.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform the data into the XGBoost data class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details see [datacamp](https://www.datacamp.com/tutorial/xgboost-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "dtrain_reg = xgboost.DMatrix(X_train, y_train)\n",
    "dtest_reg = xgboost.DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the objective\n",
    "\n",
    "XGBoost will be used here for a regression problem, with the objective to minimize the squared error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"reg:squarederror\", \n",
    "          \"tree_method\": \"hist\"} # \"gpu_hist\" for gpu only, set to 'hist' if on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "n = 500 # number of rounds\n",
    "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")] # specify the data for evaluation\n",
    "\n",
    "model = xgboost.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   evals=evals,\n",
    "   verbose_eval = 20, \n",
    "   early_stopping_rounds=20,\n",
    ")\n",
    "\n",
    "# [60]\ttrain-rmse:3.18431\tvalidation-rmse:123.78980\n",
    "# [71]\ttrain-rmse:1.87073\tvalidation-rmse:123.80743"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "results = xgboost.cv(\n",
    "   params,\n",
    "   dtrain_reg,\n",
    "   num_boost_round=n,\n",
    "   nfold=5,\n",
    "   early_stopping_rounds=20\n",
    ")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rmse = results['test-rmse-mean'].min()\n",
    "\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acessing the xgboost eval metrics via sklearn\n",
    "\n",
    "from tutorial of [xgboost](https://xgboost.readthedocs.io/en/latest/python/examples/sklearn_evals_result.html#demo-for-accessing-the-xgboost-eval-metrics-by-using-sklearn-interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "dtrain_reg = xgboost.DMatrix(X_train, y_train)\n",
    "dtest_reg = xgboost.DMatrix(X_test, y_test)\n",
    "\n",
    "params = {\"objective\": [\"reg:squarederror\"], \n",
    "          \"tree_method\": [\"hist\"]}\n",
    "\n",
    "XGB = xgboost.XGBModel(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "start_time = time.time()\n",
    "# Instantiate the regressor\n",
    "XGB = XGBRegressor()\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_estimators\": randint(100,500),\n",
    "    \"max_depth\": randint(3,100)\n",
    "}\n",
    "\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "xgb_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "xgb_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    xgb_rscv = RandomizedSearchCV(\n",
    "        XGB, # regressor\n",
    "        param_distributions = param_distribs, # hyperparameter space\n",
    "        n_iter = 10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv = inner_cv, # \"Determines the cross-validation splitting strategy\" [sklearn]\n",
    "        scoring = cv_scorer, \n",
    "        random_state = random_state, \n",
    "        verbose = 1, \n",
    "        n_jobs = n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    xgb_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    xgb_rscv_rmse1[i] = np.sqrt(-xgb_rscv.best_score_)\n",
    "    y_pred_xgb = cvp(xgb_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    xgb_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_xgb))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/ 60\n",
    "print(f\"Execution time: {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal Parameters according to RSCV:', xgb_rscv.best_params_)\n",
    "print('best score' ,xgb_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "best_params = xgb_rscv.best_params_\n",
    "\n",
    "xgb_opt = XGBRegressor(**xgb_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "xgb_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "y_pred_xgb = xgb_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram-based Gradient Boosting Regression Tree\n",
    "\n",
    "*TODO*\n",
    "- parameter distribution\n",
    "- Train  \n",
    "- Test  \n",
    "- CV Results  \n",
    "- Optimal Model Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGB\n",
    "\n",
    "# Define parameters for HGB\n",
    "#param_distribs = {\n",
    "#    'learning_rate':randint(low=0.001,high=1),\n",
    "#    'max_iter':randint(low=5,high=250), \n",
    "#     'max_leaf_nodes': randint(low=2,high=50, scale = 1)\n",
    "#                  }\n",
    "param_distribs = {'max_iter': [5,10], \n",
    "                  'max_leaf_nodes': [15,31,40],\n",
    "                  }\n",
    "\n",
    "# loop the fitting with splits of the data\n",
    "hgb_rscv_rmse1 = np.zeros((NoTrials, 1))\n",
    "hgb_rscv_rmse2 = np.zeros((NoTrials, 1))\n",
    "\n",
    "for i in range(0, NoTrials):\n",
    "    print(f\"Trial {i} of {NoTrials}\")\n",
    "\n",
    "    # Split the data into 'nfolds' number of splits \n",
    "    inner_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    outer_cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # define the RSCV object\n",
    "    hgb_rscv =RandomizedSearchCV(\n",
    "        HGB(), # regressor\n",
    "        param_distributions=param_distribs, # hyperparameter space\n",
    "        n_iter=10, # \"Number of parameter settings that are sampled.\" [sklearn]\n",
    "        cv=inner_cv, # \"Determines the cross-validation splitting strategy\"[sklearn]\n",
    "        scoring=cv_scorer, \n",
    "        random_state=random_state, \n",
    "        verbose=1, \n",
    "        n_jobs=n_jobs)\n",
    "    \n",
    "    # fit the model on the Trainig Data\n",
    "    hgb_rscv.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the CV scores\n",
    "    hgb_rscv_rmse1[i] = np.sqrt(-hgb_rscv.best_score_)\n",
    "    y_pred_hgb = cvp(hgb_rscv, X_train, y_train, cv=outer_cv, n_jobs=n_jobs)\n",
    "    hgb_rscv_rmse2[i] = np.sqrt(mean_squared_error(y_train, y_pred_hgb))\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time)/60\n",
    "print(f\"Execution time: {execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal Parameters according to RSCV:', hgb_rscv.best_params_)\n",
    "print('best score' ,hgb_rscv.best_score_) # this returns the negative of the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HGB with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "hgb_opt = HGB(**hgb_rscv.best_params_)\n",
    "\n",
    "#fit the model\n",
    "hgb_opt.fit(X_train, y_train)\n",
    "\n",
    "# predict the values for X_test\n",
    "\n",
    "y_pred_hgb = hgb_opt.predict(X_test)\n",
    "\n",
    "# calculate the error between y_test (true) and y predicted\n",
    "rmse_hgb = np.sqrt(mean_squared_error(y_test, y_pred_hgb))\n",
    "rmse_hgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computational Considerations \n",
    "\n",
    "# Define the current models: \n",
    "\n",
    "model_list = [\"rf_opt\", \"pls_opt\", \"krr_opt\", \"xgb_opt\", \"hgb_opt\"]\n",
    "\n",
    "# write the models to memory: \n",
    "if save_model == True:\n",
    "    for i in model_list: \n",
    "        # Extract model name\n",
    "        #model_name = \n",
    "        model_name = i + i.__class__.__name__\n",
    "        # Construct a filepath\n",
    "        model_filepath = MODEL_PATH + f\"/{model_name}.pkl\"\n",
    "        # Save the model\n",
    "        joblib.dump(i, model_filepath)\n",
    "else:\n",
    "    print(\"Testrun, no model is written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models from memory\n",
    "\n",
    "for model in model_list:  \n",
    "    model_name = model.__class__.__name__  \n",
    "    model_filepath = MODEL_PATH + f\"/{model_name}.pkl\"  \n",
    "    model = joblib.load(model_filepath)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "In this section the goal is to document the packages which where used during the execution of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package informations\n",
    "from sklearn import show_versions\n",
    "show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_end_time = time.time()\n",
    "nb_execution_time = (nb_end_time - nb_start_time) / 60\n",
    "print(f\"Execution time: {nb_execution_time} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Get the current date\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define the notebook name, output name, and output directory\n",
    "try:\n",
    "    nb_filepath = __vsc_ipynb_file__ # works for Visual Studio Code\n",
    "    notebook_name = nb_filepath.split('/')[-1]\n",
    "except:\n",
    "    print('Please enter the notebook name manually')\n",
    "    pass\n",
    "# notebook_name = '03_1_modeling_rscv.ipynb'\n",
    "\n",
    "output_name = f\"{notebook_name.split('.')[0]}_{date}.html\"\n",
    "\n",
    "output_directory = './results/03_modeling_results/'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Specify the full output path\n",
    "full_output_path = os.path.join(output_directory, output_name)\n",
    "\n",
    "# Convert notebook to html with specified output name and path\n",
    "subprocess.call(['jupyter', 'nbconvert', '--to', 'html', notebook_name, '--output', full_output_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['_dh'][0] # notebook path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to convert the notebook to HTML\n",
    "def convert_notebook_to_html(notebook_name, output_name, RESULTS_PATH=RESULTS_PATH):\n",
    "    full_output_path = os.path.join(RESULTS_PATH, output_name)\n",
    "        # Use subprocess to call the jupyter nbconvert command\n",
    "    subprocess.call(['jupyter', 'nbconvert', '--to', 'html', 'notebook_name','--output', 'output_name', '--output-dir', 'RESULTS_PATH'])\n",
    "    \n",
    "    # Optionally, rename the output file if needed\n",
    "    # os.rename(notebook_name.split('.')[0] + '.html', full_output_path)\n",
    "\n",
    "# Wait for a short period to ensure all cells have finished executing\n",
    "time.sleep(3) # Adjust the sleep duration as needed\n",
    "\n",
    "# Convert the notebook to HTML\n",
    "convert_notebook_to_html(notebook_name, output_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
